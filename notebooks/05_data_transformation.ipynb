{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¹ Data Transformation - Cleaning and Enriching Your Data\n",
    "\n",
    "Welcome to the fifth tutorial in our **Data Ingestion Pipeline** series! In this hands-on notebook, you'll learn how to transform raw, messy data into clean, standardized, and enriched data ready for analysis.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- âœ… Clean messy data and handle missing values\n",
    "- âœ… Standardize data formats and structures\n",
    "- âœ… Enrich data with calculated fields and metadata\n",
    "- âœ… Normalize and deduplicate datasets\n",
    "- âœ… Build transformation pipelines\n",
    "- âœ… Monitor transformation quality and performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup and Imports\n",
    "\n",
    "Let's start by importing the libraries we'll need for data transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for data transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# For data transformation\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "\n",
    "print(\"ðŸ“¦ All libraries imported successfully!\")\n",
    "print(f\"ðŸ“Š Pandas version: {pd.__version__}\")\n",
    "print(f\"ðŸ”¢ NumPy version: {np.__version__}\")\n",
    "print(f\"â° Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ­ The Data Transformation Journey\n",
    "\n",
    "Data transformation is like **renovating a house** - you take something rough and make it beautiful and functional:\n",
    "\n",
    "### ðŸ  **Before Transformation (Raw Data):**\n",
    "- ðŸ§± **Messy Structure** - Inconsistent formats, missing values\n",
    "- ðŸŽ¨ **No Standards** - Different naming conventions, mixed cases\n",
    "- ðŸ”§ **Basic Functionality** - Data exists but hard to use\n",
    "- ðŸ“ **No Measurements** - Missing calculated fields and metrics\n",
    "\n",
    "### âœ¨ **After Transformation (Clean Data):**\n",
    "- ðŸ›ï¸ **Beautiful Structure** - Consistent, standardized format\n",
    "- ðŸŽ¯ **Clear Standards** - Uniform naming, proper data types\n",
    "- ðŸš€ **Enhanced Functionality** - Ready for analysis and ML\n",
    "- ðŸ“Š **Rich Information** - Enriched with insights and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample messy data to demonstrate transformations\n",
    "print(\"ðŸŽ­ Creating Sample Messy Data\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Simulate real-world messy e-commerce data\n",
    "messy_orders = {\n",
    "    'order_id': [\n",
    "        'ORD-2024-001',    # âœ… Good format\n",
    "        'ord-2024-002',    # ðŸ”§ Wrong case\n",
    "        'ORD_2024_003',    # ðŸ”§ Wrong separator\n",
    "        'ORDER-2024-004',  # ðŸ”§ Too long prefix\n",
    "        'ORD-2024-001',    # âŒ Duplicate\n",
    "        '2024-005',        # ðŸ”§ Missing prefix\n",
    "        'ORD-2024-007',    # âœ… Good format\n",
    "        'ORD-24-008'       # ðŸ”§ Wrong year format\n",
    "    ],\n",
    "    'customer_name': [\n",
    "        'John Doe',           # âœ… Good format\n",
    "        'jane smith',         # ðŸ”§ Wrong case\n",
    "        'BOB WILSON',         # ðŸ”§ All caps\n",
    "        'Alice  Johnson',     # ðŸ”§ Extra spaces\n",
    "        'charlie brown jr.',  # ðŸ”§ Mixed case\n",
    "        'Dr. Sarah Connor',   # ðŸ”§ Title included\n",
    "        'Mike O\\'Brien',      # ðŸ”§ Apostrophe\n",
    "        'Emma-Rose Davis'     # ðŸ”§ Hyphenated\n",
    "    ],\n",
    "    'customer_email': [\n",
    "        'john@example.com',      # âœ… Good format\n",
    "        'JANE@COMPANY.COM',      # ðŸ”§ Wrong case\n",
    "        'bob@email.co.uk',       # âœ… Good format\n",
    "        'alice@domain',          # âŒ Incomplete\n",
    "        'charlie.brown@test.org', # âœ… Good format\n",
    "        'sarah.connor@skynet.mil', # âœ… Good format\n",
    "        'mike@',                 # âŒ Incomplete\n",
    "        'emma.davis@company.com' # âœ… Good format\n",
    "    ],\n",
    "    'product_name': [\n",
    "        'iPhone 15',          # âœ… Good format\n",
    "        'macbook pro',        # ðŸ”§ Wrong case\n",
    "        'AIRPODS PRO',        # ðŸ”§ All caps\n",
    "        'iPad  Air',          # ðŸ”§ Extra spaces\n",
    "        'apple watch series 9', # ðŸ”§ Wrong case\n",
    "        'Nintendo Switch OLED', # ðŸ”§ Mixed case\n",
    "        'samsung galaxy s24',   # ðŸ”§ Wrong case\n",
    "        'Sony WH-1000XM5'     # âœ… Good format\n",
    "    ],\n",
    "    'quantity': [\n",
    "        '1',      # ðŸ”§ String instead of int\n",
    "        2,        # âœ… Good format\n",
    "        '3.0',    # ðŸ”§ Float string\n",
    "        1,        # âœ… Good format\n",
    "        '2',      # ðŸ”§ String\n",
    "        1,        # âœ… Good format\n",
    "        '1',      # ðŸ”§ String\n",
    "        2         # âœ… Good format\n",
    "    ],\n",
    "    'price': [\n",
    "        '$999.99',     # ðŸ”§ Currency symbol\n",
    "        '1999.99',     # ðŸ”§ String\n",
    "        249.99,        # âœ… Good format\n",
    "        '599.99 USD',  # ðŸ”§ Currency code\n",
    "        399.99,        # âœ… Good format\n",
    "        '$299.99',     # ðŸ”§ Currency symbol\n",
    "        '899.99',      # ðŸ”§ String\n",
    "        549.99         # âœ… Good format\n",
    "    ],\n",
    "    'order_date': [\n",
    "        '2024-01-15',      # âœ… Good format\n",
    "        '01/16/2024',      # ðŸ”§ US format\n",
    "        '17-01-2024',      # ðŸ”§ DD-MM-YYYY\n",
    "        '2024/01/18',      # ðŸ”§ Slash separator\n",
    "        '19 Jan 2024',     # ðŸ”§ Text format\n",
    "        '2024-01-20',      # âœ… Good format\n",
    "        '21/01/24',        # ðŸ”§ Short year\n",
    "        '2024-01-22'       # âœ… Good format\n",
    "    ],\n",
    "    'status': [\n",
    "        'pending',      # âœ… Good format\n",
    "        'SHIPPED',      # ðŸ”§ Wrong case\n",
    "        'Processing',   # ðŸ”§ Wrong case\n",
    "        'delivered',    # âœ… Good format\n",
    "        'Cancelled',    # ðŸ”§ Wrong case\n",
    "        'pending',      # âœ… Good format\n",
    "        'shipped',      # âœ… Good format\n",
    "        'PROCESSING'    # ðŸ”§ Wrong case\n",
    "    ],\n",
    "    'phone_number': [\n",
    "        '+1-555-123-4567',    # âœ… Good format\n",
    "        '555.234.5678',       # ðŸ”§ Dot separator\n",
    "        '(555) 345-6789',     # ðŸ”§ Parentheses\n",
    "        '555-456-7890',       # ðŸ”§ Missing country code\n",
    "        '+1 555 567 8901',    # ðŸ”§ Space separator\n",
    "        '5556789012',         # ðŸ”§ No separators\n",
    "        '+1-555-789-0123',    # âœ… Good format\n",
    "        '555 890 1234'        # ðŸ”§ Space separator\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_messy = pd.DataFrame(messy_orders)\n",
    "\n",
    "print(f\"ðŸ“Š Created messy dataset with {len(df_messy)} orders\")\n",
    "print(f\"ðŸ“‹ Columns: {list(df_messy.columns)}\")\n",
    "print(f\"\\nðŸ” Sample Messy Data:\")\n",
    "display(df_messy)\n",
    "\n",
    "# Show data types\n",
    "print(f\"\\nðŸ“ˆ Data Types:\")\n",
    "for col, dtype in df_messy.dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "# Show data quality issues\n",
    "print(f\"\\nðŸš¨ Data Quality Issues Identified:\")\n",
    "print(f\"  - Mixed case in text fields\")\n",
    "print(f\"  - Inconsistent date formats\")\n",
    "print(f\"  - Currency symbols in prices\")\n",
    "print(f\"  - String numbers instead of numeric types\")\n",
    "print(f\"  - Inconsistent phone number formats\")\n",
    "print(f\"  - Extra spaces in text\")\n",
    "print(f\"  - Duplicate order IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§½ Data Cleaning - The Foundation\n",
    "\n",
    "Data cleaning is the first and most important step in transformation. Let's clean our messy data step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning system\n",
    "print(\"ðŸ§½ Data Cleaning System\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "@dataclass\n",
    "class CleaningResult:\n",
    "    \"\"\"Container for cleaning results\"\"\"\n",
    "    success: bool\n",
    "    original_records: int\n",
    "    cleaned_records: int\n",
    "    cleaning_operations: List[str]\n",
    "    cleaning_time: float\n",
    "    data: Optional[pd.DataFrame] = None\n",
    "    issues_found: List[str] = None\n",
    "    issues_fixed: List[str] = None\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"Comprehensive data cleaning system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_stats = {\n",
    "            'operations_performed': [],\n",
    "            'issues_found': [],\n",
    "            'issues_fixed': [],\n",
    "            'records_affected': 0\n",
    "        }\n",
    "        \n",
    "        print(f\"ðŸ§½ Data cleaner initialized\")\n",
    "    \n",
    "    def clean_dataset(self, df: pd.DataFrame) -> CleaningResult:\n",
    "        \"\"\"\n",
    "        Perform comprehensive data cleaning\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset to clean\n",
    "        \n",
    "        Returns:\n",
    "            CleaningResult: Cleaning results and cleaned data\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        original_records = len(df)\n",
    "        \n",
    "        print(f\"ðŸ§½ Starting data cleaning for {original_records} records\")\n",
    "        \n",
    "        try:\n",
    "            # Make a copy to avoid modifying original\n",
    "            cleaned_df = df.copy()\n",
    "            operations = []\n",
    "            issues_found = []\n",
    "            issues_fixed = []\n",
    "            \n",
    "            # Step 1: Remove exact duplicates\n",
    "            before_dedup = len(cleaned_df)\n",
    "            cleaned_df = cleaned_df.drop_duplicates()\n",
    "            after_dedup = len(cleaned_df)\n",
    "            \n",
    "            if before_dedup != after_dedup:\n",
    "                duplicates_removed = before_dedup - after_dedup\n",
    "                operations.append(f\"Removed {duplicates_removed} exact duplicate records\")\n",
    "                issues_found.append(f\"Found {duplicates_removed} exact duplicates\")\n",
    "                issues_fixed.append(f\"Removed {duplicates_removed} exact duplicates\")\n",
    "            \n",
    "            # Step 2: Clean text fields\n",
    "            text_cleaning_result = self._clean_text_fields(cleaned_df)\n",
    "            cleaned_df = text_cleaning_result['data']\n",
    "            operations.extend(text_cleaning_result['operations'])\n",
    "            issues_found.extend(text_cleaning_result['issues_found'])\n",
    "            issues_fixed.extend(text_cleaning_result['issues_fixed'])\n",
    "            \n",
    "            # Step 3: Clean numeric fields\n",
    "            numeric_cleaning_result = self._clean_numeric_fields(cleaned_df)\n",
    "            cleaned_df = numeric_cleaning_result['data']\n",
    "            operations.extend(numeric_cleaning_result['operations'])\n",
    "            issues_found.extend(numeric_cleaning_result['issues_found'])\n",
    "            issues_fixed.extend(numeric_cleaning_result['issues_fixed'])\n",
    "            \n",
    "            # Step 4: Clean date fields\n",
    "            date_cleaning_result = self._clean_date_fields(cleaned_df)\n",
    "            cleaned_df = date_cleaning_result['data']\n",
    "            operations.extend(date_cleaning_result['operations'])\n",
    "            issues_found.extend(date_cleaning_result['issues_found'])\n",
    "            issues_fixed.extend(date_cleaning_result['issues_fixed'])\n",
    "            \n",
    "            # Step 5: Clean phone numbers\n",
    "            phone_cleaning_result = self._clean_phone_numbers(cleaned_df)\n",
    "            cleaned_df = phone_cleaning_result['data']\n",
    "            operations.extend(phone_cleaning_result['operations'])\n",
    "            issues_found.extend(phone_cleaning_result['issues_found'])\n",
    "            issues_fixed.extend(phone_cleaning_result['issues_fixed'])\n",
    "            \n",
    "            # Step 6: Handle missing values\n",
    "            missing_handling_result = self._handle_missing_values(cleaned_df)\n",
    "            cleaned_df = missing_handling_result['data']\n",
    "            operations.extend(missing_handling_result['operations'])\n",
    "            issues_found.extend(missing_handling_result['issues_found'])\n",
    "            issues_fixed.extend(missing_handling_result['issues_fixed'])\n",
    "            \n",
    "            # Calculate cleaning time\n",
    "            cleaning_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            result = CleaningResult(\n",
    "                success=True,\n",
    "                original_records=original_records,\n",
    "                cleaned_records=len(cleaned_df),\n",
    "                cleaning_operations=operations,\n",
    "                cleaning_time=cleaning_time,\n",
    "                data=cleaned_df,\n",
    "                issues_found=issues_found,\n",
    "                issues_fixed=issues_fixed\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Data cleaning completed in {cleaning_time:.3f} seconds\")\n",
    "            print(f\"ðŸ“Š Records: {original_records} â†’ {len(cleaned_df)}\")\n",
    "            print(f\"ðŸ”§ Operations performed: {len(operations)}\")\n",
    "            print(f\"ðŸš¨ Issues found: {len(issues_found)}\")\n",
    "            print(f\"âœ… Issues fixed: {len(issues_fixed)}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            cleaning_time = (datetime.now() - start_time).total_seconds()\n",
    "            print(f\"âŒ Data cleaning failed: {str(e)}\")\n",
    "            \n",
    "            return CleaningResult(\n",
    "                success=False,\n",
    "                original_records=original_records,\n",
    "                cleaned_records=0,\n",
    "                cleaning_operations=[],\n",
    "                cleaning_time=cleaning_time,\n",
    "                data=None,\n",
    "                issues_found=[f\"Cleaning failed: {str(e)}\"],\n",
    "                issues_fixed=[]\n",
    "            )\n",
    "    \n",
    "    def _clean_text_fields(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Clean text fields\"\"\"\n",
    "        operations = []\n",
    "        issues_found = []\n",
    "        issues_fixed = []\n",
    "        \n",
    "        text_fields = ['customer_name', 'product_name', 'status']\n",
    "        \n",
    "        for field in text_fields:\n",
    "            if field in df.columns:\n",
    "                original_values = df[field].copy()\n",
    "                \n",
    "                # Remove extra spaces\n",
    "                df[field] = df[field].astype(str).str.strip()\n",
    "                df[field] = df[field].str.replace(r'\\s+', ' ', regex=True)\n",
    "                \n",
    "                # Standardize case based on field type\n",
    "                if field == 'customer_name':\n",
    "                    # Title case for names\n",
    "                    df[field] = df[field].str.title()\n",
    "                    # Fix common title issues\n",
    "                    df[field] = df[field].str.replace(r\"\\bDr\\b\", \"Dr.\", regex=True)\n",
    "                    df[field] = df[field].str.replace(r\"\\bMr\\b\", \"Mr.\", regex=True)\n",
    "                    df[field] = df[field].str.replace(r\"\\bMs\\b\", \"Ms.\", regex=True)\n",
    "                    df[field] = df[field].str.replace(r\"\\bMrs\\b\", \"Mrs.\", regex=True)\n",
    "                    \n",
    "                elif field == 'product_name':\n",
    "                    # Title case for products\n",
    "                    df[field] = df[field].str.title()\n",
    "                    # Fix brand names\n",
    "                    brand_fixes = {\n",
    "                        'Iphone': 'iPhone',\n",
    "                        'Macbook': 'MacBook',\n",
    "                        'Airpods': 'AirPods',\n",
    "                        'Ipad': 'iPad'\n",
    "                    }\n",
    "                    for wrong, correct in brand_fixes.items():\n",
    "                        df[field] = df[field].str.replace(wrong, correct)\n",
    "                        \n",
    "                elif field == 'status':\n",
    "                    # Lowercase for status\n",
    "                    df[field] = df[field].str.lower()\n",
    "                \n",
    "                # Count changes\n",
    "                changes = (original_values != df[field]).sum()\n",
    "                if changes > 0:\n",
    "                    operations.append(f\"Cleaned {field}: {changes} values standardized\")\n",
    "                    issues_found.append(f\"Found {changes} inconsistent {field} values\")\n",
    "                    issues_fixed.append(f\"Standardized {changes} {field} values\")\n",
    "        \n",
    "        # Clean email addresses\n",
    "        if 'customer_email' in df.columns:\n",
    "            original_emails = df['customer_email'].copy()\n",
    "            df['customer_email'] = df['customer_email'].astype(str).str.lower().str.strip()\n",
    "            \n",
    "            # Count changes\n",
    "            email_changes = (original_emails != df['customer_email']).sum()\n",
    "            if email_changes > 0:\n",
    "                operations.append(f\"Cleaned customer_email: {email_changes} values standardized\")\n",
    "                issues_found.append(f\"Found {email_changes} inconsistent email formats\")\n",
    "                issues_fixed.append(f\"Standardized {email_changes} email addresses\")\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'issues_found': issues_found,\n",
    "            'issues_fixed': issues_fixed\n",
    "        }\n",
    "    \n",
    "    def _clean_numeric_fields(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Clean numeric fields\"\"\"\n",
    "        operations = []\n",
    "        issues_found = []\n",
    "        issues_fixed = []\n",
    "        \n",
    "        # Clean price field\n",
    "        if 'price' in df.columns:\n",
    "            original_prices = df['price'].copy()\n",
    "            \n",
    "            # Convert to string first\n",
    "            df['price'] = df['price'].astype(str)\n",
    "            \n",
    "            # Remove currency symbols and text\n",
    "            df['price'] = df['price'].str.replace('$', '', regex=False)\n",
    "            df['price'] = df['price'].str.replace(' USD', '', regex=False)\n",
    "            df['price'] = df['price'].str.replace(',', '', regex=False)\n",
    "            \n",
    "            # Convert to float\n",
    "            df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "            \n",
    "            # Count changes\n",
    "            price_changes = (original_prices.astype(str) != df['price'].astype(str)).sum()\n",
    "            if price_changes > 0:\n",
    "                operations.append(f\"Cleaned price: {price_changes} values converted to numeric\")\n",
    "                issues_found.append(f\"Found {price_changes} non-numeric price values\")\n",
    "                issues_fixed.append(f\"Converted {price_changes} prices to numeric format\")\n",
    "        \n",
    "        # Clean quantity field\n",
    "        if 'quantity' in df.columns:\n",
    "            original_quantities = df['quantity'].copy()\n",
    "            \n",
    "            # Convert to numeric\n",
    "            df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')\n",
    "            df['quantity'] = df['quantity'].astype('Int64')  # Nullable integer\n",
    "            \n",
    "            # Count changes\n",
    "            quantity_changes = (original_quantities.astype(str) != df['quantity'].astype(str)).sum()\n",
    "            if quantity_changes > 0:\n",
    "                operations.append(f\"Cleaned quantity: {quantity_changes} values converted to integer\")\n",
    "                issues_found.append(f\"Found {quantity_changes} non-integer quantity values\")\n",
    "                issues_fixed.append(f\"Converted {quantity_changes} quantities to integer format\")\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'issues_found': issues_found,\n",
    "            'issues_fixed': issues_fixed\n",
    "        }\n",
    "    \n",
    "    def _clean_date_fields(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Clean date fields\"\"\"\n",
    "        operations = []\n",
    "        issues_found = []\n",
    "        issues_fixed = []\n",
    "        \n",
    "        if 'order_date' in df.columns:\n",
    "            original_dates = df['order_date'].copy()\n",
    "            \n",
    "            # Try to parse dates with multiple formats\n",
    "            date_formats = [\n",
    "                '%Y-%m-%d',      # 2024-01-15\n",
    "                '%m/%d/%Y',      # 01/15/2024\n",
    "                '%d-%m-%Y',      # 15-01-2024\n",
    "                '%Y/%m/%d',      # 2024/01/15\n",
    "                '%d %b %Y',      # 15 Jan 2024\n",
    "                '%d/%m/%y'       # 15/01/24\n",
    "            ]\n",
    "            \n",
    "            parsed_dates = []\n",
    "            for date_str in df['order_date']:\n",
    "                parsed_date = None\n",
    "                for fmt in date_formats:\n",
    "                    try:\n",
    "                        parsed_date = pd.to_datetime(date_str, format=fmt)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if parsed_date is None:\n",
    "                    try:\n",
    "                        parsed_date = pd.to_datetime(date_str, infer_datetime_format=True)\n",
    "                    except:\n",
    "                        parsed_date = pd.NaT\n",
    "                \n",
    "                parsed_dates.append(parsed_date)\n",
    "            \n",
    "            df['order_date'] = parsed_dates\n",
    "            \n",
    "            # Count successful conversions\n",
    "            successful_conversions = df['order_date'].notna().sum()\n",
    "            failed_conversions = df['order_date'].isna().sum()\n",
    "            \n",
    "            if successful_conversions > 0:\n",
    "                operations.append(f\"Cleaned order_date: {successful_conversions} dates standardized\")\n",
    "                issues_found.append(f\"Found mixed date formats in {len(df)} records\")\n",
    "                issues_fixed.append(f\"Standardized {successful_conversions} dates to ISO format\")\n",
    "            \n",
    "            if failed_conversions > 0:\n",
    "                issues_found.append(f\"Found {failed_conversions} unparseable dates\")\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'issues_found': issues_found,\n",
    "            'issues_fixed': issues_fixed\n",
    "        }\n",
    "    \n",
    "    def _clean_phone_numbers(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Clean phone number fields\"\"\"\n",
    "        operations = []\n",
    "        issues_found = []\n",
    "        issues_fixed = []\n",
    "        \n",
    "        if 'phone_number' in df.columns:\n",
    "            original_phones = df['phone_number'].copy()\n",
    "            \n",
    "            # Standardize phone numbers to +1-XXX-XXX-XXXX format\n",
    "            standardized_phones = []\n",
    "            \n",
    "            for phone in df['phone_number']:\n",
    "                if pd.isna(phone):\n",
    "                    standardized_phones.append(phone)\n",
    "                    continue\n",
    "                \n",
    "                # Extract digits only\n",
    "                digits = re.sub(r'\\D', '', str(phone))\n",
    "                \n",
    "                # Handle different formats\n",
    "                if len(digits) == 10:  # US number without country code\n",
    "                    formatted = f\"+1-{digits[:3]}-{digits[3:6]}-{digits[6:]}\"\n",
    "                elif len(digits) == 11 and digits.startswith('1'):  # US number with country code\n",
    "                    formatted = f\"+1-{digits[1:4]}-{digits[4:7]}-{digits[7:]}\"\n",
    "                else:\n",
    "                    formatted = phone  # Keep original if can't parse\n",
    "                \n",
    "                standardized_phones.append(formatted)\n",
    "            \n",
    "            df['phone_number'] = standardized_phones\n",
    "            \n",
    "            # Count changes\n",
    "            phone_changes = (original_phones != df['phone_number']).sum()\n",
    "            if phone_changes > 0:\n",
    "                operations.append(f\"Cleaned phone_number: {phone_changes} numbers standardized\")\n",
    "                issues_found.append(f\"Found {phone_changes} inconsistent phone formats\")\n",
    "                issues_fixed.append(f\"Standardized {phone_changes} phone numbers\")\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'issues_found': issues_found,\n",
    "            'issues_fixed': issues_fixed\n",
    "        }\n",
    "    \n",
    "    def _handle_missing_values(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Handle missing values intelligently\"\"\"\n",
    "        operations = []\n",
    "        issues_found = []\n",
    "        issues_fixed = []\n",
    "        \n",
    "        # Count missing values\n",
    "        missing_counts = df.isnull().sum()\n",
    "        total_missing = missing_counts.sum()\n",
    "        \n",
    "        if total_missing > 0:\n",
    "            issues_found.append(f\"Found {total_missing} missing values across all columns\")\n",
    "            \n",
    "            # Handle missing values by column\n",
    "            for column, missing_count in missing_counts.items():\n",
    "                if missing_count > 0:\n",
    "                    if column in ['customer_email', 'phone_number']:\n",
    "                        # Keep as null for optional contact info\n",
    "                        operations.append(f\"Kept {missing_count} null values in {column} (optional field)\")\n",
    "                    elif column == 'status':\n",
    "                        # Fill with default status\n",
    "                        df[column] = df[column].fillna('pending')\n",
    "                        operations.append(f\"Filled {missing_count} missing {column} values with 'pending'\")\n",
    "                        issues_fixed.append(f\"Filled {missing_count} missing status values\")\n",
    "                    elif column == 'quantity':\n",
    "                        # Fill with 1\n",
    "                        df[column] = df[column].fillna(1)\n",
    "                        operations.append(f\"Filled {missing_count} missing {column} values with 1\")\n",
    "                        issues_fixed.append(f\"Filled {missing_count} missing quantity values\")\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'issues_found': issues_found,\n",
    "            'issues_fixed': issues_fixed\n",
    "        }\n",
    "\n",
    "# Test the data cleaner\n",
    "cleaner = DataCleaner()\n",
    "cleaning_result = cleaner.clean_dataset(df_messy)\n",
    "\n",
    "if cleaning_result.success:\n",
    "    print(f\"\\nðŸŽ‰ Data cleaning successful!\")\n",
    "    print(f\"\\nðŸ“Š Cleaning Summary:\")\n",
    "    print(f\"  Original records: {cleaning_result.original_records}\")\n",
    "    print(f\"  Cleaned records: {cleaning_result.cleaned_records}\")\n",
    "    print(f\"  Cleaning time: {cleaning_result.cleaning_time:.3f} seconds\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Operations Performed:\")\n",
    "    for i, operation in enumerate(cleaning_result.cleaning_operations, 1):\n",
    "        print(f\"  {i}. {operation}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Issues Fixed:\")\n",
    "    for i, fix in enumerate(cleaning_result.issues_fixed, 1):\n",
    "        print(f\"  {i}. {fix}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Cleaned Data Sample:\")\n",
    "    display(cleaning_result.data.head())\n",
    "    \n",
    "    # Store cleaned data for next steps\n",
    "    df_cleaned = cleaning_result.data\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Data cleaning failed!\")\n",
    "    print(f\"Issues: {cleaning_result.issues_found}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Data Standardization - Making Everything Consistent\n",
    "\n",
    "After cleaning, we need to standardize our data to ensure consistency across all records. This includes formatting, naming conventions, and data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardization system\n",
    "print(\"ðŸ“ Data Standardization System\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "@dataclass\n",
    "class StandardizationResult:\n",
    "    \"\"\"Container for standardization results\"\"\"\n",
    "    success: bool\n",
    "    records_processed: int\n",
    "    fields_standardized: int\n",
    "    standardization_operations: List[str]\n",
    "    standardization_time: float\n",
    "    data: Optional[pd.DataFrame] = None\n",
    "    before_after_examples: Dict[str, List[Tuple[str, str]]] = None\n",
    "\n",
    "class DataStandardizer:\n",
    "    \"\"\"Comprehensive data standardization system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define standardization rules\n",
    "        self.standardization_rules = {\n",
    "            'order_id': {\n",
    "                'format': 'ORD-YYYY-NNN',\n",
    "                'pattern': r'^ORD-\\d{4}-\\d{3}$'\n",
    "            },\n",
    "            'product_categories': {\n",
    "                'iPhone': 'Smartphones',\n",
    "                'MacBook': 'Laptops',\n",
    "                'iPad': 'Tablets',\n",
    "                'AirPods': 'Audio',\n",
    "                'Apple Watch': 'Wearables',\n",
    "                'Nintendo Switch': 'Gaming',\n",
    "                'Samsung Galaxy': 'Smartphones',\n",
    "                'Sony': 'Audio'\n",
    "            },\n",
    "            'status_mapping': {\n",
    "                'pending': 'pending',\n",
    "                'processing': 'processing', \n",
    "                'shipped': 'shipped',\n",
    "                'delivered': 'delivered',\n",
    "                'cancelled': 'cancelled'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"ðŸ“ Data standardizer initialized\")\n",
    "        print(f\"  Standardization rules loaded: {len(self.standardization_rules)}\")\n",
    "    \n",
    "    def standardize_dataset(self, df: pd.DataFrame) -> StandardizationResult:\n",
    "        \"\"\"\n",
    "        Perform comprehensive data standardization\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset to standardize\n",
    "        \n",
    "        Returns:\n",
    "            StandardizationResult: Standardization results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        records_processed = len(df)\n",
    "        \n",
    "        print(f\"ðŸ“ Starting data standardization for {records_processed} records\")\n",
    "        \n",
    "        try:\n",
    "            # Make a copy\n",
    "            standardized_df = df.copy()\n",
    "            operations = []\n",
    "            fields_standardized = 0\n",
    "            before_after_examples = {}\n",
    "            \n",
    "            # Step 1: Standardize order IDs\n",
    "            if 'order_id' in standardized_df.columns:\n",
    "                order_id_result = self._standardize_order_ids(standardized_df)\n",
    "                standardized_df = order_id_result['data']\n",
    "                operations.extend(order_id_result['operations'])\n",
    "                before_after_examples['order_id'] = order_id_result['examples']\n",
    "                if order_id_result['standardized']:\n",
    "                    fields_standardized += 1\n",
    "            \n",
    "            # Step 2: Add product categories\n",
    "            if 'product_name' in standardized_df.columns:\n",
    "                category_result = self._add_product_categories(standardized_df)\n",
    "                standardized_df = category_result['data']\n",
    "                operations.extend(category_result['operations'])\n",
    "                before_after_examples['product_category'] = category_result['examples']\n",
    "                if category_result['added']:\n",
    "                    fields_standardized += 1\n",
    "            \n",
    "            # Step 3: Standardize customer information\n",
    "            customer_result = self._standardize_customer_info(standardized_df)\n",
    "            standardized_df = customer_result['data']\n",
    "            operations.extend(customer_result['operations'])\n",
    "            before_after_examples.update(customer_result['examples'])\n",
    "            fields_standardized += customer_result['fields_standardized']\n",
    "            \n",
    "            # Step 4: Add calculated fields\n",
    "            calculated_result = self._add_calculated_fields(standardized_df)\n",
    "            standardized_df = calculated_result['data']\n",
    "            operations.extend(calculated_result['operations'])\n",
    "            fields_standardized += calculated_result['fields_added']\n",
    "            \n",
    "            # Step 5: Standardize column names\n",
    "            column_result = self._standardize_column_names(standardized_df)\n",
    "            standardized_df = column_result['data']\n",
    "            operations.extend(column_result['operations'])\n",
    "            \n",
    "            # Calculate standardization time\n",
    "            standardization_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            result = StandardizationResult(\n",
    "                success=True,\n",
    "                records_processed=records_processed,\n",
    "                fields_standardized=fields_standardized,\n",
    "                standardization_operations=operations,\n",
    "                standardization_time=standardization_time,\n",
    "                data=standardized_df,\n",
    "                before_after_examples=before_after_examples\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Data standardization completed in {standardization_time:.3f} seconds\")\n",
    "            print(f\"ðŸ“Š Records processed: {records_processed}\")\n",
    "            print(f\"ðŸ“ Fields standardized: {fields_standardized}\")\n",
    "            print(f\"ðŸ”§ Operations performed: {len(operations)}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            standardization_time = (datetime.now() - start_time).total_seconds()\n",
    "            print(f\"âŒ Data standardization failed: {str(e)}\")\n",
    "            \n",
    "            return StandardizationResult(\n",
    "                success=False,\n",
    "                records_processed=records_processed,\n",
    "                fields_standardized=0,\n",
    "                standardization_operations=[],\n",
    "                standardization_time=standardization_time,\n",
    "                data=None\n",
    "            )\n",
    "    \n",
    "    def _standardize_order_ids(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Standardize order ID format\"\"\"\n",
    "        operations = []\n",
    "        examples = []\n",
    "        standardized = False\n",
    "        \n",
    "        if 'order_id' in df.columns:\n",
    "            original_ids = df['order_id'].copy()\n",
    "            \n",
    "            # Standardize to ORD-YYYY-NNN format\n",
    "            standardized_ids = []\n",
    "            \n",
    "            for order_id in df['order_id']:\n",
    "                if pd.isna(order_id):\n",
    "                    standardized_ids.append(order_id)\n",
    "                    continue\n",
    "                \n",
    "                order_str = str(order_id).upper()\n",
    "                \n",
    "                # Handle different formats\n",
    "                if order_str.startswith('ORD-') and len(order_str.split('-')) == 3:\n",
    "                    # Already in correct format\n",
    "                    standardized_ids.append(order_str)\n",
    "                elif order_str.startswith('ORD_'):\n",
    "                    # Replace underscores with dashes\n",
    "                    standardized_ids.append(order_str.replace('_', '-'))\n",
    "                elif order_str.startswith('ORDER-'):\n",
    "                    # Shorten ORDER to ORD\n",
    "                    standardized_ids.append(order_str.replace('ORDER-', 'ORD-'))\n",
    "                elif re.match(r'^\\d{4}-\\d{3}$', order_str):\n",
    "                    # Add ORD prefix\n",
    "                    standardized_ids.append(f'ORD-{order_str}')\n",
    "                elif re.match(r'^ORD-\\d{2}-\\d{3}$', order_str):\n",
    "                    # Fix short year\n",
    "                    parts = order_str.split('-')\n",
    "                    year = f'20{parts[1]}' if int(parts[1]) < 50 else f'19{parts[1]}'\n",
    "                    standardized_ids.append(f'ORD-{year}-{parts[2]}')\n",
    "                else:\n",
    "                    # Keep original if can't parse\n",
    "                    standardized_ids.append(order_str)\n",
    "            \n",
    "            df['order_id'] = standardized_ids\n",
    "            \n",
    "            # Count changes and create examples\n",
    "            changes = (original_ids != df['order_id']).sum()\n",
    "            if changes > 0:\n",
    "                standardized = True\n",
    "                operations.append(f\"Standardized {changes} order IDs to ORD-YYYY-NNN format\")\n",
    "                \n",
    "                # Create before/after examples\n",
    "                for i, (old, new) in enumerate(zip(original_ids, df['order_id'])):\n",
    "                    if old != new and len(examples) < 3:\n",
    "                        examples.append((str(old), str(new)))\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'examples': examples,\n",
    "            'standardized': standardized\n",
    "        }\n",
    "    \n",
    "    def _add_product_categories(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Add product categories based on product names\"\"\"\n",
    "        operations = []\n",
    "        examples = []\n",
    "        added = False\n",
    "        \n",
    "        if 'product_name' in df.columns:\n",
    "            # Create product category column\n",
    "            categories = []\n",
    "            \n",
    "            for product in df['product_name']:\n",
    "                if pd.isna(product):\n",
    "                    categories.append('Unknown')\n",
    "                    continue\n",
    "                \n",
    "                product_str = str(product)\n",
    "                category = 'Other'  # Default category\n",
    "                \n",
    "                # Match against category rules\n",
    "                for key, cat in self.standardization_rules['product_categories'].items():\n",
    "                    if key.lower() in product_str.lower():\n",
    "                        category = cat\n",
    "                        break\n",
    "                \n",
    "                categories.append(category)\n",
    "            \n",
    "            df['product_category'] = categories\n",
    "            added = True\n",
    "            \n",
    "            operations.append(f\"Added product_category field with {len(set(categories))} unique categories\")\n",
    "            \n",
    "            # Create examples\n",
    "            for i, (product, category) in enumerate(zip(df['product_name'], categories)):\n",
    "                if len(examples) < 3 and category != 'Other':\n",
    "                    examples.append((str(product), str(category)))\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'examples': examples,\n",
    "            'added': added\n",
    "        }\n",
    "    \n",
    "    def _standardize_customer_info(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Standardize customer information\"\"\"\n",
    "        operations = []\n",
    "        examples = {}\n",
    "        fields_standardized = 0\n",
    "        \n",
    "        # Extract customer domain from email\n",
    "        if 'customer_email' in df.columns:\n",
    "            domains = []\n",
    "            for email in df['customer_email']:\n",
    "                if pd.isna(email) or '@' not in str(email):\n",
    "                    domains.append('Unknown')\n",
    "                else:\n",
    "                    domain = str(email).split('@')[1]\n",
    "                    domains.append(domain)\n",
    "            \n",
    "            df['customer_domain'] = domains\n",
    "            operations.append(f\"Added customer_domain field extracted from email addresses\")\n",
    "            examples['customer_domain'] = [(df['customer_email'].iloc[0], domains[0])] if len(domains) > 0 else []\n",
    "            fields_standardized += 1\n",
    "        \n",
    "        # Create customer type based on email domain\n",
    "        if 'customer_domain' in df.columns:\n",
    "            customer_types = []\n",
    "            for domain in df['customer_domain']:\n",
    "                if domain == 'Unknown':\n",
    "                    customer_types.append('Unknown')\n",
    "                elif domain in ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com']:\n",
    "                    customer_types.append('Personal')\n",
    "                else:\n",
    "                    customer_types.append('Business')\n",
    "            \n",
    "            df['customer_type'] = customer_types\n",
    "            operations.append(f\"Added customer_type field based on email domain\")\n",
    "            fields_standardized += 1\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'examples': examples,\n",
    "            'fields_standardized': fields_standardized\n",
    "        }\n",
    "    \n",
    "    def _add_calculated_fields(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Add calculated fields\"\"\"\n",
    "        operations = []\n",
    "        fields_added = 0\n",
    "        \n",
    "        # Calculate total amount\n",
    "        if 'price' in df.columns and 'quantity' in df.columns:\n",
    "            df['total_amount'] = df['price'] * df['quantity']\n",
    "            operations.append(\"Added total_amount field (price Ã— quantity)\")\n",
    "            fields_added += 1\n",
    "        \n",
    "        # Add order value category\n",
    "        if 'total_amount' in df.columns:\n",
    "            value_categories = []\n",
    "            for amount in df['total_amount']:\n",
    "                if pd.isna(amount):\n",
    "                    value_categories.append('Unknown')\n",
    "                elif amount < 100:\n",
    "                    value_categories.append('Low')\n",
    "                elif amount < 500:\n",
    "                    value_categories.append('Medium')\n",
    "                elif amount < 1000:\n",
    "                    value_categories.append('High')\n",
    "                else:\n",
    "                    value_categories.append('Premium')\n",
    "            \n",
    "            df['order_value_category'] = value_categories\n",
    "            operations.append(\"Added order_value_category field based on total amount\")\n",
    "            fields_added += 1\n",
    "        \n",
    "        # Add processing timestamp\n",
    "        df['processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        operations.append(\"Added processed_at timestamp\")\n",
    "        fields_added += 1\n",
    "        \n",
    "        # Add record hash for tracking\n",
    "        record_hashes = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Create hash from key fields\n",
    "            key_fields = ['order_id', 'customer_email', 'product_name']\n",
    "            hash_string = ''.join([str(row.get(field, '')) for field in key_fields])\n",
    "            record_hash = hashlib.md5(hash_string.encode()).hexdigest()[:8]\n",
    "            record_hashes.append(record_hash)\n",
    "        \n",
    "        df['record_hash'] = record_hashes\n",
    "        operations.append(\"Added record_hash for data lineage tracking\")\n",
    "        fields_added += 1\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'fields_added': fields_added\n",
    "        }\n",
    "    \n",
    "    def _standardize_column_names(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Standardize column names to snake_case\"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        # Convert column names to snake_case\n",
    "        original_columns = df.columns.tolist()\n",
    "        standardized_columns = []\n",
    "        \n",
    "        for col in original_columns:\n",
    "            # Convert to snake_case\n",
    "            snake_case = re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower()\n",
    "            snake_case = re.sub(r'[^a-z0-9_]', '_', snake_case)\n",
    "            snake_case = re.sub(r'_+', '_', snake_case).strip('_')\n",
    "            standardized_columns.append(snake_case)\n",
    "        \n",
    "        # Rename columns\n",
    "        column_mapping = dict(zip(original_columns, standardized_columns))\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Count changes\n",
    "        changes = sum(1 for old, new in column_mapping.items() if old != new)\n",
    "        if changes > 0:\n",
    "            operations.append(f\"Standardized {changes} column names to snake_case\")\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations\n",
    "        }\n",
    "\n",
    "# Test the data standardizer\n",
    "standardizer = DataStandardizer()\n",
    "standardization_result = standardizer.standardize_dataset(df_cleaned)\n",
    "\n",
    "if standardization_result.success:\n",
    "    print(f\"\\nðŸŽ‰ Data standardization successful!\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Standardization Operations:\")\n",
    "    for i, operation in enumerate(standardization_result.standardization_operations, 1):\n",
    "        print(f\"  {i}. {operation}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Before/After Examples:\")\n",
    "    for field, examples in standardization_result.before_after_examples.items():\n",
    "        if examples:\n",
    "            print(f\"  {field}:\")\n",
    "            for before, after in examples[:2]:  # Show first 2 examples\n",
    "                print(f\"    '{before}' â†’ '{after}'\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Standardized Data Sample:\")\n",
    "    display(standardization_result.data.head())\n",
    "    \n",
    "    print(f\"\\nðŸ“ New Columns Added:\")\n",
    "    new_columns = set(standardization_result.data.columns) - set(df_cleaned.columns)\n",
    "    for col in sorted(new_columns):\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Store standardized data for next steps\n",
    "    df_standardized = standardization_result.data\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Data standardization failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âž• Data Enrichment - Adding Value and Intelligence\n",
    "\n",
    "Data enrichment adds valuable information and insights to your dataset. This includes calculated metrics, business intelligence, and external data integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data enrichment system\n",
    "print(\"âž• Data Enrichment System\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "@dataclass\n",
    "class EnrichmentResult:\n",
    "    \"\"\"Container for enrichment results\"\"\"\n",
    "    success: bool\n",
    "    records_processed: int\n",
    "    fields_added: int\n",
    "    enrichment_operations: List[str]\n",
    "    enrichment_time: float\n",
    "    data: Optional[pd.DataFrame] = None\n",
    "    enrichment_summary: Dict[str, Any] = None\n",
    "\n",
    "class DataEnricher:\n",
    "    \"\"\"Comprehensive data enrichment system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define enrichment rules and lookup data\n",
    "        self.product_data = {\n",
    "            'iPhone 15': {'brand': 'Apple', 'release_year': 2023, 'avg_rating': 4.5},\n",
    "            'MacBook Pro': {'brand': 'Apple', 'release_year': 2023, 'avg_rating': 4.7},\n",
    "            'iPad Air': {'brand': 'Apple', 'release_year': 2022, 'avg_rating': 4.4},\n",
    "            'AirPods Pro': {'brand': 'Apple', 'release_year': 2022, 'avg_rating': 4.6},\n",
    "            'Apple Watch Series 9': {'brand': 'Apple', 'release_year': 2023, 'avg_rating': 4.3},\n",
    "            'Nintendo Switch Oled': {'brand': 'Nintendo', 'release_year': 2021, 'avg_rating': 4.8},\n",
    "            'Samsung Galaxy S24': {'brand': 'Samsung', 'release_year': 2024, 'avg_rating': 4.4},\n",
    "            'Sony Wh-1000Xm5': {'brand': 'Sony', 'release_year': 2022, 'avg_rating': 4.7}\n",
    "        }\n",
    "        \n",
    "        self.seasonal_factors = {\n",
    "            1: 0.8,   # January - post-holiday low\n",
    "            2: 0.9,   # February\n",
    "            3: 1.0,   # March\n",
    "            4: 1.0,   # April\n",
    "            5: 1.1,   # May\n",
    "            6: 1.0,   # June\n",
    "            7: 1.0,   # July\n",
    "            8: 1.1,   # August - back to school\n",
    "            9: 1.2,   # September - back to school\n",
    "            10: 1.1,  # October\n",
    "            11: 1.4,  # November - Black Friday\n",
    "            12: 1.5   # December - Holiday season\n",
    "        }\n",
    "        \n",
    "        print(f\"âž• Data enricher initialized\")\n",
    "        print(f\"  Product database: {len(self.product_data)} products\")\n",
    "        print(f\"  Seasonal factors: {len(self.seasonal_factors)} months\")\n",
    "    \n",
    "    def enrich_dataset(self, df: pd.DataFrame) -> EnrichmentResult:\n",
    "        \"\"\"\n",
    "        Perform comprehensive data enrichment\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset to enrich\n",
    "        \n",
    "        Returns:\n",
    "            EnrichmentResult: Enrichment results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        records_processed = len(df)\n",
    "        \n",
    "        print(f\"âž• Starting data enrichment for {records_processed} records\")\n",
    "        \n",
    "        try:\n",
    "            # Make a copy\n",
    "            enriched_df = df.copy()\n",
    "            operations = []\n",
    "            fields_added = 0\n",
    "            \n",
    "            # Step 1: Add product information\n",
    "            product_result = self._add_product_information(enriched_df)\n",
    "            enriched_df = product_result['data']\n",
    "            operations.extend(product_result['operations'])\n",
    "            fields_added += product_result['fields_added']\n",
    "            \n",
    "            # Step 2: Add customer insights\n",
    "            customer_result = self._add_customer_insights(enriched_df)\n",
    "            enriched_df = customer_result['data']\n",
    "            operations.extend(customer_result['operations'])\n",
    "            fields_added += customer_result['fields_added']\n",
    "            \n",
    "            # Step 3: Add temporal features\n",
    "            temporal_result = self._add_temporal_features(enriched_df)\n",
    "            enriched_df = temporal_result['data']\n",
    "            operations.extend(temporal_result['operations'])\n",
    "            fields_added += temporal_result['fields_added']\n",
    "            \n",
    "            # Step 4: Add business metrics\n",
    "            business_result = self._add_business_metrics(enriched_df)\n",
    "            enriched_df = business_result['data']\n",
    "            operations.extend(business_result['operations'])\n",
    "            fields_added += business_result['fields_added']\n",
    "            \n",
    "            # Step 5: Add quality scores\n",
    "            quality_result = self._add_quality_scores(enriched_df)\n",
    "            enriched_df = quality_result['data']\n",
    "            operations.extend(quality_result['operations'])\n",
    "            fields_added += quality_result['fields_added']\n",
    "            \n",
    "            # Create enrichment summary\n",
    "            enrichment_summary = self._create_enrichment_summary(enriched_df)\n",
    "            \n",
    "            # Calculate enrichment time\n",
    "            enrichment_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            result = EnrichmentResult(\n",
    "                success=True,\n",
    "                records_processed=records_processed,\n",
    "                fields_added=fields_added,\n",
    "                enrichment_operations=operations,\n",
    "                enrichment_time=enrichment_time,\n",
    "                data=enriched_df,\n",
    "                enrichment_summary=enrichment_summary\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Data enrichment completed in {enrichment_time:.3f} seconds\")\n",
    "            print(f\"ðŸ“Š Records processed: {records_processed}\")\n",
    "            print(f\"âž• Fields added: {fields_added}\")\n",
    "            print(f\"ðŸ”§ Operations performed: {len(operations)}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            enrichment_time = (datetime.now() - start_time).total_seconds()\n",
    "            print(f\"âŒ Data enrichment failed: {str(e)}\")\n",
    "            \n",
    "            return EnrichmentResult(\n",
    "                success=False,\n",
    "                records_processed=records_processed,\n",
    "                fields_added=0,\n",
    "                enrichment_operations=[],\n",
    "                enrichment_time=enrichment_time,\n",
    "                data=None\n",
    "            )\n",
    "    \n",
    "    def _add_product_information(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Add product information from lookup data\"\"\"\n",
    "        operations = []\n",
    "        fields_added = 0\n",
    "        \n",
    "        if 'product_name' in df.columns:\n",
    "            # Add brand information\n",
    "            brands = []\n",
    "            release_years = []\n",
    "            ratings = []\n",
    "            \n",
    "            for product in df['product_name']:\n",
    "                if pd.isna(product):\n",
    "                    brands.append('Unknown')\n",
    "                    release_years.append(None)\n",
    "                    ratings.append(None)\n",
    "                    continue\n",
    "                \n",
    "                product_str = str(product)\n",
    "                product_info = self.product_data.get(product_str, {})\n",
    "                \n",
    "                brands.append(product_info.get('brand', 'Unknown'))\n",
    "                release_years.append(product_info.get('release_year', None))\n",
    "                ratings.append(product_info.get('avg_rating', None))\n",
    "            \n",
    "            df['product_brand'] = brands\n",
    "            df['product_release_year'] = release_years\n",
    "            df['product_avg_rating'] = ratings\n",
    "            \n",
    "            operations.append(\"Added product_brand, product_release_year, and product_avg_rating\")\n",
    "            fields_added += 3\n",
    "            \n",
    "            # Add product age\n",
    "            current_year = datetime.now().year\n",
    "            product_ages = []\n",
    "            for year in release_years:\n",
    "                if year is None:\n",
    "                    product_ages.append(None)\n",
    "                else:\n",
    "                    product_ages.append(current_year - year)\n",
    "            \n",
    "            df['product_age_years'] = product_ages\n",
    "            operations.append(\"Added product_age_years calculated from release year\")\n",
    "            fields_added += 1\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'fields_added': fields_added\n",
    "        }\n",
    "    \n",
    "    def _add_customer_insights(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Add customer insights and segmentation\"\"\"\n",
    "        operations = []\n",
    "        fields_added = 0\n",
    "        \n",
    "        # Customer segmentation based on order value\n",
    "        if 'total_amount' in df.columns:\n",
    "            customer_segments = []\n",
    "            for amount in df['total_amount']:\n",
    "                if pd.isna(amount):\n",
    "                    customer_segments.append('Unknown')\n",
    "                elif amount < 200:\n",
    "                    customer_segments.append('Budget')\n",
    "                elif amount < 500:\n",
    "                    customer_segments.append('Standard')\n",
    "                elif amount < 1000:\n",
    "                    customer_segments.append('Premium')\n",
    "                else:\n",
    "                    customer_segments.append('VIP')\n",
    "            \n",
    "            df['customer_segment'] = customer_segments\n",
    "            operations.append(\"Added customer_segment based on order value\")\n",
    "            fields_added += 1\n",
    "        \n",
    "        # Customer name analysis\n",
    "        if 'customer_name' in df.columns:\n",
    "            name_lengths = []\n",
    "            has_title = []\n",
    "            \n",
    "            for name in df['customer_name']:\n",
    "                if pd.isna(name):\n",
    "                    name_lengths.append(None)\n",
    "                    has_title.append(False)\n",
    "                else:\n",
    "                    name_str = str(name)\n",
    "                    name_lengths.append(len(name_str))\n",
    "                    has_title.append(any(title in name_str for title in ['Dr.', 'Mr.', 'Ms.', 'Mrs.']))\n",
    "            \n",
    "            df['customer_name_length'] = name_lengths\n",
    "            df['customer_has_title'] = has_title\n",
    "            operations.append(\"Added customer_name_length and customer_has_title\")\n",
    "            fields_added += 2\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'fields_added': fields_added\n",
    "        }\n",
    "    \n",
    "    def _add_temporal_features(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Add temporal features and seasonal analysis\"\"\"\n",
    "        operations = []\n",
    "        fields_added = 0\n",
    "        \n",
    "        if 'order_date' in df.columns:\n",
    "            # Extract date components\n",
    "            df['order_year'] = df['order_date'].dt.year\n",
    "            df['order_month'] = df['order_date'].dt.month\n",
    "            df['order_day'] = df['order_date'].dt.day\n",
    "            df['order_weekday'] = df['order_date'].dt.day_name()\n",
    "            df['order_quarter'] = df['order_date'].dt.quarter\n",
    "            \n",
    "            operations.append(\"Added order_year, order_month, order_day, order_weekday, order_quarter\")\n",
    "            fields_added += 5\n",
    "            \n",
    "            # Add seasonal factors\n",
    "            seasonal_factors = []\n",
    "            for month in df['order_month']:\n",
    "                if pd.isna(month):\n",
    "                    seasonal_factors.append(1.0)\n",
    "                else:\n",
    "                    seasonal_factors.append(self.seasonal_factors.get(int(month), 1.0))\n",
    "            \n",
    "            df['seasonal_factor'] = seasonal_factors\n",
    "            operations.append(\"Added seasonal_factor based on order month\")\n",
    "            fields_added += 1\n",
    "            \n",
    "            # Add days since order\n",
    "            current_date = datetime.now()\n",
    "            days_since_order = []\n",
    "            for order_date in df['order_date']:\n",
    "                if pd.isna(order_date):\n",
    "                    days_since_order.append(None)\n",
    "                else:\n",
    "                    delta = current_date - order_date.to_pydatetime()\n",
    "                    days_since_order.append(delta.days)\n",
    "            \n",
    "            df['days_since_order'] = days_since_order\n",
    "            operations.append(\"Added days_since_order calculated from current date\")\n",
    "            fields_added += 1\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'fields_added': fields_added\n",
    "        }\n",
    "    \n",
    "    def _add_business_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Add business intelligence metrics\"\"\"\n",
    "        operations = []\n",
    "        fields_added = 0\n",
    "        \n",
    "        # Calculate profit margin (simplified)\n",
    "        if 'total_amount' in df.columns:\n",
    "            # Assume 30% profit margin\n",
    "            df['estimated_profit'] = df['total_amount'] * 0.30\n",
    "            operations.append(\"Added estimated_profit (30% of total_amount)\")\n",
    "            fields_added += 1\n",
    "        \n",
    "        # Add order priority based on multiple factors\n",
    "        if all(col in df.columns for col in ['total_amount', 'customer_segment', 'product_avg_rating']):\n",
    "            priorities = []\n",
    "            for _, row in df.iterrows():\n",
    "                priority_score = 0\n",
    "                \n",
    "                # Amount factor\n",
    "                if not pd.isna(row['total_amount']):\n",
    "                    if row['total_amount'] > 1000:\n",
    "                        priority_score += 3\n",
    "                    elif row['total_amount'] > 500:\n",
    "                        priority_score += 2\n",
    "                    else:\n",
    "                        priority_score += 1\n",
    "                \n",
    "                # Customer segment factor\n",
    "                segment_scores = {'VIP': 3, 'Premium': 2, 'Standard': 1, 'Budget': 0}\n",
    "                priority_score += segment_scores.get(row.get('customer_segment', 'Budget'), 0)\n",
    "                \n",
    "                # Product rating factor\n",
    "                if not pd.isna(row.get('product_avg_rating')):\n",
    "                    if row['product_avg_rating'] >= 4.5:\n",
    "                        priority_score += 2\n",
    "                    elif row['product_avg_rating'] >= 4.0:\n",
    "                        priority_score += 1\n",
    "                \n",
    "                # Convert to priority level\n",
    "                if priority_score >= 7:\n",
    "                    priorities.append('Critical')\n",
    "                elif priority_score >= 5:\n",
    "                    priorities.append('High')\n",
    "                elif priority_score >= 3:\n",
    "                    priorities.append('Medium')\n",
    "                else:\n",
    "                    priorities.append('Low')\n",
    "            \n",
    "            df['order_priority'] = priorities\n",
    "            operations.append(\"Added order_priority based on amount, segment, and rating\")\n",
    "            fields_added += 1\n",
    "        \n",
    "        # Add risk score\n",
    "        risk_scores = []\n",
    "        for _, row in df.iterrows():\n",
    "            risk_score = 0\n",
    "            \n",
    "            # High value orders have higher risk\n",
    "            if not pd.isna(row.get('total_amount')) and row['total_amount'] > 1000:\n",
    "                risk_score += 2\n",
    "            \n",
    "            # New products have higher risk\n",
    "            if not pd.isna(row.get('product_age_years')) and row['product_age_years'] < 1:\n",
    "                risk_score += 1\n",
    "            \n",
    "            # Business customers have lower risk\n",
    "            if row.get('customer_type') == 'Business':\n",
    "                risk_score -= 1\n",
    "            \n",
    "            # Convert to risk category\n",
    "            if risk_score >= 3:\n",
    "                risk_scores.append('High')\n",
    "            elif risk_score >= 1:\n",
    "                risk_scores.append('Medium')\n",
    "            else:\n",
    "                risk_scores.append('Low')\n",
    "        \n",
    "        df['risk_score'] = risk_scores\n",
    "        operations.append(\"Added risk_score based on multiple risk factors\")\n",
    "        fields_added += 1\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'fields_added': fields_added\n",
    "        }\n",
    "    \n",
    "    def _add_quality_scores(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Add data quality scores for each record\"\"\"\n",
    "        operations = []\n",
    "        fields_added = 0\n",
    "        \n",
    "        # Calculate completeness score for each record\n",
    "        completeness_scores = []\n",
    "        required_fields = ['order_id', 'customer_name', 'product_name', 'quantity', 'price']\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            filled_fields = 0\n",
    "            for field in required_fields:\n",
    "                if field in row.index and pd.notna(row[field]) and str(row[field]).strip() != '':\n",
    "                    filled_fields += 1\n",
    "            \n",
    "            completeness_score = (filled_fields / len(required_fields)) * 100\n",
    "            completeness_scores.append(completeness_score)\n",
    "        \n",
    "        df['data_completeness_score'] = completeness_scores\n",
    "        operations.append(\"Added data_completeness_score for each record\")\n",
    "        fields_added += 1\n",
    "        \n",
    "        # Add overall data quality grade\n",
    "        quality_grades = []\n",
    "        for score in completeness_scores:\n",
    "            if score >= 95:\n",
    "                quality_grades.append('A')\n",
    "            elif score >= 85:\n",
    "                quality_grades.append('B')\n",
    "            elif score >= 70:\n",
    "                quality_grades.append('C')\n",
    "            elif score >= 50:\n",
    "                quality_grades.append('D')\n",
    "            else:\n",
    "                quality_grades.append('F')\n",
    "        \n",
    "        df['data_quality_grade'] = quality_grades\n",
    "        operations.append(\"Added data_quality_grade based on completeness score\")\n",
    "        fields_added += 1\n",
    "        \n",
    "        return {\n",
    "            'data': df,\n",
    "            'operations': operations,\n",
    "            'fields_added': fields_added\n",
    "        }\n",
    "    \n",
    "    def _create_enrichment_summary(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Create summary of enrichment results\"\"\"\n",
    "        summary = {\n",
    "            'total_records': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'customer_segments': df['customer_segment'].value_counts().to_dict() if 'customer_segment' in df.columns else {},\n",
    "            'product_brands': df['product_brand'].value_counts().to_dict() if 'product_brand' in df.columns else {},\n",
    "            'order_priorities': df['order_priority'].value_counts().to_dict() if 'order_priority' in df.columns else {},\n",
    "            'risk_distribution': df['risk_score'].value_counts().to_dict() if 'risk_score' in df.columns else {},\n",
    "            'quality_grades': df['data_quality_grade'].value_counts().to_dict() if 'data_quality_grade' in df.columns else {},\n",
    "            'avg_completeness_score': df['data_completeness_score'].mean() if 'data_completeness_score' in df.columns else 0\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Test the data enricher\n",
    "enricher = DataEnricher()\n",
    "enrichment_result = enricher.enrich_dataset(df_standardized)\n",
    "\n",
    "if enrichment_result.success:\n",
    "    print(f\"\\nðŸŽ‰ Data enrichment successful!\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Enrichment Operations:\")\n",
    "    for i, operation in enumerate(enrichment_result.enrichment_operations, 1):\n",
    "        print(f\"  {i}. {operation}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Enrichment Summary:\")\n",
    "    summary = enrichment_result.enrichment_summary\n",
    "    print(f\"  Total records: {summary['total_records']}\")\n",
    "    print(f\"  Total columns: {summary['total_columns']}\")\n",
    "    print(f\"  Average completeness score: {summary['avg_completeness_score']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nðŸ‘¥ Customer Segments:\")\n",
    "    for segment, count in summary['customer_segments'].items():\n",
    "        print(f\"  {segment}: {count}\")\n",
    "    \n",
    "    print(f\"\\nðŸ·ï¸ Product Brands:\")\n",
    "    for brand, count in summary['product_brands'].items():\n",
    "        print(f\"  {brand}: {count}\")\n",
    "    \n",
    "    print(f\"\\nâš¡ Order Priorities:\")\n",
    "    for priority, count in summary['order_priorities'].items():\n",
    "        print(f\"  {priority}: {count}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Enriched Data Sample:\")\n",
    "    # Show key enriched columns\n",
    "    key_columns = ['order_id', 'customer_name', 'product_name', 'total_amount', \n",
    "                   'customer_segment', 'product_brand', 'order_priority', 'data_quality_grade']\n",
    "    available_columns = [col for col in key_columns if col in enrichment_result.data.columns]\n",
    "    display(enrichment_result.data[available_columns].head())\n",
    "    \n",
    "    # Store enriched data for final analysis\n",
    "    df_enriched = enrichment_result.data\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Data enrichment failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Complete Transformation Pipeline\n",
    "\n",
    "Let's put everything together into a complete, reusable transformation pipeline that you can use in production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete transformation pipeline\n",
    "print(\"ðŸ”„ Complete Data Transformation Pipeline\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "@dataclass\n",
    "class TransformationResult:\n",
    "    \"\"\"Container for complete transformation results\"\"\"\n",
    "    success: bool\n",
    "    pipeline_name: str\n",
    "    original_records: int\n",
    "    final_records: int\n",
    "    original_columns: int\n",
    "    final_columns: int\n",
    "    total_time: float\n",
    "    cleaning_result: Optional[CleaningResult] = None\n",
    "    standardization_result: Optional[StandardizationResult] = None\n",
    "    enrichment_result: Optional[EnrichmentResult] = None\n",
    "    final_data: Optional[pd.DataFrame] = None\n",
    "    transformation_summary: Dict[str, Any] = None\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    \"\"\"Complete data transformation pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str = \"data_transformation_pipeline\"):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        \n",
    "        # Initialize components\n",
    "        self.cleaner = DataCleaner()\n",
    "        self.standardizer = DataStandardizer()\n",
    "        self.enricher = DataEnricher()\n",
    "        \n",
    "        # Pipeline configuration\n",
    "        self.enable_cleaning = True\n",
    "        self.enable_standardization = True\n",
    "        self.enable_enrichment = True\n",
    "        \n",
    "        print(f\"ðŸ”„ Transformation pipeline '{pipeline_name}' initialized\")\n",
    "        print(f\"  Components: Cleaning, Standardization, Enrichment\")\n",
    "    \n",
    "    def transform_dataset(self, df: pd.DataFrame, \n",
    "                         enable_cleaning: bool = True,\n",
    "                         enable_standardization: bool = True,\n",
    "                         enable_enrichment: bool = True) -> TransformationResult:\n",
    "        \"\"\"\n",
    "        Execute complete transformation pipeline\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset to transform\n",
    "            enable_cleaning (bool): Enable cleaning stage\n",
    "            enable_standardization (bool): Enable standardization stage\n",
    "            enable_enrichment (bool): Enable enrichment stage\n",
    "        \n",
    "        Returns:\n",
    "            TransformationResult: Complete transformation results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        original_records = len(df)\n",
    "        original_columns = len(df.columns)\n",
    "        \n",
    "        print(f\"ðŸš€ Starting transformation pipeline: {self.pipeline_name}\")\n",
    "        print(f\"ðŸ“Š Input dataset: {original_records} records, {original_columns} columns\")\n",
    "        print(f\"ðŸ”§ Stages enabled: Cleaning={enable_cleaning}, Standardization={enable_standardization}, Enrichment={enable_enrichment}\")\n",
    "        \n",
    "        try:\n",
    "            current_data = df.copy()\n",
    "            cleaning_result = None\n",
    "            standardization_result = None\n",
    "            enrichment_result = None\n",
    "            \n",
    "            # Stage 1: Data Cleaning\n",
    "            if enable_cleaning:\n",
    "                print(f\"\\nðŸ§½ Stage 1: Data Cleaning\")\n",
    "                print(f\"  Input: {len(current_data)} records, {len(current_data.columns)} columns\")\n",
    "                \n",
    "                cleaning_result = self.cleaner.clean_dataset(current_data)\n",
    "                \n",
    "                if cleaning_result.success:\n",
    "                    current_data = cleaning_result.data\n",
    "                    print(f\"  âœ… Cleaning completed: {len(current_data)} records, {len(current_data.columns)} columns\")\n",
    "                    print(f\"  ðŸ”§ Operations: {len(cleaning_result.cleaning_operations)}\")\n",
    "                    print(f\"  âœ… Issues fixed: {len(cleaning_result.issues_fixed)}\")\n",
    "                else:\n",
    "                    raise Exception(f\"Data cleaning failed: {cleaning_result.issues_found}\")\n",
    "            else:\n",
    "                print(f\"\\nâ­ï¸ Stage 1: Data Cleaning (Skipped)\")\n",
    "            \n",
    "            # Stage 2: Data Standardization\n",
    "            if enable_standardization:\n",
    "                print(f\"\\nðŸ“ Stage 2: Data Standardization\")\n",
    "                print(f\"  Input: {len(current_data)} records, {len(current_data.columns)} columns\")\n",
    "                \n",
    "                standardization_result = self.standardizer.standardize_dataset(current_data)\n",
    "                \n",
    "                if standardization_result.success:\n",
    "                    current_data = standardization_result.data\n",
    "                    print(f\"  âœ… Standardization completed: {len(current_data)} records, {len(current_data.columns)} columns\")\n",
    "                    print(f\"  ðŸ“ Fields standardized: {standardization_result.fields_standardized}\")\n",
    "                else:\n",
    "                    raise Exception(\"Data standardization failed\")\n",
    "            else:\n",
    "                print(f\"\\nâ­ï¸ Stage 2: Data Standardization (Skipped)\")\n",
    "            \n",
    "            # Stage 3: Data Enrichment\n",
    "            if enable_enrichment:\n",
    "                print(f\"\\nâž• Stage 3: Data Enrichment\")\n",
    "                print(f\"  Input: {len(current_data)} records, {len(current_data.columns)} columns\")\n",
    "                \n",
    "                enrichment_result = self.enricher.enrich_dataset(current_data)\n",
    "                \n",
    "                if enrichment_result.success:\n",
    "                    current_data = enrichment_result.data\n",
    "                    print(f\"  âœ… Enrichment completed: {len(current_data)} records, {len(current_data.columns)} columns\")\n",
    "                    print(f\"  âž• Fields added: {enrichment_result.fields_added}\")\n",
    "                else:\n",
    "                    raise Exception(\"Data enrichment failed\")\n",
    "            else:\n",
    "                print(f\"\\nâ­ï¸ Stage 3: Data Enrichment (Skipped)\")\n",
    "            \n",
    "            # Calculate final metrics\n",
    "            total_time = (datetime.now() - start_time).total_seconds()\n",
    "            final_records = len(current_data)\n",
    "            final_columns = len(current_data.columns)\n",
    "            \n",
    "            # Create transformation summary\n",
    "            transformation_summary = self._create_transformation_summary(\n",
    "                df, current_data, cleaning_result, standardization_result, enrichment_result\n",
    "            )\n",
    "            \n",
    "            result = TransformationResult(\n",
    "                success=True,\n",
    "                pipeline_name=self.pipeline_name,\n",
    "                original_records=original_records,\n",
    "                final_records=final_records,\n",
    "                original_columns=original_columns,\n",
    "                final_columns=final_columns,\n",
    "                total_time=total_time,\n",
    "                cleaning_result=cleaning_result,\n",
    "                standardization_result=standardization_result,\n",
    "                enrichment_result=enrichment_result,\n",
    "                final_data=current_data,\n",
    "                transformation_summary=transformation_summary\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nðŸŽ‰ Transformation pipeline completed successfully!\")\n",
    "            print(f\"â±ï¸ Total time: {total_time:.3f} seconds\")\n",
    "            print(f\"ðŸ“Š Final dataset: {final_records} records, {final_columns} columns\")\n",
    "            print(f\"ðŸ“ˆ Records retained: {(final_records/original_records)*100:.1f}%\")\n",
    "            print(f\"âž• Columns added: {final_columns - original_columns}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_time = (datetime.now() - start_time).total_seconds()\n",
    "            print(f\"\\nâŒ Transformation pipeline failed: {str(e)}\")\n",
    "            \n",
    "            return TransformationResult(\n",
    "                success=False,\n",
    "                pipeline_name=self.pipeline_name,\n",
    "                original_records=original_records,\n",
    "                final_records=0,\n",
    "                original_columns=original_columns,\n",
    "                final_columns=0,\n",
    "                total_time=total_time,\n",
    "                final_data=None\n",
    "            )\n",
    "    \n",
    "    def _create_transformation_summary(self, original_df: pd.DataFrame, \n",
    "                                     final_df: pd.DataFrame,\n",
    "                                     cleaning_result: Optional[CleaningResult],\n",
    "                                     standardization_result: Optional[StandardizationResult],\n",
    "                                     enrichment_result: Optional[EnrichmentResult]) -> Dict[str, Any]:\n",
    "        \"\"\"Create comprehensive transformation summary\"\"\"\n",
    "        \n",
    "        summary = {\n",
    "            'pipeline_name': self.pipeline_name,\n",
    "            'transformation_timestamp': datetime.now().isoformat(),\n",
    "            'data_flow': {\n",
    "                'original_records': len(original_df),\n",
    "                'final_records': len(final_df),\n",
    "                'records_retained_pct': (len(final_df) / len(original_df)) * 100,\n",
    "                'original_columns': len(original_df.columns),\n",
    "                'final_columns': len(final_df.columns),\n",
    "                'columns_added': len(final_df.columns) - len(original_df.columns)\n",
    "            },\n",
    "            'stage_results': {}\n",
    "        }\n",
    "        \n",
    "        # Add stage-specific results\n",
    "        if cleaning_result:\n",
    "            summary['stage_results']['cleaning'] = {\n",
    "                'success': cleaning_result.success,\n",
    "                'operations_count': len(cleaning_result.cleaning_operations),\n",
    "                'issues_fixed_count': len(cleaning_result.issues_fixed),\n",
    "                'time_seconds': cleaning_result.cleaning_time\n",
    "            }\n",
    "        \n",
    "        if standardization_result:\n",
    "            summary['stage_results']['standardization'] = {\n",
    "                'success': standardization_result.success,\n",
    "                'fields_standardized': standardization_result.fields_standardized,\n",
    "                'operations_count': len(standardization_result.standardization_operations),\n",
    "                'time_seconds': standardization_result.standardization_time\n",
    "            }\n",
    "        \n",
    "        if enrichment_result:\n",
    "            summary['stage_results']['enrichment'] = {\n",
    "                'success': enrichment_result.success,\n",
    "                'fields_added': enrichment_result.fields_added,\n",
    "                'operations_count': len(enrichment_result.enrichment_operations),\n",
    "                'time_seconds': enrichment_result.enrichment_time,\n",
    "                'avg_completeness_score': enrichment_result.enrichment_summary.get('avg_completeness_score', 0)\n",
    "            }\n",
    "        \n",
    "        # Add data quality metrics\n",
    "        if 'data_quality_grade' in final_df.columns:\n",
    "            summary['data_quality'] = {\n",
    "                'quality_grades': final_df['data_quality_grade'].value_counts().to_dict(),\n",
    "                'avg_completeness': final_df['data_completeness_score'].mean() if 'data_completeness_score' in final_df.columns else 0\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def generate_transformation_report(self, result: TransformationResult) -> str:\n",
    "        \"\"\"Generate comprehensive transformation report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"# ðŸ”„ Data Transformation Pipeline Report\")\n",
    "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        report.append(\"## ðŸŽ¯ Executive Summary\")\n",
    "        report.append(\"\")\n",
    "        status = \"âœ… SUCCESS\" if result.success else \"âŒ FAILED\"\n",
    "        report.append(f\"**Pipeline Status:** {status}\")\n",
    "        report.append(f\"**Pipeline Name:** {result.pipeline_name}\")\n",
    "        report.append(f\"**Total Execution Time:** {result.total_time:.3f} seconds\")\n",
    "        report.append(f\"**Records Processed:** {result.original_records:,} â†’ {result.final_records:,}\")\n",
    "        report.append(f\"**Columns:** {result.original_columns} â†’ {result.final_columns} (+{result.final_columns - result.original_columns})\")\n",
    "        report.append(f\"**Data Retention Rate:** {(result.final_records/result.original_records)*100:.1f}%\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Stage Results\n",
    "        report.append(\"## ðŸ”§ Stage Results\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        if result.cleaning_result:\n",
    "            cleaning = result.cleaning_result\n",
    "            status_icon = \"âœ…\" if cleaning.success else \"âŒ\"\n",
    "            report.append(f\"### {status_icon} Data Cleaning\")\n",
    "            report.append(f\"- **Execution Time:** {cleaning.cleaning_time:.3f} seconds\")\n",
    "            report.append(f\"- **Operations Performed:** {len(cleaning.cleaning_operations)}\")\n",
    "            report.append(f\"- **Issues Fixed:** {len(cleaning.issues_fixed)}\")\n",
    "            report.append(f\"- **Records Retained:** {cleaning.cleaned_records}/{cleaning.original_records}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        if result.standardization_result:\n",
    "            standardization = result.standardization_result\n",
    "            status_icon = \"âœ…\" if standardization.success else \"âŒ\"\n",
    "            report.append(f\"### {status_icon} Data Standardization\")\n",
    "            report.append(f\"- **Execution Time:** {standardization.standardization_time:.3f} seconds\")\n",
    "            report.append(f\"- **Fields Standardized:** {standardization.fields_standardized}\")\n",
    "            report.append(f\"- **Operations Performed:** {len(standardization.standardization_operations)}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        if result.enrichment_result:\n",
    "            enrichment = result.enrichment_result\n",
    "            status_icon = \"âœ…\" if enrichment.success else \"âŒ\"\n",
    "            report.append(f\"### {status_icon} Data Enrichment\")\n",
    "            report.append(f\"- **Execution Time:** {enrichment.enrichment_time:.3f} seconds\")\n",
    "            report.append(f\"- **Fields Added:** {enrichment.fields_added}\")\n",
    "            report.append(f\"- **Operations Performed:** {len(enrichment.enrichment_operations)}\")\n",
    "            if enrichment.enrichment_summary:\n",
    "                avg_completeness = enrichment.enrichment_summary.get('avg_completeness_score', 0)\n",
    "                report.append(f\"- **Average Data Completeness:** {avg_completeness:.1f}%\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Data Quality Summary\n",
    "        if result.final_data is not None and 'data_quality_grade' in result.final_data.columns:\n",
    "            report.append(\"## ðŸ“Š Data Quality Summary\")\n",
    "            report.append(\"\")\n",
    "            quality_grades = result.final_data['data_quality_grade'].value_counts()\n",
    "            for grade, count in quality_grades.items():\n",
    "                percentage = (count / len(result.final_data)) * 100\n",
    "                report.append(f\"- **Grade {grade}:** {count} records ({percentage:.1f}%)\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(\"## ðŸ’¡ Recommendations\")\n",
    "        report.append(\"\")\n",
    "        if result.success:\n",
    "            report.append(\"- âœ… **Transformation completed successfully**\")\n",
    "            report.append(\"- ðŸ“Š **Monitor data quality trends over time**\")\n",
    "            report.append(\"- ðŸ”„ **Consider automating this pipeline for regular execution**\")\n",
    "            report.append(\"- ðŸ“ˆ **Use enriched data for advanced analytics and ML**\")\n",
    "        else:\n",
    "            report.append(\"- âŒ **Review and fix transformation errors**\")\n",
    "            report.append(\"- ðŸ” **Check input data quality and format**\")\n",
    "            report.append(\"- ðŸ› ï¸ **Update transformation rules as needed**\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        report.append(\"---\")\n",
    "        report.append(f\"*Report generated by Data Transformation Pipeline v1.0*\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Test the complete transformation pipeline\n",
    "print(\"\\nðŸ§ª Testing Complete Transformation Pipeline\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DataTransformationPipeline(\"E-commerce Order Transformation\")\n",
    "\n",
    "# Run complete transformation\n",
    "transformation_result = pipeline.transform_dataset(\n",
    "    df_messy,\n",
    "    enable_cleaning=True,\n",
    "    enable_standardization=True,\n",
    "    enable_enrichment=True\n",
    ")\n",
    "\n",
    "if transformation_result.success:\n",
    "    print(f\"\\nðŸ“Š Transformation Summary:\")\n",
    "    summary = transformation_result.transformation_summary\n",
    "    \n",
    "    print(f\"  Pipeline: {summary['pipeline_name']}\")\n",
    "    print(f\"  Records: {summary['data_flow']['original_records']} â†’ {summary['data_flow']['final_records']}\")\n",
    "    print(f\"  Columns: {summary['data_flow']['original_columns']} â†’ {summary['data_flow']['final_columns']}\")\n",
    "    print(f\"  Retention Rate: {summary['data_flow']['records_retained_pct']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Stage Performance:\")\n",
    "    for stage, results in summary['stage_results'].items():\n",
    "        status = \"âœ…\" if results['success'] else \"âŒ\"\n",
    "        print(f\"  {status} {stage.title()}: {results['time_seconds']:.3f}s\")\n",
    "    \n",
    "    # Show final transformed data\n",
    "    print(f\"\\nðŸ“‹ Final Transformed Data (Sample):\")\n",
    "    key_columns = ['order_id', 'customer_name', 'product_name', 'total_amount', \n",
    "                   'customer_segment', 'product_brand', 'order_priority', 'data_quality_grade']\n",
    "    available_columns = [col for col in key_columns if col in transformation_result.final_data.columns]\n",
    "    display(transformation_result.final_data[available_columns].head())\n",
    "    \n",
    "    # Generate and display report\n",
    "    report = pipeline.generate_transformation_report(transformation_result)\n",
    "    print(f\"\\nðŸ“‹ Transformation Report Generated ({len(report):,} characters)\")\n",
    "    \n",
    "    # Show report preview\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(report[:1000] + \"...\" if len(report) > 1000 else report)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Transformation pipeline failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Before vs After Analysis\n",
    "\n",
    "Let's create a comprehensive comparison between our original messy data and the final transformed data to see the impact of our transformation pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before vs After analysis\n",
    "print(\"ðŸ“Š Before vs After Transformation Analysis\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def analyze_transformation_impact(original_df: pd.DataFrame, \n",
    "                                transformed_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the impact of data transformation\n",
    "    \n",
    "    Args:\n",
    "        original_df (pd.DataFrame): Original messy data\n",
    "        transformed_df (pd.DataFrame): Transformed clean data\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: Analysis results\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'data_structure': {},\n",
    "        'data_quality': {},\n",
    "        'data_richness': {},\n",
    "        'business_value': {}\n",
    "    }\n",
    "    \n",
    "    # Data Structure Analysis\n",
    "    analysis['data_structure'] = {\n",
    "        'records': {\n",
    "            'before': len(original_df),\n",
    "            'after': len(transformed_df),\n",
    "            'change': len(transformed_df) - len(original_df),\n",
    "            'retention_rate': (len(transformed_df) / len(original_df)) * 100\n",
    "        },\n",
    "        'columns': {\n",
    "            'before': len(original_df.columns),\n",
    "            'after': len(transformed_df.columns),\n",
    "            'added': len(transformed_df.columns) - len(original_df.columns),\n",
    "            'new_columns': list(set(transformed_df.columns) - set(original_df.columns))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Data Quality Analysis\n",
    "    original_missing = original_df.isnull().sum().sum()\n",
    "    transformed_missing = transformed_df.isnull().sum().sum()\n",
    "    \n",
    "    analysis['data_quality'] = {\n",
    "        'missing_values': {\n",
    "            'before': original_missing,\n",
    "            'after': transformed_missing,\n",
    "            'improvement': original_missing - transformed_missing\n",
    "        },\n",
    "        'completeness': {\n",
    "            'before': ((original_df.size - original_missing) / original_df.size) * 100,\n",
    "            'after': ((transformed_df.size - transformed_missing) / transformed_df.size) * 100\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add quality grades if available\n",
    "    if 'data_quality_grade' in transformed_df.columns:\n",
    "        quality_distribution = transformed_df['data_quality_grade'].value_counts().to_dict()\n",
    "        analysis['data_quality']['quality_grades'] = quality_distribution\n",
    "    \n",
    "    # Data Richness Analysis\n",
    "    analysis['data_richness'] = {\n",
    "        'calculated_fields': [],\n",
    "        'enrichment_fields': [],\n",
    "        'standardized_fields': []\n",
    "    }\n",
    "    \n",
    "    # Identify different types of new fields\n",
    "    new_columns = set(transformed_df.columns) - set(original_df.columns)\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if 'total' in col.lower() or 'amount' in col.lower():\n",
    "            analysis['data_richness']['calculated_fields'].append(col)\n",
    "        elif any(word in col.lower() for word in ['brand', 'category', 'segment', 'priority', 'risk']):\n",
    "            analysis['data_richness']['enrichment_fields'].append(col)\n",
    "        else:\n",
    "            analysis['data_richness']['standardized_fields'].append(col)\n",
    "    \n",
    "    # Business Value Analysis\n",
    "    analysis['business_value'] = {\n",
    "        'analytics_ready': True,\n",
    "        'ml_ready': 'data_quality_grade' in transformed_df.columns,\n",
    "        'business_insights': [],\n",
    "        'automation_potential': 'High'\n",
    "    }\n",
    "    \n",
    "    # Identify business insights\n",
    "    if 'customer_segment' in transformed_df.columns:\n",
    "        analysis['business_value']['business_insights'].append('Customer Segmentation Available')\n",
    "    \n",
    "    if 'product_brand' in transformed_df.columns:\n",
    "        analysis['business_value']['business_insights'].append('Brand Analysis Possible')\n",
    "    \n",
    "    if 'order_priority' in transformed_df.columns:\n",
    "        analysis['business_value']['business_insights'].append('Order Prioritization Enabled')\n",
    "    \n",
    "    if 'seasonal_factor' in transformed_df.columns:\n",
    "        analysis['business_value']['business_insights'].append('Seasonal Analysis Available')\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Perform the analysis\n",
    "if transformation_result.success:\n",
    "    impact_analysis = analyze_transformation_impact(df_messy, transformation_result.final_data)\n",
    "    \n",
    "    print(\"ðŸ“Š TRANSFORMATION IMPACT ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Data Structure Impact\n",
    "    print(\"\\nðŸ—ï¸ Data Structure Impact:\")\n",
    "    structure = impact_analysis['data_structure']\n",
    "    print(f\"  Records: {structure['records']['before']} â†’ {structure['records']['after']} ({structure['records']['retention_rate']:.1f}% retained)\")\n",
    "    print(f\"  Columns: {structure['columns']['before']} â†’ {structure['columns']['after']} (+{structure['columns']['added']} new)\")\n",
    "    \n",
    "    # Data Quality Impact\n",
    "    print(\"\\nâœ¨ Data Quality Impact:\")\n",
    "    quality = impact_analysis['data_quality']\n",
    "    print(f\"  Missing Values: {quality['missing_values']['before']} â†’ {quality['missing_values']['after']} ({quality['missing_values']['improvement']} improvement)\")\n",
    "    print(f\"  Completeness: {quality['completeness']['before']:.1f}% â†’ {quality['completeness']['after']:.1f}%\")\n",
    "    \n",
    "    if 'quality_grades' in quality:\n",
    "        print(f\"  Quality Grades:\")\n",
    "        for grade, count in quality['quality_grades'].items():\n",
    "            percentage = (count / len(transformation_result.final_data)) * 100\n",
    "            print(f\"    Grade {grade}: {count} records ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Data Richness Impact\n",
    "    print(\"\\nðŸŽ¯ Data Enrichment Impact:\")\n",
    "    richness = impact_analysis['data_richness']\n",
    "    print(f\"  Calculated Fields: {len(richness['calculated_fields'])} ({', '.join(richness['calculated_fields'][:3])})\")\n",
    "    print(f\"  Enrichment Fields: {len(richness['enrichment_fields'])} ({', '.join(richness['enrichment_fields'][:3])})\")\n",
    "    print(f\"  Standardized Fields: {len(richness['standardized_fields'])} ({', '.join(richness['standardized_fields'][:3])})\")\n",
    "    \n",
    "    # Business Value Impact\n",
    "    print(\"\\nðŸ’¼ Business Value Impact:\")\n",
    "    business = impact_analysis['business_value']\n",
    "    print(f\"  Analytics Ready: {'âœ… Yes' if business['analytics_ready'] else 'âŒ No'}\")\n",
    "    print(f\"  ML Ready: {'âœ… Yes' if business['ml_ready'] else 'âŒ No'}\")\n",
    "    print(f\"  Automation Potential: {business['automation_potential']}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Business Insights Enabled:\")\n",
    "    for insight in business['business_insights']:\n",
    "        print(f\"    âœ… {insight}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Data Structure Comparison\n",
    "    categories = ['Records', 'Columns']\n",
    "    before_values = [structure['records']['before'], structure['columns']['before']]\n",
    "    after_values = [structure['records']['after'], structure['columns']['after']]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, before_values, width, label='Before', alpha=0.7, color='red')\n",
    "    ax1.bar(x + width/2, after_values, width, label='After', alpha=0.7, color='green')\n",
    "    ax1.set_xlabel('Data Structure')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Data Structure: Before vs After')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (before, after) in enumerate(zip(before_values, after_values)):\n",
    "        ax1.text(i - width/2, before + 0.1, str(before), ha='center', va='bottom')\n",
    "        ax1.text(i + width/2, after + 0.1, str(after), ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Data Quality Grades (if available)\n",
    "    if 'quality_grades' in quality:\n",
    "        grades = list(quality['quality_grades'].keys())\n",
    "        counts = list(quality['quality_grades'].values())\n",
    "        colors = ['green', 'lightgreen', 'yellow', 'orange', 'red'][:len(grades)]\n",
    "        \n",
    "        ax2.pie(counts, labels=grades, autopct='%1.1f%%', colors=colors)\n",
    "        ax2.set_title('Data Quality Grade Distribution')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Quality Grades\\nNot Available', ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Data Quality Grades')\n",
    "    \n",
    "    # 3. Customer Segment Distribution (if available)\n",
    "    if 'customer_segment' in transformation_result.final_data.columns:\n",
    "        segment_counts = transformation_result.final_data['customer_segment'].value_counts()\n",
    "        ax3.bar(segment_counts.index, segment_counts.values, alpha=0.7)\n",
    "        ax3.set_xlabel('Customer Segment')\n",
    "        ax3.set_ylabel('Count')\n",
    "        ax3.set_title('Customer Segmentation')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Customer Segmentation\\nNot Available', ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title('Customer Segmentation')\n",
    "    \n",
    "    # 4. Order Priority Distribution (if available)\n",
    "    if 'order_priority' in transformation_result.final_data.columns:\n",
    "        priority_counts = transformation_result.final_data['order_priority'].value_counts()\n",
    "        colors = {'Critical': 'red', 'High': 'orange', 'Medium': 'yellow', 'Low': 'green'}\n",
    "        bar_colors = [colors.get(priority, 'blue') for priority in priority_counts.index]\n",
    "        \n",
    "        ax4.bar(priority_counts.index, priority_counts.values, color=bar_colors, alpha=0.7)\n",
    "        ax4.set_xlabel('Order Priority')\n",
    "        ax4.set_ylabel('Count')\n",
    "        ax4.set_title('Order Priority Distribution')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Order Priority\\nNot Available', ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Order Priority Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show side-by-side comparison\n",
    "    print(f\"\\nðŸ“‹ Side-by-Side Data Comparison:\")\n",
    "    print(f\"\\nðŸ”´ BEFORE (Original Messy Data):\")\n",
    "    display(df_messy.head(3))\n",
    "    \n",
    "    print(f\"\\nðŸŸ¢ AFTER (Transformed Clean Data):\")\n",
    "    key_columns = ['order_id', 'customer_name', 'product_name', 'total_amount', \n",
    "                   'customer_segment', 'product_brand', 'order_priority', 'data_quality_grade']\n",
    "    available_columns = [col for col in key_columns if col in transformation_result.final_data.columns]\n",
    "    display(transformation_result.final_data[available_columns].head(3))\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot perform analysis - transformation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "Congratulations! You've completed the data transformation tutorial. Here's what you've mastered:\n",
    "\n",
    "### âœ… **Core Transformation Skills**\n",
    "- **ðŸ§½ Data Cleaning**: Removing inconsistencies, fixing formats, handling missing values\n",
    "- **ðŸ“ Data Standardization**: Creating consistent formats and structures\n",
    "- **âž• Data Enrichment**: Adding calculated fields, business intelligence, and metadata\n",
    "- **ðŸ”„ Pipeline Orchestration**: Building complete transformation workflows\n",
    "- **ðŸ“Š Impact Analysis**: Measuring transformation effectiveness\n",
    "\n",
    "### âœ… **Advanced Techniques Mastered**\n",
    "- **Text Standardization**: Case normalization, brand name fixing, format consistency\n",
    "- **Numeric Cleaning**: Currency symbol removal, type conversion, validation\n",
    "- **Date Standardization**: Multi-format parsing, ISO standardization\n",
    "- **Phone Number Formatting**: Pattern recognition and standardization\n",
    "- **Business Intelligence**: Customer segmentation, product categorization, risk scoring\n",
    "- **Quality Scoring**: Record-level quality assessment and grading\n",
    "\n",
    "### âœ… **Production-Ready Features**\n",
    "- **Comprehensive Error Handling**: Graceful failure management at every stage\n",
    "- **Performance Monitoring**: Execution time tracking and optimization\n",
    "- **Detailed Reporting**: Before/after analysis and transformation reports\n",
    "- **Configurable Pipelines**: Enable/disable stages based on requirements\n",
    "- **Data Lineage**: Hash-based tracking for data provenance\n",
    "\n",
    "### âœ… **Business Value Created**\n",
    "- **Analytics-Ready Data**: Clean, consistent data for analysis\n",
    "- **ML-Ready Features**: Quality scores and enriched attributes\n",
    "- **Business Intelligence**: Customer segments, product insights, risk assessment\n",
    "- **Operational Efficiency**: Automated data preparation and quality assurance\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ What's Next?\n",
    "\n",
    "In the next tutorial, **\"06_building_complete_pipeline.ipynb\"**, you'll learn:\n",
    "- ðŸ”— How to connect all pipeline components together\n",
    "- âš¡ Building end-to-end automated workflows\n",
    "- ðŸ“Š Advanced monitoring and alerting systems\n",
    "- ðŸ”„ Error recovery and retry mechanisms\n",
    "- ðŸŽ¯ Production deployment strategies\n",
    "\n",
    "### ðŸŽ¯ **Practice Exercise**\n",
    "\n",
    "Before moving to the next tutorial, try this exercise:\n",
    "\n",
    "1. **Create your own messy dataset** for a different domain (healthcare, finance, retail)\n",
    "2. **Identify transformation needs** specific to that domain\n",
    "3. **Customize the transformation pipeline** with domain-specific rules\n",
    "4. **Add custom enrichment logic** relevant to your business case\n",
    "5. **Measure the transformation impact** using our analysis framework\n",
    "6. **Create a transformation report** for stakeholders\n",
    "\n",
    "### ðŸ’¡ **Advanced Transformation Ideas:**\n",
    "- **Fuzzy Matching**: Handle similar but not identical values\n",
    "- **External Data Integration**: Enrich with third-party data sources\n",
    "- **Machine Learning Enhancement**: Use ML for data quality prediction\n",
    "- **Real-time Transformation**: Stream processing for live data\n",
    "- **A/B Testing**: Compare different transformation strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Outstanding work mastering data transformation! ðŸŽ‰**\n",
    "\n",
    "You now have the skills to transform any messy dataset into clean, enriched, analytics-ready data. Data transformation is where raw data becomes valuable business intelligence - you're creating the foundation for all downstream analytics and machine learning.\n",
    "\n",
    "**Happy Transforming! ðŸ”„**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}