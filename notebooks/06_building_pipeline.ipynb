{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 Building a Complete Data Pipeline\n",
    "\n",
    "Welcome to the **sixth tutorial** in our Data Ingestion Pipeline series! In this comprehensive notebook, you'll learn how to orchestrate all the components we've built into a complete, production-ready data pipeline.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ✅ Understand pipeline orchestration and workflow management\n",
    "- ✅ Build a complete end-to-end data pipeline\n",
    "- ✅ Implement error handling and recovery mechanisms\n",
    "- ✅ Add performance monitoring and optimization\n",
    "- ✅ Create comprehensive pipeline reporting\n",
    "- ✅ Test and validate your complete pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Pipeline Architecture Overview\n",
    "\n",
    "Before we start building, let's understand the complete architecture of our data pipeline:\n",
    "\n",
    "```\n",
    "📥 DATA SOURCES          🔄 PIPELINE STAGES           📊 OUTPUTS\n",
    "┌─────────────────┐     ┌──────────────────────┐     ┌─────────────────┐\n",
    "│ 📁 CSV Files    │────▶│ 1️⃣ Data Ingestion    │────▶│ 🗄️ Database     │\n",
    "│ 📄 JSON Files   │     │ 2️⃣ Data Validation   │     │ 📊 Reports      │\n",
    "│ 🌐 REST APIs    │     │ 3️⃣ Data Transformation│     │ 📈 Analytics    │\n",
    "│ 🗄️ Databases    │     │ 4️⃣ Data Storage      │     │ 🚨 Alerts       │\n",
    "└─────────────────┘     │ 5️⃣ Monitoring        │     └─────────────────┘\n",
    "                        └──────────────────────┘\n",
    "```\n",
    "\n",
    "### 🎯 **Pipeline Goals**\n",
    "- **Reliability**: Handle failures gracefully\n",
    "- **Scalability**: Process large volumes of data\n",
    "- **Maintainability**: Easy to understand and modify\n",
    "- **Observability**: Monitor performance and health\n",
    "- **Flexibility**: Support multiple data sources and formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "# Import our pipeline components\n",
    "try:\n",
    "    from pipeline.pipeline_manager import PipelineManager\n",
    "    from ingestion.file_ingestion import FileIngestion\n",
    "    from ingestion.api_ingestion import APIIngestion\n",
    "    from validation.data_validator import DataValidator\n",
    "    from transformation.data_cleaner import DataCleaner\n",
    "    from transformation.data_enricher import DataEnricher\n",
    "    from storage.database_manager import DatabaseManager\n",
    "    from utils.config import config\n",
    "    from utils.helpers import ensure_directory_exists, format_duration\n",
    "    print(\"✅ All pipeline components imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Import error: {e}\")\n",
    "    print(\"📝 Note: Some components may not be available in this demo environment\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 Pipeline building environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Step 1: Pipeline Configuration\n",
    "\n",
    "First, let's set up our pipeline configuration. This includes defining data sources, processing parameters, and output destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Configuration\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration class for our data pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Data source configuration\n",
    "        self.data_sources = {\n",
    "            'csv_files': {\n",
    "                'enabled': True,\n",
    "                'path': '../data/input/csv/',\n",
    "                'pattern': '*.csv',\n",
    "                'encoding': 'utf-8'\n",
    "            },\n",
    "            'json_files': {\n",
    "                'enabled': True,\n",
    "                'path': '../data/input/json/',\n",
    "                'pattern': '*.json'\n",
    "            },\n",
    "            'api_sources': {\n",
    "                'enabled': True,\n",
    "                'endpoints': [\n",
    "                    'https://jsonplaceholder.typicode.com/posts'\n",
    "                ],\n",
    "                'timeout': 30,\n",
    "                'retry_attempts': 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Processing configuration\n",
    "        self.processing = {\n",
    "            'batch_size': 1000,\n",
    "            'max_workers': 4,\n",
    "            'enable_parallel': True,\n",
    "            'validation_threshold': 0.8,  # 80% quality threshold\n",
    "            'enable_enrichment': True,\n",
    "            'enable_standardization': True\n",
    "        }\n",
    "        \n",
    "        # Output configuration\n",
    "        self.outputs = {\n",
    "            'database': {\n",
    "                'enabled': True,\n",
    "                'path': '../data/orders.db',\n",
    "                'table': 'orders'\n",
    "            },\n",
    "            'files': {\n",
    "                'enabled': True,\n",
    "                'output_dir': '../data/output/',\n",
    "                'formats': ['csv', 'json']\n",
    "            },\n",
    "            'reports': {\n",
    "                'enabled': True,\n",
    "                'report_dir': '../data/output/reports/'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Monitoring configuration\n",
    "        self.monitoring = {\n",
    "            'log_level': 'INFO',\n",
    "            'log_file': '../logs/pipeline.log',\n",
    "            'metrics_enabled': True,\n",
    "            'alerts_enabled': False,  # Disable for demo\n",
    "            'performance_tracking': True\n",
    "        }\n",
    "\n",
    "# Initialize configuration\n",
    "pipeline_config = PipelineConfig()\n",
    "\n",
    "print(\"⚙️ Pipeline Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📁 Data Sources: {len(pipeline_config.data_sources)} configured\")\n",
    "print(f\"🔄 Batch Size: {pipeline_config.processing['batch_size']}\")\n",
    "print(f\"👥 Max Workers: {pipeline_config.processing['max_workers']}\")\n",
    "print(f\"📊 Quality Threshold: {pipeline_config.processing['validation_threshold']*100}%\")\n",
    "print(f\"💾 Database Output: {'✅ Enabled' if pipeline_config.outputs['database']['enabled'] else '❌ Disabled'}\")\n",
    "print(f\"📈 Monitoring: {'✅ Enabled' if pipeline_config.monitoring['metrics_enabled'] else '❌ Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 2: Create Sample Data\n",
    "\n",
    "Let's create some sample data to demonstrate our pipeline. In a real scenario, this data would come from your actual sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_directory_exists(directory_path):\n",
    "    \"\"\"Ensure directory exists, create if it doesn't\"\"\"\n",
    "    Path(directory_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample data files for pipeline demonstration\"\"\"\n",
    "\n",
    "    # Get the current working directory and construct paths\n",
    "    current_dir = Path.cwd()\n",
    "\n",
    "    # If we're in notebooks folder, go up one level\n",
    "    if current_dir.name == 'notebooks':\n",
    "        base_dir = current_dir.parent\n",
    "    else:\n",
    "        base_dir = current_dir\n",
    "\n",
    "    # Define data directories\n",
    "    csv_dir = base_dir / 'data' / 'input' / 'csv'\n",
    "    json_dir = base_dir / 'data' / 'input' / 'json'\n",
    "    output_dir = base_dir / 'data' / 'output'\n",
    "    logs_dir = base_dir / 'logs'\n",
    "\n",
    "    # Ensure directories exist\n",
    "    ensure_directory_exists(csv_dir)\n",
    "    ensure_directory_exists(json_dir)\n",
    "    ensure_directory_exists(output_dir)\n",
    "    ensure_directory_exists(logs_dir)\n",
    "\n",
    "    # Sample CSV data (Store orders)\n",
    "    csv_data = {\n",
    "        'order_id': ['ORD-2024-001', 'ORD-2024-002', 'ORD-2024-003', 'ORD-2024-004', 'ORD-2024-005'],\n",
    "        'customer_name': ['John Doe', 'Jane Smith', 'Bob Wilson', 'Alice Johnson', 'Charlie Brown'],\n",
    "        'product': ['iPhone 15', 'MacBook Pro', 'AirPods Pro', 'iPad Air', 'Apple Watch'],\n",
    "        'quantity': [1, 1, 2, 1, 1],\n",
    "        'price': [999.99, 1999.99, 249.99, 599.99, 399.99],\n",
    "        'order_date': ['2024-01-15', '2024-01-15', '2024-01-16', '2024-01-16', '2024-01-17'],\n",
    "        'store_location': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n",
    "        'customer_email': ['john@example.com', 'jane@example.com', 'bob@example.com', 'alice@example.com', 'charlie@example.com']\n",
    "    }\n",
    "\n",
    "    df_csv = pd.DataFrame(csv_data)\n",
    "    csv_file_path = csv_dir / 'store_orders_2024.csv'\n",
    "    df_csv.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    # Sample JSON data (Mobile app orders)\n",
    "    json_data = {\n",
    "        'app_version': '2.1.0',\n",
    "        'upload_time': '2024-01-17T12:00:00Z',\n",
    "        'orders': [\n",
    "            {\n",
    "                'order_id': 'APP-2024-001',\n",
    "                'customer_name': 'Sarah Connor',\n",
    "                'product': 'Nintendo Switch',\n",
    "                'quantity': 1,\n",
    "                'price': 299.99,\n",
    "                'order_date': '2024-01-17T10:30:00Z',\n",
    "                'device_type': 'iOS',\n",
    "                'customer_email': 'sarah@example.com'\n",
    "            },\n",
    "            {\n",
    "                'order_id': 'APP-2024-002',\n",
    "                'customer_name': 'Mike Johnson',\n",
    "                'product': 'PlayStation 5',\n",
    "                'quantity': 1,\n",
    "                'price': 499.99,\n",
    "                'order_date': '2024-01-17T11:15:00Z',\n",
    "                'device_type': 'Android',\n",
    "                'customer_email': 'mike@example.com'\n",
    "            },\n",
    "            {\n",
    "                'order_id': 'APP-2024-003',\n",
    "                'customer_name': 'Lisa Davis',\n",
    "                'product': 'Xbox Series X',\n",
    "                'quantity': 1,\n",
    "                'price': 499.99,\n",
    "                'order_date': '2024-01-17T14:20:00Z',\n",
    "                'device_type': 'iOS',\n",
    "                'customer_email': 'lisa@example.com'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    json_file_path = json_dir / 'mobile_orders_2024.json'\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Created CSV file: {csv_file_path}\")\n",
    "    print(f\"✅ Created JSON file: {json_file_path}\")\n",
    "\n",
    "    return len(csv_data['order_id']), len(json_data['orders'])\n",
    "\n",
    "# Create sample data\n",
    "try:\n",
    "    csv_count, json_count = create_sample_data()\n",
    "\n",
    "    print(\"\\n📊 Sample Data Created:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"📁 CSV Orders: {csv_count}\")\n",
    "    print(f\"📄 JSON Orders: {json_count}\")\n",
    "    print(f\"📈 Total Orders: {csv_count + json_count}\")\n",
    "    print(\"\\n✅ Sample data files created successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating sample data: {e}\")\n",
    "    print(f\"Current working directory: {Path.cwd()}\")\n",
    "    print(\"Please make sure you're running this from the project root or notebooks directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Step 3: Build Pipeline Components\n",
    "\n",
    "Now let's build our pipeline step by step, integrating all the components we've learned about in previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Pipeline Builder Class\n",
    "class DataPipelineBuilder:\n",
    "    \"\"\"Custom pipeline builder for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.pipeline_id = f\"PIPELINE-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.results = {\n",
    "            'ingestion': {},\n",
    "            'validation': {},\n",
    "            'transformation': {},\n",
    "            'storage': {},\n",
    "            'monitoring': {}\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup pipeline logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, self.config.monitoring['log_level']),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.StreamHandler(),\n",
    "                logging.FileHandler(self.config.monitoring['log_file'])\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(f'Pipeline-{self.pipeline_id}')\n",
    "        \n",
    "    def run_ingestion_stage(self):\n",
    "        \"\"\"Run data ingestion stage\"\"\"\n",
    "        self.logger.info(\"🔄 Starting data ingestion stage\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        all_data = []\n",
    "        ingestion_summary = {\n",
    "            'sources_processed': 0,\n",
    "            'total_records': 0,\n",
    "            'successful_sources': 0,\n",
    "            'failed_sources': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        # Process CSV files\n",
    "        if self.config.data_sources['csv_files']['enabled']:\n",
    "            try:\n",
    "                csv_path = self.config.data_sources['csv_files']['path']\n",
    "                csv_files = list(Path(csv_path).glob('*.csv'))\n",
    "                \n",
    "                for csv_file in csv_files:\n",
    "                    try:\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        df['source'] = 'csv_file'\n",
    "                        df['source_file'] = csv_file.name\n",
    "                        df['ingested_at'] = datetime.now().isoformat()\n",
    "                        all_data.append(df)\n",
    "                        \n",
    "                        ingestion_summary['total_records'] += len(df)\n",
    "                        ingestion_summary['successful_sources'] += 1\n",
    "                        self.logger.info(f\"✅ Processed CSV file: {csv_file.name} ({len(df)} records)\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"Failed to process CSV file {csv_file.name}: {str(e)}\"\n",
    "                        ingestion_summary['errors'].append(error_msg)\n",
    "                        ingestion_summary['failed_sources'] += 1\n",
    "                        self.logger.error(f\"❌ {error_msg}\")\n",
    "                        \n",
    "                ingestion_summary['sources_processed'] += len(csv_files)\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"CSV ingestion failed: {str(e)}\"\n",
    "                ingestion_summary['errors'].append(error_msg)\n",
    "                self.logger.error(f\"❌ {error_msg}\")\n",
    "        \n",
    "        # Process JSON files\n",
    "        if self.config.data_sources['json_files']['enabled']:\n",
    "            try:\n",
    "                json_path = self.config.data_sources['json_files']['path']\n",
    "                json_files = list(Path(json_path).glob('*.json'))\n",
    "                \n",
    "                for json_file in json_files:\n",
    "                    try:\n",
    "                        with open(json_file, 'r') as f:\n",
    "                            json_data = json.load(f)\n",
    "                        \n",
    "                        # Extract orders from JSON structure\n",
    "                        if 'orders' in json_data:\n",
    "                            df = pd.DataFrame(json_data['orders'])\n",
    "                        else:\n",
    "                            df = pd.DataFrame([json_data])\n",
    "                        \n",
    "                        df['source'] = 'json_file'\n",
    "                        df['source_file'] = json_file.name\n",
    "                        df['ingested_at'] = datetime.now().isoformat()\n",
    "                        \n",
    "                        # Add JSON metadata if available\n",
    "                        if 'app_version' in json_data:\n",
    "                            df['app_version'] = json_data['app_version']\n",
    "                        \n",
    "                        all_data.append(df)\n",
    "                        \n",
    "                        ingestion_summary['total_records'] += len(df)\n",
    "                        ingestion_summary['successful_sources'] += 1\n",
    "                        self.logger.info(f\"✅ Processed JSON file: {json_file.name} ({len(df)} records)\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"Failed to process JSON file {json_file.name}: {str(e)}\"\n",
    "                        ingestion_summary['errors'].append(error_msg)\n",
    "                        ingestion_summary['failed_sources'] += 1\n",
    "                        self.logger.error(f\"❌ {error_msg}\")\n",
    "                        \n",
    "                ingestion_summary['sources_processed'] += len(json_files)\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"JSON ingestion failed: {str(e)}\"\n",
    "                ingestion_summary['errors'].append(error_msg)\n",
    "                self.logger.error(f\"❌ {error_msg}\")\n",
    "        \n",
    "        # Combine all data\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data, ignore_index=True)\n",
    "            # Remove duplicates based on order_id if present\n",
    "            if 'order_id' in combined_data.columns:\n",
    "                before_dedup = len(combined_data)\n",
    "                combined_data = combined_data.drop_duplicates(subset=['order_id'], keep='first')\n",
    "                after_dedup = len(combined_data)\n",
    "                if before_dedup != after_dedup:\n",
    "                    self.logger.info(f\"🧹 Removed {before_dedup - after_dedup} duplicate records\")\n",
    "        else:\n",
    "            combined_data = pd.DataFrame()\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        ingestion_summary['processing_time'] = stage_time\n",
    "        ingestion_summary['success'] = len(combined_data) > 0\n",
    "        \n",
    "        self.results['ingestion'] = ingestion_summary\n",
    "        \n",
    "        self.logger.info(f\"✅ Ingestion stage completed in {stage_time:.2f}s: {len(combined_data)} records\")\n",
    "        return combined_data\n",
    "    \n",
    "    def run_validation_stage(self, data):\n",
    "        \"\"\"Run data validation stage\"\"\"\n",
    "        self.logger.info(\"🔍 Starting data validation stage\")\n",
    "        stage_start = time.time()\n",
    "        \n",
    "        validation_summary = {\n",
    "            'total_records': len(data),\n",
    "            'valid_records': 0,\n",
    "            'invalid_records': 0,\n",
    "            'quality_score': 0.0,\n",
    "            'errors': [],\n",
    "            'warnings': []\n",
    "        }\n",
    "        \n",
    "        if data.empty:\n",
    "            validation_summary['errors'].append(\"No data to validate\")\n",
    "            validation_summary['success'] = False\n",
    "            self.results['validation'] = validation_summary\n",
    "            return data\n",
    "        \n",
    "        # Required fields validation\n",
    "        required_fields = ['order_id', 'customer_name', 'product', 'quantity', 'price']\n",
    "        missing_fields = [field for field in required_fields if field not in data.columns]\n",
    "        \n",
    "        if missing_fields:\n",
    "            validation_summary['errors'].append(f\"Missing required fields: {missing_fields}\")\n",
    "        \n",
    "        # Data quality checks\n",
    "        valid_record_count = 0\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            record_valid = True\n",
    "            \n",
    "            # Check required fields are not empty\n",
    "            for field in required_fields:\n",
    "                if field in row and (pd.isna(row[field]) or str(row[field]).strip() == ''):\n",
    "                    validation_summary['errors'].append(f\"Row {index}: Missing {field}\")\n",
    "                    record_valid = False\n",
    "            \n",
    "            # Business rule validations\n",
    "            if 'quantity' in row and pd.notna(row['quantity']):\n",
    "                try:\n",
    "                    quantity = float(row['quantity'])\n",
    "                    if quantity <= 0:\n",
    "                        validation_summary['errors'].append(f\"Row {index}: Invalid quantity {quantity}\")\n",
    "                        record_valid = False\n",
    "                except (ValueError, TypeError):\n",
    "                    validation_summary['errors'].append(f\"Row {index}: Non-numeric quantity\")\n",
    "                    record_valid = False\n",
    "            \n",
    "            if 'price' in row and pd.notna(row['price']):\n",
    "                try:\n",
    "                    price = float(row['price'])\n",
    "                    if price <= 0:\n",
    "                        validation_summary['errors'].append(f\"Row {index}: Invalid price {price}\")\n",
    "                        record_valid = False\n",
    "                except (ValueError, TypeError):\n",
    "                    validation_summary['errors'].append(f\"Row {index}: Non-numeric price\")\n",
    "                    record_valid = False\n",
    "            \n",
    "            if record_valid:\n",
    "                valid_record_count += 1\n",
    "        \n",
    "        validation_summary['valid_records'] = valid_record_count\n",
    "        validation_summary['invalid_records'] = len(data) - valid_record_count\n",
    "        validation_summary['quality_score'] = (valid_record_count / len(data)) * 100 if len(data) > 0 else 0\n",
    "        \n",
    "        # Check if quality meets threshold\n",
    "        quality_threshold = self.config.processing['validation_threshold'] * 100\n",
    "        validation_summary['meets_threshold'] = validation_summary['quality_score'] >= quality_threshold\n",
    "        validation_summary['success'] = validation_summary['meets_threshold']\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        validation_summary['processing_time'] = stage_time\n",
    "        \n",
    "        self.results['validation'] = validation_summary\n",
    "        \n",
    "        self.logger.info(f\"✅ Validation stage completed in {stage_time:.2f}s: {validation_summary['quality_score']:.1f}% quality\")\n",
    "        \n",
    "        if not validation_summary['meets_threshold']:\n",
    "            self.logger.warning(f\"⚠️ Data quality ({validation_summary['quality_score']:.1f}%) below threshold ({quality_threshold}%)\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "# Initialize pipeline builder\n",
    "pipeline_builder = DataPipelineBuilder(pipeline_config)\n",
    "print(f\"🔧 Pipeline Builder initialized: {pipeline_builder.pipeline_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Step 4: Data Transformation Stage\n",
    "\n",
    "Let's add the transformation stage to clean, standardize, and enrich our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add transformation methods to our pipeline builder\n",
    "def run_transformation_stage(self, data):\n",
    "    \"\"\"Run data transformation stage\"\"\"\n",
    "    self.logger.info(\"🧹 Starting data transformation stage\")\n",
    "    stage_start = time.time()\n",
    "    \n",
    "    transformation_summary = {\n",
    "        'original_records': len(data),\n",
    "        'transformed_records': 0,\n",
    "        'operations_performed': [],\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    if data.empty:\n",
    "        transformation_summary['errors'].append(\"No data to transform\")\n",
    "        transformation_summary['success'] = False\n",
    "        self.results['transformation'] = transformation_summary\n",
    "        return data\n",
    "    \n",
    "    transformed_data = data.copy()\n",
    "    \n",
    "    try:\n",
    "        # 1. Data Cleaning\n",
    "        # Standardize customer names\n",
    "        if 'customer_name' in transformed_data.columns:\n",
    "            transformed_data['customer_name'] = transformed_data['customer_name'].str.strip().str.title()\n",
    "            transformation_summary['operations_performed'].append('Standardized customer names')\n",
    "        \n",
    "        # Clean product names\n",
    "        if 'product' in transformed_data.columns:\n",
    "            product_mapping = {\n",
    "                'iphone 15': 'iPhone 15',\n",
    "                'macbook pro': 'MacBook Pro',\n",
    "                'airpods pro': 'AirPods Pro',\n",
    "                'ipad air': 'iPad Air',\n",
    "                'apple watch': 'Apple Watch',\n",
    "                'nintendo switch': 'Nintendo Switch',\n",
    "                'playstation 5': 'PlayStation 5',\n",
    "                'xbox series x': 'Xbox Series X'\n",
    "            }\n",
    "            \n",
    "            transformed_data['product'] = transformed_data['product'].str.lower()\n",
    "            for old_name, new_name in product_mapping.items():\n",
    "                transformed_data['product'] = transformed_data['product'].replace(old_name, new_name)\n",
    "            transformation_summary['operations_performed'].append('Standardized product names')\n",
    "        \n",
    "        # 2. Data Type Conversion\n",
    "        if 'quantity' in transformed_data.columns:\n",
    "            transformed_data['quantity'] = pd.to_numeric(transformed_data['quantity'], errors='coerce')\n",
    "            transformed_data['quantity'] = transformed_data['quantity'].fillna(1).astype(int)\n",
    "            transformation_summary['operations_performed'].append('Fixed quantity data types')\n",
    "        \n",
    "        if 'price' in transformed_data.columns:\n",
    "            transformed_data['price'] = pd.to_numeric(transformed_data['price'], errors='coerce')\n",
    "            transformed_data['price'] = transformed_data['price'].round(2)\n",
    "            transformation_summary['operations_performed'].append('Fixed price data types')\n",
    "        \n",
    "        # 3. Data Enrichment\n",
    "        if self.config.processing['enable_enrichment']:\n",
    "            # Calculate total amount\n",
    "            if 'quantity' in transformed_data.columns and 'price' in transformed_data.columns:\n",
    "                transformed_data['total_amount'] = transformed_data['quantity'] * transformed_data['price']\n",
    "                transformation_summary['operations_performed'].append('Added total_amount calculation')\n",
    "            \n",
    "            # Add order size category\n",
    "            if 'total_amount' in transformed_data.columns:\n",
    "                transformed_data['order_size'] = pd.cut(\n",
    "                    transformed_data['total_amount'],\n",
    "                    bins=[0, 100, 500, 1000, 2000, float('inf')],\n",
    "                    labels=['XSmall', 'Small', 'Medium', 'Large', 'XLarge']\n",
    "                )\n",
    "                transformation_summary['operations_performed'].append('Added order size categorization')\n",
    "            \n",
    "            # Add customer segment\n",
    "            if 'total_amount' in transformed_data.columns:\n",
    "                def categorize_customer(amount):\n",
    "                    if amount >= 1500:\n",
    "                        return 'VIP'\n",
    "                    elif amount >= 800:\n",
    "                        return 'Premium'\n",
    "                    elif amount >= 300:\n",
    "                        return 'Standard'\n",
    "                    else:\n",
    "                        return 'Budget'\n",
    "                \n",
    "                transformed_data['customer_segment'] = transformed_data['total_amount'].apply(categorize_customer)\n",
    "                transformation_summary['operations_performed'].append('Added customer segmentation')\n",
    "            \n",
    "            # Add processing metadata\n",
    "            transformed_data['processed_at'] = datetime.now().isoformat()\n",
    "            transformed_data['pipeline_id'] = self.pipeline_id\n",
    "            transformation_summary['operations_performed'].append('Added processing metadata')\n",
    "        \n",
    "        # 4. Data Quality Score\n",
    "        def calculate_record_quality(row):\n",
    "            score = 100\n",
    "            \n",
    "            # Deduct for missing optional fields\n",
    "            optional_fields = ['customer_email', 'store_location']\n",
    "            for field in optional_fields:\n",
    "                if field in row and (pd.isna(row[field]) or str(row[field]).strip() == ''):\n",
    "                    score -= 10\n",
    "            \n",
    "            # Deduct for suspicious values\n",
    "            if 'customer_name' in row and len(str(row['customer_name'])) < 3:\n",
    "                score -= 15\n",
    "            \n",
    "            if 'total_amount' in row and row['total_amount'] > 5000:\n",
    "                score -= 5  # High value orders might need review\n",
    "            \n",
    "            return max(0, score)\n",
    "        \n",
    "        transformed_data['quality_score'] = transformed_data.apply(calculate_record_quality, axis=1)\n",
    "        transformation_summary['operations_performed'].append('Added quality scoring')\n",
    "        \n",
    "        # Remove records with critical issues (if any)\n",
    "        before_filter = len(transformed_data)\n",
    "        transformed_data = transformed_data.dropna(subset=['order_id', 'customer_name', 'product'])\n",
    "        after_filter = len(transformed_data)\n",
    "        \n",
    "        if before_filter != after_filter:\n",
    "            removed_count = before_filter - after_filter\n",
    "            transformation_summary['operations_performed'].append(f'Removed {removed_count} records with critical issues')\n",
    "            self.logger.warning(f\"⚠️ Removed {removed_count} records with critical missing data\")\n",
    "        \n",
    "        transformation_summary['transformed_records'] = len(transformed_data)\n",
    "        transformation_summary['success'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Transformation failed: {str(e)}\"\n",
    "        transformation_summary['errors'].append(error_msg)\n",
    "        transformation_summary['success'] = False\n",
    "        self.logger.error(f\"❌ {error_msg}\")\n",
    "        transformed_data = data  # Return original data if transformation fails\n",
    "    \n",
    "    stage_time = time.time() - stage_start\n",
    "    transformation_summary['processing_time'] = stage_time\n",
    "    \n",
    "    self.results['transformation'] = transformation_summary\n",
    "    \n",
    "    self.logger.info(f\"✅ Transformation stage completed in {stage_time:.2f}s: {len(transformation_summary['operations_performed'])} operations\")\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "# Add method to pipeline builder class\n",
    "DataPipelineBuilder.run_transformation_stage = run_transformation_stage\n",
    "\n",
    "print(\"🧹 Transformation stage methods added to pipeline builder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Step 5: Data Storage Stage\n",
    "\n",
    "Now let's add the storage stage to save our processed data to various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add storage methods to our pipeline builder\n",
    "def run_storage_stage(self, data):\n",
    "    \"\"\"Run data storage stage\"\"\"\n",
    "    self.logger.info(\"💾 Starting data storage stage\")\n",
    "    stage_start = time.time()\n",
    "    \n",
    "    storage_summary = {\n",
    "        'total_records': len(data),\n",
    "        'stored_records': 0,\n",
    "        'storage_operations': [],\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    if data.empty:\n",
    "        storage_summary['errors'].append(\"No data to store\")\n",
    "        storage_summary['success'] = False\n",
    "        self.results['storage'] = storage_summary\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # 1. File Storage\n",
    "        if self.config.outputs['files']['enabled']:\n",
    "            output_dir = Path(self.config.outputs['files']['output_dir'])\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            \n",
    "            # Save as CSV\n",
    "            if 'csv' in self.config.outputs['files']['formats']:\n",
    "                csv_file = output_dir / f'processed_orders_{timestamp}.csv'\n",
    "                data.to_csv(csv_file, index=False)\n",
    "                storage_summary['storage_operations'].append(f'Saved CSV: {csv_file.name}')\n",
    "                self.logger.info(f\"💾 Saved CSV file: {csv_file}\")\n",
    "            \n",
    "            # Save as JSON\n",
    "            if 'json' in self.config.outputs['files']['formats']:\n",
    "                json_file = output_dir / f'processed_orders_{timestamp}.json'\n",
    "                # Convert DataFrame to JSON with proper handling of special types\n",
    "                data_dict = data.to_dict('records')\n",
    "                with open(json_file, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'pipeline_id': self.pipeline_id,\n",
    "                        'processed_at': datetime.now().isoformat(),\n",
    "                        'record_count': len(data_dict),\n",
    "                        'orders': data_dict\n",
    "                    }, f, indent=2, default=str)\n",
    "                storage_summary['storage_operations'].append(f'Saved JSON: {json_file.name}')\n",
    "                self.logger.info(f\"💾 Saved JSON file: {json_file}\")\n",
    "        \n",
    "        # 2. Database Storage (Simulated)\n",
    "        if self.config.outputs['database']['enabled']:\n",
    "            # In a real implementation, this would use the DatabaseManager\n",
    "            # For demo purposes, we'll simulate database storage\n",
    "            db_path = self.config.outputs['database']['path']\n",
    "            table_name = self.config.outputs['database']['table']\n",
    "            \n",
    "            # Simulate database save\n",
    "            storage_summary['storage_operations'].append(f'Saved to database: {table_name} ({len(data)} records)')\n",
    "            self.logger.info(f\"💾 Simulated database save: {len(data)} records to {table_name}\")\n",
    "        \n",
    "        storage_summary['stored_records'] = len(data)\n",
    "        storage_summary['success'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Storage failed: {str(e)}\"\n",
    "        storage_summary['errors'].append(error_msg)\n",
    "        storage_summary['success'] = False\n",
    "        self.logger.error(f\"❌ {error_msg}\")\n",
    "    \n",
    "    stage_time = time.time() - stage_start\n",
    "    storage_summary['processing_time'] = stage_time\n",
    "    \n",
    "    self.results['storage'] = storage_summary\n",
    "    \n",
    "    self.logger.info(f\"✅ Storage stage completed in {stage_time:.2f}s: {len(storage_summary['storage_operations'])} operations\")\n",
    "\n",
    "# Add method to pipeline builder class\n",
    "DataPipelineBuilder.run_storage_stage = run_storage_stage\n",
    "\n",
    "print(\"💾 Storage stage methods added to pipeline builder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Step 6: Execute Complete Pipeline\n",
    "\n",
    "Now let's put it all together and execute our complete data pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add main pipeline execution method\n",
    "def execute_pipeline(self):\n",
    "    \"\"\"Execute the complete data pipeline\"\"\"\n",
    "    self.start_time = datetime.now()\n",
    "    self.logger.info(f\"🚀 Starting pipeline execution: {self.pipeline_id}\")\n",
    "    \n",
    "    pipeline_summary = {\n",
    "        'pipeline_id': self.pipeline_id,\n",
    "        'start_time': self.start_time.isoformat(),\n",
    "        'stages_completed': [],\n",
    "        'stages_failed': [],\n",
    "        'total_execution_time': 0,\n",
    "        'final_record_count': 0,\n",
    "        'overall_success': False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Data Ingestion\n",
    "        data = self.run_ingestion_stage()\n",
    "        if self.results['ingestion']['success']:\n",
    "            pipeline_summary['stages_completed'].append('ingestion')\n",
    "        else:\n",
    "            pipeline_summary['stages_failed'].append('ingestion')\n",
    "            if len(data) == 0:\n",
    "                raise Exception(\"No data ingested, stopping pipeline\")\n",
    "        \n",
    "        # Stage 2: Data Validation\n",
    "        data = self.run_validation_stage(data)\n",
    "        if self.results['validation']['success']:\n",
    "            pipeline_summary['stages_completed'].append('validation')\n",
    "        else:\n",
    "            pipeline_summary['stages_failed'].append('validation')\n",
    "            # Continue with warnings but don't stop pipeline\n",
    "            self.logger.warning(\"⚠️ Validation stage failed but continuing pipeline\")\n",
    "        \n",
    "        # Stage 3: Data Transformation\n",
    "        data = self.run_transformation_stage(data)\n",
    "        if self.results['transformation']['success']:\n",
    "            pipeline_summary['stages_completed'].append('transformation')\n",
    "        else:\n",
    "            pipeline_summary['stages_failed'].append('transformation')\n",
    "            raise Exception(\"Transformation stage failed, stopping pipeline\")\n",
    "        \n",
    "        # Stage 4: Data Storage\n",
    "        self.run_storage_stage(data)\n",
    "        if self.results['storage']['success']:\n",
    "            pipeline_summary['stages_completed'].append('storage')\n",
    "        else:\n",
    "            pipeline_summary['stages_failed'].append('storage')\n",
    "            raise Exception(\"Storage stage failed, stopping pipeline\")\n",
    "        \n",
    "        # Pipeline completed successfully\n",
    "        pipeline_summary['overall_success'] = True\n",
    "        pipeline_summary['final_record_count'] = len(data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_summary['overall_success'] = False\n",
    "        pipeline_summary['error_message'] = str(e)\n",
    "        self.logger.error(f\"❌ Pipeline execution failed: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        self.end_time = datetime.now()\n",
    "        pipeline_summary['end_time'] = self.end_time.isoformat()\n",
    "        pipeline_summary['total_execution_time'] = (self.end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        self.results['pipeline_summary'] = pipeline_summary\n",
    "        \n",
    "        # Log final results\n",
    "        status = \"✅ SUCCESS\" if pipeline_summary['overall_success'] else \"❌ FAILED\"\n",
    "        self.logger.info(f\"{status} Pipeline execution completed in {pipeline_summary['total_execution_time']:.2f}s\")\n",
    "        \n",
    "        return pipeline_summary, data if 'data' in locals() else pd.DataFrame()\n",
    "\n",
    "# Add method to pipeline builder class\n",
    "DataPipelineBuilder.execute_pipeline = execute_pipeline\n",
    "\n",
    "# Execute the complete pipeline\n",
    "print(\"🚀 Executing complete data pipeline...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pipeline_summary, final_data = pipeline_builder.execute_pipeline()\n",
    "\n",
    "print(\"\\n📊 PIPELINE EXECUTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Pipeline ID: {pipeline_summary['pipeline_id']}\")\n",
    "print(f\"Status: {'✅ SUCCESS' if pipeline_summary['overall_success'] else '❌ FAILED'}\")\n",
    "print(f\"Execution Time: {pipeline_summary['total_execution_time']:.2f} seconds\")\n",
    "print(f\"Final Record Count: {pipeline_summary['final_record_count']}\")\n",
    "print(f\"Stages Completed: {', '.join(pipeline_summary['stages_completed'])}\")\n",
    "\n",
    "if pipeline_summary['stages_failed']:\n",
    "    print(f\"Stages Failed: {', '.join(pipeline_summary['stages_failed'])}\")\n",
    "\n",
    "if 'error_message' in pipeline_summary:\n",
    "    print(f\"Error: {pipeline_summary['error_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 7: Pipeline Results Analysis\n",
    "\n",
    "Let's analyze the results of our pipeline execution and create visualizations to understand the data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pipeline results\n",
    "def analyze_pipeline_results(pipeline_builder, final_data):\n",
    "    \"\"\"Analyze and visualize pipeline results\"\"\"\n",
    "    \n",
    "    results = pipeline_builder.results\n",
    "    \n",
    "    print(\"📈 DETAILED STAGE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Ingestion Analysis\n",
    "    if 'ingestion' in results:\n",
    "        ing = results['ingestion']\n",
    "        print(f\"\\n📥 INGESTION STAGE:\")\n",
    "        print(f\"  Sources Processed: {ing.get('sources_processed', 0)}\")\n",
    "        print(f\"  Successful Sources: {ing.get('successful_sources', 0)}\")\n",
    "        print(f\"  Failed Sources: {ing.get('failed_sources', 0)}\")\n",
    "        print(f\"  Total Records: {ing.get('total_records', 0)}\")\n",
    "        print(f\"  Processing Time: {ing.get('processing_time', 0):.2f}s\")\n",
    "        if ing.get('errors'):\n",
    "            print(f\"  Errors: {len(ing['errors'])}\")\n",
    "    \n",
    "    # Validation Analysis\n",
    "    if 'validation' in results:\n",
    "        val = results['validation']\n",
    "        print(f\"\\n🔍 VALIDATION STAGE:\")\n",
    "        print(f\"  Total Records: {val.get('total_records', 0)}\")\n",
    "        print(f\"  Valid Records: {val.get('valid_records', 0)}\")\n",
    "        print(f\"  Invalid Records: {val.get('invalid_records', 0)}\")\n",
    "        print(f\"  Quality Score: {val.get('quality_score', 0):.1f}%\")\n",
    "        print(f\"  Meets Threshold: {'✅ Yes' if val.get('meets_threshold', False) else '❌ No'}\")\n",
    "        print(f\"  Processing Time: {val.get('processing_time', 0):.2f}s\")\n",
    "        if val.get('errors'):\n",
    "            print(f\"  Errors: {len(val['errors'])}\")\n",
    "    \n",
    "    # Transformation Analysis\n",
    "    if 'transformation' in results:\n",
    "        trans = results['transformation']\n",
    "        print(f\"\\n🧹 TRANSFORMATION STAGE:\")\n",
    "        print(f\"  Original Records: {trans.get('original_records', 0)}\")\n",
    "        print(f\"  Transformed Records: {trans.get('transformed_records', 0)}\")\n",
    "        print(f\"  Operations Performed: {len(trans.get('operations_performed', []))}\")\n",
    "        print(f\"  Processing Time: {trans.get('processing_time', 0):.2f}s\")\n",
    "        if trans.get('operations_performed'):\n",
    "            print(\"  Operations:\")\n",
    "            for op in trans['operations_performed']:\n",
    "                print(f\"    - {op}\")\n",
    "    \n",
    "    # Storage Analysis\n",
    "    if 'storage' in results:\n",
    "        stor = results['storage']\n",
    "        print(f\"\\n💾 STORAGE STAGE:\")\n",
    "        print(f\"  Total Records: {stor.get('total_records', 0)}\")\n",
    "        print(f\"  Stored Records: {stor.get('stored_records', 0)}\")\n",
    "        print(f\"  Storage Operations: {len(stor.get('storage_operations', []))}\")\n",
    "        print(f\"  Processing Time: {stor.get('processing_time', 0):.2f}s\")\n",
    "        if stor.get('storage_operations'):\n",
    "            print(\"  Operations:\")\n",
    "            for op in stor['storage_operations']:\n",
    "                print(f\"    - {op}\")\n",
    "    \n",
    "    # Data Analysis\n",
    "    if not final_data.empty:\n",
    "        print(f\"\\n📊 FINAL DATA ANALYSIS:\")\n",
    "        print(f\"  Total Records: {len(final_data)}\")\n",
    "        print(f\"  Total Columns: {len(final_data.columns)}\")\n",
    "        \n",
    "        # Show column info\n",
    "        print(f\"  Columns: {', '.join(final_data.columns.tolist())}\")\n",
    "        \n",
    "        # Data quality analysis\n",
    "        if 'quality_score' in final_data.columns:\n",
    "            avg_quality = final_data['quality_score'].mean()\n",
    "            min_quality = final_data['quality_score'].min()\n",
    "            max_quality = final_data['quality_score'].max()\n",
    "            print(f\"  Average Quality Score: {avg_quality:.1f}\")\n",
    "            print(f\"  Quality Range: {min_quality:.1f} - {max_quality:.1f}\")\n",
    "        \n",
    "        # Business insights\n",
    "        if 'total_amount' in final_data.columns:\n",
    "            total_revenue = final_data['total_amount'].sum()\n",
    "            avg_order_value = final_data['total_amount'].mean()\n",
    "            print(f\"  Total Revenue: ${total_revenue:,.2f}\")\n",
    "            print(f\"  Average Order Value: ${avg_order_value:.2f}\")\n",
    "        \n",
    "        if 'customer_segment' in final_data.columns:\n",
    "            segment_dist = final_data['customer_segment'].value_counts()\n",
    "            print(f\"  Customer Segments: {dict(segment_dist)}\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_pipeline_results(pipeline_builder, final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Step 8: Pipeline Visualization\n",
    "\n",
    "Let's create visualizations to better understand our pipeline performance and data insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline visualizations\n",
    "def create_pipeline_visualizations(pipeline_builder, final_data):\n",
    "    \"\"\"Create comprehensive pipeline visualizations\"\"\"\n",
    "    \n",
    "    results = pipeline_builder.results\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Pipeline Stage Performance\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    stages = ['Ingestion', 'Validation', 'Transformation', 'Storage']\n",
    "    times = [\n",
    "        results.get('ingestion', {}).get('processing_time', 0),\n",
    "        results.get('validation', {}).get('processing_time', 0),\n",
    "        results.get('transformation', {}).get('processing_time', 0),\n",
    "        results.get('storage', {}).get('processing_time', 0)\n",
    "    ]\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    bars = ax1.bar(stages, times, color=colors, alpha=0.8)\n",
    "    ax1.set_title('Pipeline Stage Performance', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Processing Time (seconds)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time in zip(bars, times):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{time:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Data Flow Through Pipeline\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    flow_stages = ['Ingested', 'Validated', 'Transformed', 'Stored']\n",
    "    record_counts = [\n",
    "        results.get('ingestion', {}).get('total_records', 0),\n",
    "        results.get('validation', {}).get('valid_records', 0),\n",
    "        results.get('transformation', {}).get('transformed_records', 0),\n",
    "        results.get('storage', {}).get('stored_records', 0)\n",
    "    ]\n",
    "    \n",
    "    ax2.plot(flow_stages, record_counts, marker='o', linewidth=3, markersize=8, color='#FF6B6B')\n",
    "    ax2.fill_between(flow_stages, record_counts, alpha=0.3, color='#FF6B6B')\n",
    "    ax2.set_title('Data Flow Through Pipeline', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Records')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, count in enumerate(record_counts):\n",
    "        ax2.annotate(f'{count}', (i, count), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontweight='bold')\n",
    "    \n",
    "    # 3. Data Quality Distribution\n",
    "    if not final_data.empty and 'quality_score' in final_data.columns:\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        quality_scores = final_data['quality_score']\n",
    "        ax3.hist(quality_scores, bins=10, color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "        ax3.axvline(quality_scores.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {quality_scores.mean():.1f}')\n",
    "        ax3.set_title('Data Quality Score Distribution', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Quality Score')\n",
    "        ax3.set_ylabel('Number of Records')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Customer Segment Analysis\n",
    "    if not final_data.empty and 'customer_segment' in final_data.columns:\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        segment_counts = final_data['customer_segment'].value_counts()\n",
    "        colors_pie = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "        wedges, texts, autotexts = ax4.pie(segment_counts.values, labels=segment_counts.index, \n",
    "                                          autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "        ax4.set_title('Customer Segment Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Make percentage text bold\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "    \n",
    "    # 5. Order Size Analysis\n",
    "    if not final_data.empty and 'order_size' in final_data.columns:\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        order_size_counts = final_data['order_size'].value_counts()\n",
    "        bars = ax5.bar(order_size_counts.index, order_size_counts.values, \n",
    "                      color='#45B7D1', alpha=0.8)\n",
    "        ax5.set_title('Order Size Distribution', fontsize=14, fontweight='bold')\n",
    "        ax5.set_xlabel('Order Size Category')\n",
    "        ax5.set_ylabel('Number of Orders')\n",
    "        ax5.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 6. Revenue Analysis\n",
    "    if not final_data.empty and 'total_amount' in final_data.columns:\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        \n",
    "        # Revenue by source if available\n",
    "        if 'source' in final_data.columns:\n",
    "            revenue_by_source = final_data.groupby('source')['total_amount'].sum()\n",
    "            bars = ax6.bar(revenue_by_source.index, revenue_by_source.values, \n",
    "                          color='#96CEB4', alpha=0.8)\n",
    "            ax6.set_title('Revenue by Source', fontsize=14, fontweight='bold')\n",
    "            ax6.set_xlabel('Data Source')\n",
    "            ax6.set_ylabel('Total Revenue ($)')\n",
    "            ax6.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax6.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                        f'${height:,.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "        else:\n",
    "            # Simple revenue histogram\n",
    "            ax6.hist(final_data['total_amount'], bins=10, color='#96CEB4', alpha=0.7, edgecolor='black')\n",
    "            ax6.set_title('Order Value Distribution', fontsize=14, fontweight='bold')\n",
    "            ax6.set_xlabel('Order Value ($)')\n",
    "            ax6.set_ylabel('Number of Orders')\n",
    "            ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📊 PIPELINE PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    total_time = sum(times)\n",
    "    print(f\"Total Pipeline Time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average Stage Time: {total_time/len(times):.2f} seconds\")\n",
    "    print(f\"Throughput: {record_counts[-1]/total_time:.1f} records/second\")\n",
    "    \n",
    "    if not final_data.empty:\n",
    "        print(f\"\\n💰 BUSINESS INSIGHTS\")\n",
    "        print(\"=\" * 30)\n",
    "        if 'total_amount' in final_data.columns:\n",
    "            total_revenue = final_data['total_amount'].sum()\n",
    "            avg_order = final_data['total_amount'].mean()\n",
    "            print(f\"Total Revenue Processed: ${total_revenue:,.2f}\")\n",
    "            print(f\"Average Order Value: ${avg_order:.2f}\")\n",
    "            print(f\"Revenue per Second: ${total_revenue/total_time:.2f}\")\n",
    "\n",
    "# Create visualizations\n",
    "if not final_data.empty:\n",
    "    create_pipeline_visualizations(pipeline_builder, final_data)\n",
    "    \n",
    "    # Display sample of final data\n",
    "    print(\"\\n📋 SAMPLE OF PROCESSED DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    display(final_data.head())\n",
    "else:\n",
    "    print(\"⚠️ No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Step 9: Error Handling and Recovery\n",
    "\n",
    "Let's demonstrate how to handle errors and implement recovery mechanisms in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate error handling and recovery\n",
    "def demonstrate_error_handling():\n",
    "    \"\"\"Demonstrate various error scenarios and recovery mechanisms\"\"\"\n",
    "    \n",
    "    print(\"🔧 DEMONSTRATING ERROR HANDLING & RECOVERY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Scenario 1: Missing data files\n",
    "    print(\"\\n📁 Scenario 1: Missing Data Files\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create a pipeline with non-existent data sources\n",
    "    error_config = PipelineConfig()\n",
    "    error_config.data_sources['csv_files']['path'] = '../data/input/nonexistent/'\n",
    "    \n",
    "    error_pipeline = DataPipelineBuilder(error_config)\n",
    "    \n",
    "    try:\n",
    "        data = error_pipeline.run_ingestion_stage()\n",
    "        print(f\"Result: {len(data)} records ingested\")\n",
    "        print(f\"Errors: {len(error_pipeline.results['ingestion']['errors'])}\")\n",
    "        for error in error_pipeline.results['ingestion']['errors'][:3]:  # Show first 3 errors\n",
    "            print(f\"  - {error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception caught: {e}\")\n",
    "    \n",
    "    # Scenario 2: Invalid data format\n",
    "    print(\"\\n📊 Scenario 2: Invalid Data Format\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create invalid data\n",
    "    invalid_data = pd.DataFrame({\n",
    "        'order_id': ['', 'ORD-002', None],  # Missing IDs\n",
    "        'customer_name': ['John Doe', '', 'Bob Wilson'],  # Missing name\n",
    "        'product': ['iPhone 15', 'MacBook Pro', ''],  # Missing product\n",
    "        'quantity': [-1, 'invalid', 2],  # Invalid quantity\n",
    "        'price': [999.99, -100, 'free']  # Invalid price\n",
    "    })\n",
    "    \n",
    "    validation_pipeline = DataPipelineBuilder(pipeline_config)\n",
    "    validated_data = validation_pipeline.run_validation_stage(invalid_data)\n",
    "    \n",
    "    val_results = validation_pipeline.results['validation']\n",
    "    print(f\"Quality Score: {val_results['quality_score']:.1f}%\")\n",
    "    print(f\"Valid Records: {val_results['valid_records']}/{val_results['total_records']}\")\n",
    "    print(f\"Errors Found: {len(val_results['errors'])}\")\n",
    "    \n",
    "    # Show some errors\n",
    "    for error in val_results['errors'][:5]:  # Show first 5 errors\n",
    "        print(f\"  - {error}\")\n",
    "    \n",
    "    # Scenario 3: Recovery strategies\n",
    "    print(\"\\n🔄 Scenario 3: Recovery Strategies\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    recovery_strategies = {\n",
    "        'Data Quality Issues': [\n",
    "            'Skip invalid records and continue processing',\n",
    "            'Apply default values for missing fields',\n",
    "            'Flag records for manual review',\n",
    "            'Send alerts to data stewards'\n",
    "        ],\n",
    "        'Source Unavailability': [\n",
    "            'Retry with exponential backoff',\n",
    "            'Switch to backup data sources',\n",
    "            'Continue with available sources',\n",
    "            'Queue failed sources for later retry'\n",
    "        ],\n",
    "        'Processing Failures': [\n",
    "            'Checkpoint and resume from last successful stage',\n",
    "            'Rollback to previous known good state',\n",
    "            'Partial processing with error reporting',\n",
    "            'Circuit breaker to prevent cascade failures'\n",
    "        ],\n",
    "        'Storage Issues': [\n",
    "            'Write to alternative storage locations',\n",
    "            'Buffer data in memory/temporary files',\n",
    "            'Compress data to reduce storage requirements',\n",
    "            'Implement storage quotas and cleanup'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for scenario, strategies in recovery_strategies.items():\n",
    "        print(f\"\\n🎯 {scenario}:\")\n",
    "        for i, strategy in enumerate(strategies, 1):\n",
    "            print(f\"  {i}. {strategy}\")\n",
    "    \n",
    "    # Scenario 4: Circuit breaker pattern\n",
    "    print(\"\\n⚡ Scenario 4: Circuit Breaker Implementation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    class CircuitBreaker:\n",
    "        def __init__(self, failure_threshold=3, recovery_timeout=60):\n",
    "            self.failure_threshold = failure_threshold\n",
    "            self.recovery_timeout = recovery_timeout\n",
    "            self.failure_count = 0\n",
    "            self.last_failure_time = None\n",
    "            self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN\n",
    "        \n",
    "        def call(self, func, *args, **kwargs):\n",
    "            if self.state == 'OPEN':\n",
    "                if time.time() - self.last_failure_time > self.recovery_timeout:\n",
    "                    self.state = 'HALF_OPEN'\n",
    "                else:\n",
    "                    raise Exception(\"Circuit breaker is OPEN\")\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                self.on_success()\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                self.on_failure()\n",
    "                raise e\n",
    "        \n",
    "        def on_success(self):\n",
    "            self.failure_count = 0\n",
    "            self.state = 'CLOSED'\n",
    "        \n",
    "        def on_failure(self):\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = time.time()\n",
    "            if self.failure_count >= self.failure_threshold:\n",
    "                self.state = 'OPEN'\n",
    "    \n",
    "    # Demonstrate circuit breaker\n",
    "    circuit_breaker = CircuitBreaker(failure_threshold=2, recovery_timeout=5)\n",
    "    \n",
    "    def unreliable_function():\n",
    "        import random\n",
    "        if random.random() < 0.7:  # 70% failure rate\n",
    "            raise Exception(\"Service unavailable\")\n",
    "        return \"Success!\"\n",
    "    \n",
    "    print(\"Testing circuit breaker with unreliable function:\")\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            result = circuit_breaker.call(unreliable_function)\n",
    "            print(f\"  Attempt {i+1}: {result} (State: {circuit_breaker.state})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Attempt {i+1}: Failed - {e} (State: {circuit_breaker.state})\")\n",
    "        time.sleep(0.1)  # Small delay\n",
    "\n",
    "# Run error handling demonstration\n",
    "demonstrate_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Step 10: Pipeline Report Generation\n",
    "\n",
    "Let's create a comprehensive report of our pipeline execution that can be saved and shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive pipeline report\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_pipeline_report(pipeline_builder, final_data):\n",
    "    \"\"\"Generate a comprehensive pipeline execution report\"\"\"\n",
    "\n",
    "    results = pipeline_builder.results\n",
    "    pipeline_summary = results.get('pipeline_summary', {})\n",
    "\n",
    "    report = []\n",
    "    report.append(\"# 📊 Data Pipeline Execution Report\")\n",
    "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Executive Summary\n",
    "    report.append(\"## 🎯 Executive Summary\")\n",
    "    report.append(f\"- **Pipeline ID**: {pipeline_builder.pipeline_id}\")\n",
    "    report.append(f\"- **Status**: {'✅ SUCCESS' if pipeline_summary.get('overall_success', False) else '❌ FAILED'}\")\n",
    "    report.append(f\"- **Execution Time**: {pipeline_summary.get('total_execution_time', 0):.2f} seconds\")\n",
    "    report.append(f\"- **Records Processed**: {pipeline_summary.get('final_record_count', 0):,}\")\n",
    "    report.append(f\"- **Stages Completed**: {len(pipeline_summary.get('stages_completed', []))}\")\n",
    "    report.append(f\"- **Stages Failed**: {len(pipeline_summary.get('stages_failed', []))}\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Stage Details\n",
    "    report.append(\"## 🔄 Stage Execution Details\")\n",
    "\n",
    "    # Ingestion Stage\n",
    "    if 'ingestion' in results:\n",
    "        ing = results['ingestion']\n",
    "        report.append(\"### 📥 Data Ingestion Stage\")\n",
    "        report.append(f\"- **Status**: {'✅ Success' if ing.get('success', False) else '❌ Failed'}\")\n",
    "        report.append(f\"- **Sources Processed**: {ing.get('sources_processed', 0)}\")\n",
    "        report.append(f\"- **Successful Sources**: {ing.get('successful_sources', 0)}\")\n",
    "        report.append(f\"- **Failed Sources**: {ing.get('failed_sources', 0)}\")\n",
    "        report.append(f\"- **Total Records**: {ing.get('total_records', 0):,}\")\n",
    "        report.append(f\"- **Processing Time**: {ing.get('processing_time', 0):.2f} seconds\")\n",
    "        if ing.get('errors'):\n",
    "            report.append(f\"- **Errors**: {len(ing['errors'])}\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    # Validation Stage\n",
    "    if 'validation' in results:\n",
    "        val = results['validation']\n",
    "        report.append(\"### 🔍 Data Validation Stage\")\n",
    "        report.append(f\"- **Status**: {'✅ Success' if val.get('success', False) else '❌ Failed'}\")\n",
    "        report.append(f\"- **Total Records**: {val.get('total_records', 0):,}\")\n",
    "        report.append(f\"- **Valid Records**: {val.get('valid_records', 0):,}\")\n",
    "        report.append(f\"- **Invalid Records**: {val.get('invalid_records', 0):,}\")\n",
    "        report.append(f\"- **Quality Score**: {val.get('quality_score', 0):.1f}%\")\n",
    "        report.append(f\"- **Meets Threshold**: {'✅ Yes' if val.get('meets_threshold', False) else '❌ No'}\")\n",
    "        report.append(f\"- **Processing Time**: {val.get('processing_time', 0):.2f} seconds\")\n",
    "        if val.get('errors'):\n",
    "            report.append(f\"- **Validation Errors**: {len(val['errors'])}\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    # Transformation Stage\n",
    "    if 'transformation' in results:\n",
    "        trans = results['transformation']\n",
    "        report.append(\"### 🧹 Data Transformation Stage\")\n",
    "        report.append(f\"- **Status**: {'✅ Success' if trans.get('success', False) else '❌ Failed'}\")\n",
    "        report.append(f\"- **Original Records**: {trans.get('original_records', 0):,}\")\n",
    "        report.append(f\"- **Transformed Records**: {trans.get('transformed_records', 0):,}\")\n",
    "        report.append(f\"- **Operations Performed**: {len(trans.get('operations_performed', []))}\")\n",
    "        report.append(f\"- **Processing Time**: {trans.get('processing_time', 0):.2f} seconds\")\n",
    "\n",
    "        if trans.get('operations_performed'):\n",
    "            report.append(\"- **Operations**:\")\n",
    "            for op in trans['operations_performed']:\n",
    "                report.append(f\"  - {op}\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    # Storage Stage\n",
    "    if 'storage' in results:\n",
    "        stor = results['storage']\n",
    "        report.append(\"### 💾 Data Storage Stage\")\n",
    "        report.append(f\"- **Status**: {'✅ Success' if stor.get('success', False) else '❌ Failed'}\")\n",
    "        report.append(f\"- **Records Stored**: {stor.get('stored_records', 0):,}\")\n",
    "        report.append(f\"- **Storage Operations**: {len(stor.get('storage_operations', []))}\")\n",
    "        report.append(f\"- **Processing Time**: {stor.get('processing_time', 0):.2f} seconds\")\n",
    "\n",
    "        if stor.get('storage_operations'):\n",
    "            report.append(\"- **Operations**:\")\n",
    "            for op in stor['storage_operations']:\n",
    "                report.append(f\"  - {op}\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    # Data Quality Analysis\n",
    "    if not final_data.empty:\n",
    "        report.append(\"## 📊 Data Quality Analysis\")\n",
    "        report.append(f\"- **Total Records**: {len(final_data):,}\")\n",
    "        report.append(f\"- **Total Columns**: {len(final_data.columns)}\")\n",
    "\n",
    "        if 'quality_score' in final_data.columns:\n",
    "            avg_quality = final_data['quality_score'].mean()\n",
    "            min_quality = final_data['quality_score'].min()\n",
    "            max_quality = final_data['quality_score'].max()\n",
    "            report.append(f\"- **Average Quality Score**: {avg_quality:.1f}/100\")\n",
    "            report.append(f\"- **Quality Range**: {min_quality:.1f} - {max_quality:.1f}\")\n",
    "\n",
    "            # Quality distribution\n",
    "            excellent = (final_data['quality_score'] >= 95).sum()\n",
    "            good = ((final_data['quality_score'] >= 80) & (final_data['quality_score'] < 95)).sum()\n",
    "            fair = ((final_data['quality_score'] >= 60) & (final_data['quality_score'] < 80)).sum()\n",
    "            poor = (final_data['quality_score'] < 60).sum()\n",
    "\n",
    "            report.append(f\"- **Quality Distribution**:\")\n",
    "            report.append(f\"  - Excellent (95-100): {excellent} records ({excellent/len(final_data)*100:.1f}%)\")\n",
    "            report.append(f\"  - Good (80-94): {good} records ({good/len(final_data)*100:.1f}%)\")\n",
    "            report.append(f\"  - Fair (60-79): {fair} records ({fair/len(final_data)*100:.1f}%)\")\n",
    "            report.append(f\"  - Poor (<60): {poor} records ({poor/len(final_data)*100:.1f}%)\")\n",
    "\n",
    "        report.append(\"\")\n",
    "\n",
    "    # Business Insights\n",
    "    if not final_data.empty:\n",
    "        report.append(\"## 💰 Business Insights\")\n",
    "\n",
    "        if 'total_amount' in final_data.columns:\n",
    "            total_revenue = final_data['total_amount'].sum()\n",
    "            avg_order_value = final_data['total_amount'].mean()\n",
    "            median_order_value = final_data['total_amount'].median()\n",
    "\n",
    "            report.append(f\"- **Total Revenue**: ${total_revenue:,.2f}\")\n",
    "            report.append(f\"- **Average Order Value**: ${avg_order_value:.2f}\")\n",
    "            report.append(f\"- **Median Order Value**: ${median_order_value:.2f}\")\n",
    "\n",
    "        if 'customer_segment' in final_data.columns:\n",
    "            segment_dist = final_data['customer_segment'].value_counts()\n",
    "            report.append(f\"- **Customer Segments**:\")\n",
    "            for segment, count in segment_dist.items():\n",
    "                percentage = (count / len(final_data)) * 100\n",
    "                report.append(f\"  - {segment}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "        if 'source' in final_data.columns:\n",
    "            source_dist = final_data['source'].value_counts()\n",
    "            report.append(f\"- **Data Sources**:\")\n",
    "            for source, count in source_dist.items():\n",
    "                percentage = (count / len(final_data)) * 100\n",
    "                report.append(f\"  - {source}: {count} records ({percentage:.1f}%)\")\n",
    "\n",
    "        report.append(\"\")\n",
    "\n",
    "    # Performance Metrics\n",
    "    report.append(\"## ⚡ Performance Metrics\")\n",
    "    total_time = pipeline_summary.get('total_execution_time', 0)\n",
    "    total_records = pipeline_summary.get('final_record_count', 0)\n",
    "\n",
    "    if total_time > 0:\n",
    "        throughput = total_records / total_time\n",
    "        report.append(f\"- **Throughput**: {throughput:.1f} records/second\")\n",
    "        report.append(f\"- **Processing Rate**: {total_records/total_time*60:.0f} records/minute\")\n",
    "\n",
    "    # Stage performance breakdown\n",
    "    stage_times = {\n",
    "        'Ingestion': results.get('ingestion', {}).get('processing_time', 0),\n",
    "        'Validation': results.get('validation', {}).get('processing_time', 0),\n",
    "        'Transformation': results.get('transformation', {}).get('processing_time', 0),\n",
    "        'Storage': results.get('storage', {}).get('processing_time', 0)\n",
    "    }\n",
    "\n",
    "    report.append(f\"- **Stage Performance**:\")\n",
    "    for stage, time_taken in stage_times.items():\n",
    "        if total_time > 0:\n",
    "            percentage = (time_taken / total_time) * 100\n",
    "            report.append(f\"  - {stage}: {time_taken:.2f}s ({percentage:.1f}% of total)\")\n",
    "\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Recommendations\n",
    "    report.append(\"## 💡 Recommendations\")\n",
    "\n",
    "    if pipeline_summary.get('overall_success', False):\n",
    "        report.append(\"- ✅ Pipeline executed successfully\")\n",
    "\n",
    "        # Performance recommendations\n",
    "        if total_time > 0 and total_records > 0:\n",
    "            throughput = total_records / total_time\n",
    "            if throughput < 10:\n",
    "                report.append(\"- ⚡ Consider optimizing processing speed (current throughput is low)\")\n",
    "            if stage_times['Validation'] > stage_times['Transformation']:\n",
    "                report.append(\"- 🔍 Validation stage is taking longer than transformation - consider optimizing validation rules\")\n",
    "\n",
    "        # Data quality recommendations\n",
    "        if not final_data.empty and 'quality_score' in final_data.columns:\n",
    "            avg_quality = final_data['quality_score'].mean()\n",
    "            if avg_quality < 80:\n",
    "                report.append(\"- 📊 Data quality is below recommended threshold - review data sources\")\n",
    "            elif avg_quality >= 95:\n",
    "                report.append(\"- 🌟 Excellent data quality maintained\")\n",
    "\n",
    "        report.append(\"- 📈 Monitor pipeline performance trends over time\")\n",
    "        report.append(\"- 🔄 Consider scheduling regular pipeline runs\")\n",
    "\n",
    "    else:\n",
    "        report.append(\"- ❌ Pipeline execution failed\")\n",
    "        report.append(\"- 🔍 Review error logs and fix issues\")\n",
    "        report.append(\"- 🔄 Implement retry mechanisms for failed stages\")\n",
    "        report.append(\"- 📞 Contact data engineering team if issues persist\")\n",
    "\n",
    "    report.append(\"\")\n",
    "    report.append(\"---\")\n",
    "    report.append(f\"*Report generated by Data Pipeline Builder v1.0*\")\n",
    "\n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate and display report\n",
    "try:\n",
    "    pipeline_report = generate_pipeline_report(pipeline_builder, final_data)\n",
    "\n",
    "    print(\"📋 COMPREHENSIVE PIPELINE REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(pipeline_report)\n",
    "\n",
    "    # Save report to file with better path handling\n",
    "    current_dir = Path.cwd()\n",
    "\n",
    "    # If we're in notebooks folder, go up one level\n",
    "    if current_dir.name == 'notebooks':\n",
    "        base_dir = current_dir.parent\n",
    "    else:\n",
    "        base_dir = current_dir\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = base_dir / 'data' / 'output'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Generate report filename\n",
    "    report_filename = output_dir / f\"pipeline_report_{pipeline_builder.pipeline_id}.md\"\n",
    "\n",
    "    # Save report\n",
    "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(pipeline_report)\n",
    "\n",
    "    print(f\"\\n💾 Report saved to: {report_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error generating pipeline report: {e}\")\n",
    "    print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "    # Try to display basic information if available\n",
    "    try:\n",
    "        if 'pipeline_builder' in locals() and hasattr(pipeline_builder, 'pipeline_id'):\n",
    "            print(f\"Pipeline ID: {pipeline_builder.pipeline_id}\")\n",
    "        if 'final_data' in locals() and not final_data.empty:\n",
    "            print(f\"Final data shape: {final_data.shape}\")\n",
    "            print(f\"Final data columns: {list(final_data.columns)}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "Congratulations! You've successfully built and executed a complete data ingestion pipeline. Here's what you've accomplished:\n",
    "\n",
    "### ✅ **Pipeline Components Mastered**\n",
    "- **📥 Data Ingestion**: Multi-source data collection with error handling\n",
    "- **🔍 Data Validation**: Quality scoring and business rule validation\n",
    "- **🧹 Data Transformation**: Cleaning, standardization, and enrichment\n",
    "- **💾 Data Storage**: Multiple output formats and destinations\n",
    "- **📊 Monitoring**: Performance tracking and comprehensive reporting\n",
    "\n",
    "### ✅ **Production-Ready Features**\n",
    "- **Error Handling**: Graceful failure management and recovery\n",
    "- **Performance Monitoring**: Stage-by-stage timing and throughput analysis\n",
    "- **Data Quality**: Comprehensive quality scoring and reporting\n",
    "- **Configurability**: Flexible configuration for different environments\n",
    "- **Observability**: Detailed logging and execution reports\n",
    "\n",
    "### ✅ **Business Value**\n",
    "- **Automated Processing**: Hands-off data processing workflows\n",
    "- **Quality Assurance**: Built-in data quality checks and scoring\n",
    "- **Scalability**: Modular design supporting growth\n",
    "- **Reliability**: Error recovery and fault tolerance\n",
    "- **Insights**: Business intelligence and performance metrics\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What's Next?\n",
    "\n",
    "In the final tutorial, **\"07_monitoring_pipeline.ipynb\"**, you'll learn:\n",
    "- 📈 **Advanced Monitoring**: Real-time pipeline health monitoring\n",
    "- 🚨 **Alerting Systems**: Automated notifications for issues\n",
    "- 📊 **Dashboard Creation**: Visual monitoring dashboards\n",
    "- 🔍 **Performance Optimization**: Identifying and fixing bottlenecks\n",
    "- 📋 **Operational Procedures**: Best practices for production operations\n",
    "\n",
    "### 🎯 **Practice Exercises**\n",
    "\n",
    "Before moving to the next tutorial, try these exercises:\n",
    "\n",
    "1. **Extend the Pipeline**: Add a new data source (e.g., XML files)\n",
    "2. **Custom Validation**: Create domain-specific validation rules\n",
    "3. **Performance Testing**: Test with larger datasets\n",
    "4. **Error Scenarios**: Simulate various failure conditions\n",
    "5. **Configuration**: Create different configurations for dev/prod\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Additional Resources\n",
    "\n",
    "- **🏗️ Architecture**: Study enterprise data pipeline patterns\n",
    "- **⚡ Performance**: Learn about parallel processing and optimization\n",
    "- **🔒 Security**: Implement data encryption and access controls\n",
    "- **☁️ Cloud**: Deploy pipelines to cloud platforms (AWS, GCP, Azure)\n",
    "- **🔄 Orchestration**: Explore Apache Airflow, Prefect, or Dagster\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work building your complete data pipeline! 🎉**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ingestion_env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
