{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Data Validation - Ensuring Quality and Reliability\n",
    "\n",
    "Welcome to the fourth tutorial in our **Data Ingestion Pipeline** series! In this hands-on notebook, you'll learn how to validate data quality and ensure your data is reliable, complete, and ready for analysis.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ✅ Understand why data validation is crucial\n",
    "- ✅ Implement schema validation for data structure\n",
    "- ✅ Create business rule validation for data logic\n",
    "- ✅ Build data quality scoring systems\n",
    "- ✅ Handle validation failures gracefully\n",
    "- ✅ Generate comprehensive validation reports\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Setup and Imports\n",
    "\n",
    "Let's start by importing the libraries we'll need for data validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for data validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# For data validation\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "print(\"📦 All libraries imported successfully!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "print(f\"⏰ Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Why Data Validation Matters\n",
    "\n",
    "Data validation is like **quality control in manufacturing** - it ensures that your data meets specific standards before it enters your system.\n",
    "\n",
    "### 💰 **The Cost of Bad Data:**\n",
    "- **Wrong Business Decisions** - Incorrect insights lead to poor choices\n",
    "- **Customer Dissatisfaction** - Wrong orders, incorrect billing\n",
    "- **Compliance Issues** - Regulatory violations and fines\n",
    "- **System Failures** - Applications crash on unexpected data\n",
    "- **Time Waste** - Hours spent debugging data issues\n",
    "\n",
    "### ✅ **Benefits of Good Validation:**\n",
    "- **Reliable Analytics** - Trust your reports and dashboards\n",
    "- **Automated Processing** - Systems run smoothly without manual intervention\n",
    "- **Early Problem Detection** - Catch issues at the source\n",
    "- **Compliance Assurance** - Meet regulatory requirements\n",
    "- **Better User Experience** - Applications work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with various quality issues\n",
    "print(\"🧪 Creating Sample Data with Quality Issues\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Sample e-commerce order data with intentional issues\n",
    "sample_orders = {\n",
    "    'order_id': [\n",
    "        'ORD-2024-001',    # ✅ Valid\n",
    "        '',                # ❌ Missing\n",
    "        'ORD-2024-003',    # ✅ Valid\n",
    "        'INVALID-ID',      # ❌ Wrong format\n",
    "        'ORD-2024-001',    # ❌ Duplicate\n",
    "        'ORD-2024-006',    # ✅ Valid\n",
    "        None,              # ❌ Null\n",
    "        'ORD-2024-008'     # ✅ Valid\n",
    "    ],\n",
    "    'customer_name': [\n",
    "        'John Doe',        # ✅ Valid\n",
    "        'jane smith',      # ⚠️ Inconsistent case\n",
    "        '',                # ❌ Missing\n",
    "        'Bob Wilson',      # ✅ Valid\n",
    "        'ALICE JOHNSON',   # ⚠️ All caps\n",
    "        'X',               # ❌ Too short\n",
    "        'Test Customer',   # ⚠️ Test data\n",
    "        'Charlie Brown'    # ✅ Valid\n",
    "    ],\n",
    "    'customer_email': [\n",
    "        'john@example.com',     # ✅ Valid\n",
    "        'invalid-email',        # ❌ Invalid format\n",
    "        'bob@company.co.uk',    # ✅ Valid\n",
    "        '',                     # ❌ Missing\n",
    "        'alice@domain',         # ❌ Incomplete\n",
    "        'test@test.com',        # ⚠️ Test email\n",
    "        None,                   # ❌ Null\n",
    "        'charlie@email.com'     # ✅ Valid\n",
    "    ],\n",
    "    'product': [\n",
    "        'iPhone 15',       # ✅ Valid\n",
    "        'MacBook Pro',     # ✅ Valid\n",
    "        '',                # ❌ Missing\n",
    "        'AirPods Pro',     # ✅ Valid\n",
    "        'iPad Air',        # ✅ Valid\n",
    "        'Apple Watch',     # ✅ Valid\n",
    "        'Test Product',    # ⚠️ Test data\n",
    "        'Nintendo Switch'  # ✅ Valid\n",
    "    ],\n",
    "    'quantity': [\n",
    "        1,                 # ✅ Valid\n",
    "        -1,                # ❌ Negative\n",
    "        2,                 # ✅ Valid\n",
    "        0,                 # ❌ Zero\n",
    "        1,                 # ✅ Valid\n",
    "        1000,              # ⚠️ Unusually high\n",
    "        None,              # ❌ Null\n",
    "        2                  # ✅ Valid\n",
    "    ],\n",
    "    'price': [\n",
    "        999.99,            # ✅ Valid\n",
    "        -100.00,           # ❌ Negative\n",
    "        1999.99,           # ✅ Valid\n",
    "        0.00,              # ❌ Zero\n",
    "        599.99,            # ✅ Valid\n",
    "        50000.00,          # ⚠️ Unusually high\n",
    "        None,              # ❌ Null\n",
    "        299.99             # ✅ Valid\n",
    "    ],\n",
    "    'order_date': [\n",
    "        '2024-01-15',      # ✅ Valid\n",
    "        '2025-12-31',      # ❌ Future date\n",
    "        '2024-01-16',      # ✅ Valid\n",
    "        'invalid-date',    # ❌ Invalid format\n",
    "        '2024-01-17',      # ✅ Valid\n",
    "        '1990-01-01',      # ⚠️ Very old\n",
    "        '',                # ❌ Missing\n",
    "        '2024-01-18'       # ✅ Valid\n",
    "    ],\n",
    "    'status': [\n",
    "        'pending',         # ✅ Valid\n",
    "        'SHIPPED',         # ⚠️ Inconsistent case\n",
    "        'processing',      # ✅ Valid\n",
    "        'invalid_status',  # ❌ Invalid value\n",
    "        'delivered',       # ✅ Valid\n",
    "        'cancelled',       # ✅ Valid\n",
    "        None,              # ❌ Null\n",
    "        'shipped'          # ✅ Valid\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_orders = pd.DataFrame(sample_orders)\n",
    "\n",
    "print(f\"📊 Created sample dataset with {len(df_orders)} orders\")\n",
    "print(f\"📋 Columns: {list(df_orders.columns)}\")\n",
    "print(f\"\\n🔍 Sample Data (with intentional quality issues):\")\n",
    "display(df_orders)\n",
    "\n",
    "# Quick overview of data types\n",
    "print(f\"\\n📈 Data Types:\")\n",
    "for col, dtype in df_orders.dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Schema Validation\n",
    "\n",
    "Schema validation ensures that your data has the correct structure - the right columns, data types, and format. It's like checking that a form has all the required fields filled out correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data schema and validation rules\n",
    "print(\"📋 Schema Validation\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Container for validation results\"\"\"\n",
    "    is_valid: bool\n",
    "    errors: List[Dict[str, Any]]\n",
    "    warnings: List[Dict[str, Any]]\n",
    "    valid_records: int\n",
    "    total_records: int\n",
    "    quality_score: float\n",
    "    validation_time: float\n",
    "\n",
    "class SchemaValidator:\n",
    "    \"\"\"Schema validation for data structure and types\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define expected schema\n",
    "        self.required_columns = [\n",
    "            'order_id', 'customer_name', 'product', 'quantity', 'price', 'order_date'\n",
    "        ]\n",
    "        \n",
    "        self.optional_columns = [\n",
    "            'customer_email', 'status', 'discount', 'notes'\n",
    "        ]\n",
    "        \n",
    "        self.column_types = {\n",
    "            'order_id': 'string',\n",
    "            'customer_name': 'string',\n",
    "            'customer_email': 'string',\n",
    "            'product': 'string',\n",
    "            'quantity': 'numeric',\n",
    "            'price': 'numeric',\n",
    "            'order_date': 'date',\n",
    "            'status': 'string'\n",
    "        }\n",
    "        \n",
    "        print(f\"📋 Schema validator initialized\")\n",
    "        print(f\"  Required columns: {len(self.required_columns)}\")\n",
    "        print(f\"  Optional columns: {len(self.optional_columns)}\")\n",
    "    \n",
    "    def validate_schema(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate DataFrame schema\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to validate\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Any]: Validation results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        validation_result = {\n",
    "            'is_valid': True,\n",
    "            'errors': [],\n",
    "            'warnings': [],\n",
    "            'column_analysis': {},\n",
    "            'missing_columns': [],\n",
    "            'extra_columns': [],\n",
    "            'type_issues': []\n",
    "        }\n",
    "        \n",
    "        # Check if DataFrame is empty\n",
    "        if df.empty:\n",
    "            validation_result['is_valid'] = False\n",
    "            validation_result['errors'].append({\n",
    "                'type': 'empty_dataset',\n",
    "                'message': 'Dataset is empty',\n",
    "                'severity': 'critical'\n",
    "            })\n",
    "            return validation_result\n",
    "        \n",
    "        # Check required columns\n",
    "        missing_columns = set(self.required_columns) - set(df.columns)\n",
    "        if missing_columns:\n",
    "            validation_result['is_valid'] = False\n",
    "            validation_result['missing_columns'] = list(missing_columns)\n",
    "            validation_result['errors'].append({\n",
    "                'type': 'missing_columns',\n",
    "                'message': f\"Missing required columns: {', '.join(missing_columns)}\",\n",
    "                'severity': 'critical',\n",
    "                'columns': list(missing_columns)\n",
    "            })\n",
    "        \n",
    "        # Check for extra columns\n",
    "        expected_columns = set(self.required_columns + self.optional_columns)\n",
    "        extra_columns = set(df.columns) - expected_columns\n",
    "        if extra_columns:\n",
    "            validation_result['extra_columns'] = list(extra_columns)\n",
    "            validation_result['warnings'].append({\n",
    "                'type': 'extra_columns',\n",
    "                'message': f\"Unexpected columns found: {', '.join(extra_columns)}\",\n",
    "                'severity': 'low',\n",
    "                'columns': list(extra_columns)\n",
    "            })\n",
    "        \n",
    "        # Validate column types\n",
    "        for column, expected_type in self.column_types.items():\n",
    "            if column in df.columns:\n",
    "                type_issues = self._validate_column_type(df[column], column, expected_type)\n",
    "                if type_issues:\n",
    "                    validation_result['type_issues'].extend(type_issues)\n",
    "                    validation_result['warnings'].extend(type_issues)\n",
    "        \n",
    "        # Analyze each column\n",
    "        for column in df.columns:\n",
    "            analysis = self._analyze_column(df[column], column)\n",
    "            validation_result['column_analysis'][column] = analysis\n",
    "        \n",
    "        validation_time = (datetime.now() - start_time).total_seconds()\n",
    "        validation_result['validation_time'] = validation_time\n",
    "        \n",
    "        return validation_result\n",
    "    \n",
    "    def _validate_column_type(self, series: pd.Series, column_name: str, expected_type: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Validate column data type\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        if expected_type == 'numeric':\n",
    "            # Check if values can be converted to numeric\n",
    "            non_numeric = pd.to_numeric(series, errors='coerce').isna() & series.notna()\n",
    "            if non_numeric.any():\n",
    "                issues.append({\n",
    "                    'type': 'type_mismatch',\n",
    "                    'column': column_name,\n",
    "                    'message': f\"Column '{column_name}' contains non-numeric values\",\n",
    "                    'severity': 'medium',\n",
    "                    'affected_count': non_numeric.sum()\n",
    "                })\n",
    "        \n",
    "        elif expected_type == 'date':\n",
    "            # Check if values can be converted to datetime\n",
    "            try:\n",
    "                pd.to_datetime(series, errors='coerce')\n",
    "            except Exception:\n",
    "                issues.append({\n",
    "                    'type': 'type_mismatch',\n",
    "                    'column': column_name,\n",
    "                    'message': f\"Column '{column_name}' contains invalid date values\",\n",
    "                    'severity': 'medium'\n",
    "                })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def _analyze_column(self, series: pd.Series, column_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze individual column statistics\"\"\"\n",
    "        analysis = {\n",
    "            'name': column_name,\n",
    "            'dtype': str(series.dtype),\n",
    "            'total_count': len(series),\n",
    "            'null_count': series.isnull().sum(),\n",
    "            'null_percentage': (series.isnull().sum() / len(series)) * 100,\n",
    "            'unique_count': series.nunique(),\n",
    "            'unique_percentage': (series.nunique() / len(series)) * 100\n",
    "        }\n",
    "        \n",
    "        # Add type-specific analysis\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            analysis.update({\n",
    "                'min_value': series.min(),\n",
    "                'max_value': series.max(),\n",
    "                'mean_value': series.mean(),\n",
    "                'std_value': series.std()\n",
    "            })\n",
    "        elif pd.api.types.is_string_dtype(series) or series.dtype == 'object':\n",
    "            # String analysis\n",
    "            non_null_series = series.dropna().astype(str)\n",
    "            if not non_null_series.empty:\n",
    "                analysis.update({\n",
    "                    'min_length': non_null_series.str.len().min(),\n",
    "                    'max_length': non_null_series.str.len().max(),\n",
    "                    'avg_length': non_null_series.str.len().mean(),\n",
    "                    'empty_strings': (non_null_series == '').sum()\n",
    "                })\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Test schema validation\n",
    "schema_validator = SchemaValidator()\n",
    "schema_result = schema_validator.validate_schema(df_orders)\n",
    "\n",
    "print(f\"\\n🔍 Schema Validation Results:\")\n",
    "print(f\"  Valid: {'✅ Yes' if schema_result['is_valid'] else '❌ No'}\")\n",
    "print(f\"  Errors: {len(schema_result['errors'])}\")\n",
    "print(f\"  Warnings: {len(schema_result['warnings'])}\")\n",
    "print(f\"  Validation time: {schema_result['validation_time']:.3f}s\")\n",
    "\n",
    "if schema_result['errors']:\n",
    "    print(f\"\\n❌ Schema Errors:\")\n",
    "    for error in schema_result['errors']:\n",
    "        print(f\"  - {error['message']} (Severity: {error['severity']})\")\n",
    "\n",
    "if schema_result['warnings']:\n",
    "    print(f\"\\n⚠️ Schema Warnings:\")\n",
    "    for warning in schema_result['warnings']:\n",
    "        print(f\"  - {warning['message']} (Severity: {warning['severity']})\")\n",
    "\n",
    "# Show column analysis\n",
    "print(f\"\\n📊 Column Analysis Summary:\")\n",
    "for col_name, analysis in schema_result['column_analysis'].items():\n",
    "    null_pct = analysis['null_percentage']\n",
    "    unique_pct = analysis['unique_percentage']\n",
    "    print(f\"  {col_name}: {null_pct:.1f}% null, {unique_pct:.1f}% unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Business Rule Validation\n",
    "\n",
    "Business rule validation checks if your data makes sense from a business perspective. For example, prices should be positive, dates should be reasonable, and email addresses should be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business rule validation\n",
    "print(\"🎯 Business Rule Validation\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "class BusinessRuleValidator:\n",
    "    \"\"\"Validate business logic and rules\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define business rules\n",
    "        self.rules = {\n",
    "            'order_id_format': r'^[A-Z]{3}-\\d{4}-\\d{3}$',  # ORD-2024-001\n",
    "            'email_format': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n",
    "            'valid_statuses': ['pending', 'processing', 'shipped', 'delivered', 'cancelled'],\n",
    "            'min_price': 0.01,\n",
    "            'max_price': 10000.00,\n",
    "            'min_quantity': 1,\n",
    "            'max_quantity': 100,\n",
    "            'min_name_length': 2,\n",
    "            'max_name_length': 100,\n",
    "            'date_range_years': 5  # Orders within last 5 years\n",
    "        }\n",
    "        \n",
    "        print(f\"🎯 Business rule validator initialized\")\n",
    "        print(f\"  Rules defined: {len(self.rules)}\")\n",
    "    \n",
    "    def validate_business_rules(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Validate business rules for the dataset\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to validate\n",
    "        \n",
    "        Returns:\n",
    "            ValidationResult: Comprehensive validation results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        errors = []\n",
    "        warnings = []\n",
    "        valid_records = 0\n",
    "        \n",
    "        # Validate each record\n",
    "        for index, row in df.iterrows():\n",
    "            record_errors = self._validate_record(row, index)\n",
    "            if not record_errors:\n",
    "                valid_records += 1\n",
    "            else:\n",
    "                errors.extend(record_errors)\n",
    "        \n",
    "        # Add dataset-level validations\n",
    "        dataset_issues = self._validate_dataset(df)\n",
    "        errors.extend(dataset_issues['errors'])\n",
    "        warnings.extend(dataset_issues['warnings'])\n",
    "        \n",
    "        # Calculate quality score\n",
    "        total_records = len(df)\n",
    "        quality_score = (valid_records / total_records) * 100 if total_records > 0 else 0\n",
    "        \n",
    "        validation_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return ValidationResult(\n",
    "            is_valid=len(errors) == 0,\n",
    "            errors=errors,\n",
    "            warnings=warnings,\n",
    "            valid_records=valid_records,\n",
    "            total_records=total_records,\n",
    "            quality_score=quality_score,\n",
    "            validation_time=validation_time\n",
    "        )\n",
    "    \n",
    "    def _validate_record(self, record: pd.Series, index: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Validate individual record against business rules\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Validate order ID format\n",
    "        if 'order_id' in record.index and pd.notna(record['order_id']):\n",
    "            if not re.match(self.rules['order_id_format'], str(record['order_id'])):\n",
    "                errors.append({\n",
    "                    'type': 'format_error',\n",
    "                    'field': 'order_id',\n",
    "                    'record_index': index,\n",
    "                    'value': record['order_id'],\n",
    "                    'message': f\"Invalid order ID format: {record['order_id']}\",\n",
    "                    'severity': 'high'\n",
    "                })\n",
    "        \n",
    "        # Validate email format\n",
    "        if 'customer_email' in record.index and pd.notna(record['customer_email']) and record['customer_email'] != '':\n",
    "            if not re.match(self.rules['email_format'], str(record['customer_email'])):\n",
    "                errors.append({\n",
    "                    'type': 'format_error',\n",
    "                    'field': 'customer_email',\n",
    "                    'record_index': index,\n",
    "                    'value': record['customer_email'],\n",
    "                    'message': f\"Invalid email format: {record['customer_email']}\",\n",
    "                    'severity': 'medium'\n",
    "                })\n",
    "        \n",
    "        # Validate price range\n",
    "        if 'price' in record.index and pd.notna(record['price']):\n",
    "            try:\n",
    "                price = float(record['price'])\n",
    "                if price < self.rules['min_price']:\n",
    "                    errors.append({\n",
    "                        'type': 'business_rule_error',\n",
    "                        'field': 'price',\n",
    "                        'record_index': index,\n",
    "                        'value': price,\n",
    "                        'message': f\"Price too low: ${price:.2f} < ${self.rules['min_price']:.2f}\",\n",
    "                        'severity': 'high'\n",
    "                    })\n",
    "                elif price > self.rules['max_price']:\n",
    "                    errors.append({\n",
    "                        'type': 'business_rule_error',\n",
    "                        'field': 'price',\n",
    "                        'record_index': index,\n",
    "                        'value': price,\n",
    "                        'message': f\"Price too high: ${price:.2f} > ${self.rules['max_price']:.2f}\",\n",
    "                        'severity': 'medium'\n",
    "                    })\n",
    "            except (ValueError, TypeError):\n",
    "                errors.append({\n",
    "                    'type': 'data_type_error',\n",
    "                    'field': 'price',\n",
    "                    'record_index': index,\n",
    "                    'value': record['price'],\n",
    "                    'message': f\"Invalid price value: {record['price']}\",\n",
    "                    'severity': 'high'\n",
    "                })\n",
    "        \n",
    "        # Validate quantity range\n",
    "        if 'quantity' in record.index and pd.notna(record['quantity']):\n",
    "            try:\n",
    "                quantity = int(record['quantity'])\n",
    "                if quantity < self.rules['min_quantity']:\n",
    "                    errors.append({\n",
    "                        'type': 'business_rule_error',\n",
    "                        'field': 'quantity',\n",
    "                        'record_index': index,\n",
    "                        'value': quantity,\n",
    "                        'message': f\"Quantity too low: {quantity} < {self.rules['min_quantity']}\",\n",
    "                        'severity': 'high'\n",
    "                    })\n",
    "                elif quantity > self.rules['max_quantity']:\n",
    "                    errors.append({\n",
    "                        'type': 'business_rule_error',\n",
    "                        'field': 'quantity',\n",
    "                        'record_index': index,\n",
    "                        'value': quantity,\n",
    "                        'message': f\"Quantity too high: {quantity} > {self.rules['max_quantity']}\",\n",
    "                        'severity': 'medium'\n",
    "                    })\n",
    "            except (ValueError, TypeError):\n",
    "                errors.append({\n",
    "                    'type': 'data_type_error',\n",
    "                    'field': 'quantity',\n",
    "                    'record_index': index,\n",
    "                    'value': record['quantity'],\n",
    "                    'message': f\"Invalid quantity value: {record['quantity']}\",\n",
    "                    'severity': 'high'\n",
    "                })\n",
    "        \n",
    "        # Validate customer name length\n",
    "        if 'customer_name' in record.index and pd.notna(record['customer_name']) and record['customer_name'] != '':\n",
    "            name_length = len(str(record['customer_name']))\n",
    "            if name_length < self.rules['min_name_length']:\n",
    "                errors.append({\n",
    "                    'type': 'business_rule_error',\n",
    "                    'field': 'customer_name',\n",
    "                    'record_index': index,\n",
    "                    'value': record['customer_name'],\n",
    "                    'message': f\"Customer name too short: {name_length} characters\",\n",
    "                    'severity': 'medium'\n",
    "                })\n",
    "            elif name_length > self.rules['max_name_length']:\n",
    "                errors.append({\n",
    "                    'type': 'business_rule_error',\n",
    "                    'field': 'customer_name',\n",
    "                    'record_index': index,\n",
    "                    'value': str(record['customer_name'])[:50] + \"...\",\n",
    "                    'message': f\"Customer name too long: {name_length} characters\",\n",
    "                    'severity': 'low'\n",
    "                })\n",
    "        \n",
    "        # Validate order status\n",
    "        if 'status' in record.index and pd.notna(record['status']):\n",
    "            status = str(record['status']).lower()\n",
    "            if status not in self.rules['valid_statuses']:\n",
    "                errors.append({\n",
    "                    'type': 'business_rule_error',\n",
    "                    'field': 'status',\n",
    "                    'record_index': index,\n",
    "                    'value': record['status'],\n",
    "                    'message': f\"Invalid status: {record['status']}. Valid: {', '.join(self.rules['valid_statuses'])}\",\n",
    "                    'severity': 'medium'\n",
    "                })\n",
    "        \n",
    "        # Validate order date\n",
    "        if 'order_date' in record.index and pd.notna(record['order_date']) and record['order_date'] != '':\n",
    "            try:\n",
    "                order_date = pd.to_datetime(record['order_date'])\n",
    "                current_date = datetime.now()\n",
    "                \n",
    "                # Check if date is in the future\n",
    "                if order_date > current_date:\n",
    "                    errors.append({\n",
    "                        'type': 'business_rule_error',\n",
    "                        'field': 'order_date',\n",
    "                        'record_index': index,\n",
    "                        'value': record['order_date'],\n",
    "                        'message': f\"Order date cannot be in the future: {order_date.strftime('%Y-%m-%d')}\",\n",
    "                        'severity': 'high'\n",
    "                    })\n",
    "                \n",
    "                # Check if date is too old\n",
    "                years_ago = current_date - timedelta(days=self.rules['date_range_years']*365)\n",
    "                if order_date < years_ago:\n",
    "                    errors.append({\n",
    "                        'type': 'business_rule_error',\n",
    "                        'field': 'order_date',\n",
    "                        'record_index': index,\n",
    "                        'value': record['order_date'],\n",
    "                        'message': f\"Order date is very old: {order_date.strftime('%Y-%m-%d')}\",\n",
    "                        'severity': 'low'\n",
    "                    })\n",
    "                    \n",
    "            except (ValueError, TypeError):\n",
    "                errors.append({\n",
    "                    'type': 'data_type_error',\n",
    "                    'field': 'order_date',\n",
    "                    'record_index': index,\n",
    "                    'value': record['order_date'],\n",
    "                    'message': f\"Invalid date format: {record['order_date']}\",\n",
    "                    'severity': 'high'\n",
    "                })\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def _validate_dataset(self, df: pd.DataFrame) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Validate dataset-level business rules\"\"\"\n",
    "        errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        # Check for duplicate order IDs\n",
    "        if 'order_id' in df.columns:\n",
    "            duplicates = df[df.duplicated(subset=['order_id'], keep=False) & df['order_id'].notna()]\n",
    "            if not duplicates.empty:\n",
    "                duplicate_ids = duplicates['order_id'].unique()\n",
    "                errors.append({\n",
    "                    'type': 'duplicate_error',\n",
    "                    'field': 'order_id',\n",
    "                    'message': f\"Duplicate order IDs found: {', '.join(duplicate_ids[:5])}{'...' if len(duplicate_ids) > 5 else ''}\",\n",
    "                    'severity': 'high',\n",
    "                    'affected_records': len(duplicates)\n",
    "                })\n",
    "        \n",
    "        # Check for suspicious patterns\n",
    "        if 'customer_name' in df.columns:\n",
    "            # Check for test data\n",
    "            test_patterns = ['test', 'dummy', 'sample', 'example']\n",
    "            test_mask = df['customer_name'].str.lower().str.contains('|'.join(test_patterns), na=False)\n",
    "            if test_mask.any():\n",
    "                warnings.append({\n",
    "                    'type': 'data_quality_warning',\n",
    "                    'field': 'customer_name',\n",
    "                    'message': f\"Potential test data found in customer names: {test_mask.sum()} records\",\n",
    "                    'severity': 'medium',\n",
    "                    'affected_records': test_mask.sum()\n",
    "                })\n",
    "        \n",
    "        return {'errors': errors, 'warnings': warnings}\n",
    "\n",
    "# Test business rule validation\n",
    "business_validator = BusinessRuleValidator()\n",
    "business_result = business_validator.validate_business_rules(df_orders)\n",
    "\n",
    "print(f\"\\n🎯 Business Rule Validation Results:\")\n",
    "print(f\"  Valid: {'✅ Yes' if business_result.is_valid else '❌ No'}\")\n",
    "print(f\"  Quality Score: {business_result.quality_score:.1f}%\")\n",
    "print(f\"  Valid Records: {business_result.valid_records}/{business_result.total_records}\")\n",
    "print(f\"  Errors: {len(business_result.errors)}\")\n",
    "print(f\"  Warnings: {len(business_result.warnings)}\")\n",
    "print(f\"  Validation time: {business_result.validation_time:.3f}s\")\n",
    "\n",
    "# Show error breakdown by type\n",
    "if business_result.errors:\n",
    "    print(f\"\\n❌ Error Breakdown:\")\n",
    "    error_types = {}\n",
    "    for error in business_result.errors:\n",
    "        error_type = error['type']\n",
    "        error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "    \n",
    "    for error_type, count in error_types.items():\n",
    "        print(f\"  {error_type.replace('_', ' ').title()}: {count}\")\n",
    "\n",
    "# Show sample errors\n",
    "if business_result.errors:\n",
    "    print(f\"\\n🔍 Sample Errors (first 5):\")\n",
    "    for i, error in enumerate(business_result.errors[:5], 1):\n",
    "        print(f\"  {i}. Row {error.get('record_index', 'N/A')}, {error.get('field', 'N/A')}: {error['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Data Quality Scoring\n",
    "\n",
    "Let's create a comprehensive data quality scoring system that gives us a single score representing the overall quality of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality scoring system\n",
    "print(\"📊 Data Quality Scoring System\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "class DataQualityScorer:\n",
    "    \"\"\"Comprehensive data quality scoring system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define quality dimensions and their weights\n",
    "        self.quality_dimensions = {\n",
    "            'completeness': 0.25,    # How much data is missing?\n",
    "            'validity': 0.25,        # Does data conform to rules?\n",
    "            'consistency': 0.20,     # Is data consistent across records?\n",
    "            'accuracy': 0.15,        # Is data correct?\n",
    "            'uniqueness': 0.15       # Are there duplicates?\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 Quality scorer initialized\")\n",
    "        print(f\"  Dimensions: {list(self.quality_dimensions.keys())}\")\n",
    "        print(f\"  Weights: {list(self.quality_dimensions.values())}\")\n",
    "    \n",
    "    def calculate_quality_score(self, df: pd.DataFrame, validation_result: ValidationResult) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Calculate comprehensive data quality score\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset to score\n",
    "            validation_result (ValidationResult): Validation results\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Any]: Quality scores and analysis\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # 1. Completeness Score (0-100)\n",
    "        scores['completeness'] = self._calculate_completeness_score(df)\n",
    "        \n",
    "        # 2. Validity Score (0-100)\n",
    "        scores['validity'] = self._calculate_validity_score(validation_result)\n",
    "        \n",
    "        # 3. Consistency Score (0-100)\n",
    "        scores['consistency'] = self._calculate_consistency_score(df)\n",
    "        \n",
    "        # 4. Accuracy Score (0-100)\n",
    "        scores['accuracy'] = self._calculate_accuracy_score(df)\n",
    "        \n",
    "        # 5. Uniqueness Score (0-100)\n",
    "        scores['uniqueness'] = self._calculate_uniqueness_score(df)\n",
    "        \n",
    "        # Calculate weighted overall score\n",
    "        overall_score = sum(\n",
    "            scores[dimension] * weight \n",
    "            for dimension, weight in self.quality_dimensions.items()\n",
    "        )\n",
    "        \n",
    "        # Determine quality level\n",
    "        quality_level = self._determine_quality_level(overall_score)\n",
    "        \n",
    "        return {\n",
    "            'overall_score': overall_score,\n",
    "            'quality_level': quality_level,\n",
    "            'dimension_scores': scores,\n",
    "            'weights': self.quality_dimensions,\n",
    "            'recommendations': self._generate_recommendations(scores)\n",
    "        }\n",
    "    \n",
    "    def _calculate_completeness_score(self, df: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate completeness score based on missing values\"\"\"\n",
    "        if df.empty:\n",
    "            return 0.0\n",
    "        \n",
    "        total_cells = df.size\n",
    "        missing_cells = df.isnull().sum().sum()\n",
    "        empty_strings = (df == '').sum().sum() if df.select_dtypes(include=['object']).size > 0 else 0\n",
    "        \n",
    "        missing_total = missing_cells + empty_strings\n",
    "        completeness = ((total_cells - missing_total) / total_cells) * 100\n",
    "        \n",
    "        return max(0.0, min(100.0, completeness))\n",
    "    \n",
    "    def _calculate_validity_score(self, validation_result: ValidationResult) -> float:\n",
    "        \"\"\"Calculate validity score based on validation results\"\"\"\n",
    "        if validation_result.total_records == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Count critical and high severity errors more heavily\n",
    "        error_penalty = 0\n",
    "        for error in validation_result.errors:\n",
    "            severity = error.get('severity', 'medium')\n",
    "            if severity == 'critical':\n",
    "                error_penalty += 10\n",
    "            elif severity == 'high':\n",
    "                error_penalty += 5\n",
    "            elif severity == 'medium':\n",
    "                error_penalty += 2\n",
    "            else:  # low\n",
    "                error_penalty += 1\n",
    "        \n",
    "        # Calculate validity score\n",
    "        max_possible_penalty = validation_result.total_records * 10  # Assume worst case\n",
    "        validity = max(0.0, 100.0 - (error_penalty / max_possible_penalty * 100))\n",
    "        \n",
    "        return validity\n",
    "    \n",
    "    def _calculate_consistency_score(self, df: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate consistency score based on data patterns\"\"\"\n",
    "        if df.empty:\n",
    "            return 0.0\n",
    "        \n",
    "        consistency_issues = 0\n",
    "        total_checks = 0\n",
    "        \n",
    "        # Check string case consistency\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            if col in ['customer_name', 'status']:\n",
    "                non_null_values = df[col].dropna()\n",
    "                if not non_null_values.empty:\n",
    "                    total_checks += 1\n",
    "                    # Check for mixed case patterns\n",
    "                    mixed_case = sum([\n",
    "                        str(val).islower() for val in non_null_values\n",
    "                    ]) + sum([\n",
    "                        str(val).isupper() for val in non_null_values\n",
    "                    ]) + sum([\n",
    "                        str(val).istitle() for val in non_null_values\n",
    "                    ])\n",
    "                    \n",
    "                    if mixed_case < len(non_null_values) * 0.8:  # Less than 80% consistent\n",
    "                        consistency_issues += 1\n",
    "        \n",
    "        # Check date format consistency\n",
    "        if 'order_date' in df.columns:\n",
    "            total_checks += 1\n",
    "            date_formats = set()\n",
    "            for date_val in df['order_date'].dropna():\n",
    "                if pd.notna(date_val) and str(date_val) != '':\n",
    "                    # Simple format detection\n",
    "                    date_str = str(date_val)\n",
    "                    if '-' in date_str:\n",
    "                        date_formats.add('dash_separated')\n",
    "                    elif '/' in date_str:\n",
    "                        date_formats.add('slash_separated')\n",
    "                    else:\n",
    "                        date_formats.add('other')\n",
    "            \n",
    "            if len(date_formats) > 1:\n",
    "                consistency_issues += 1\n",
    "        \n",
    "        if total_checks == 0:\n",
    "            return 100.0\n",
    "        \n",
    "        consistency_score = ((total_checks - consistency_issues) / total_checks) * 100\n",
    "        return max(0.0, consistency_score)\n",
    "    \n",
    "    def _calculate_accuracy_score(self, df: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate accuracy score based on data reasonableness\"\"\"\n",
    "        if df.empty:\n",
    "            return 0.0\n",
    "        \n",
    "        accuracy_issues = 0\n",
    "        total_records = len(df)\n",
    "        \n",
    "        # Check for obviously incorrect data\n",
    "        for index, row in df.iterrows():\n",
    "            record_issues = 0\n",
    "            \n",
    "            # Check for test/dummy data\n",
    "            if 'customer_name' in row.index and pd.notna(row['customer_name']):\n",
    "                name = str(row['customer_name']).lower()\n",
    "                if any(test_word in name for test_word in ['test', 'dummy', 'sample', 'example']):\n",
    "                    record_issues += 1\n",
    "            \n",
    "            # Check for unrealistic prices\n",
    "            if 'price' in row.index and pd.notna(row['price']):\n",
    "                try:\n",
    "                    price = float(row['price'])\n",
    "                    if price > 10000 or price < 0.01:  # Unrealistic price range\n",
    "                        record_issues += 1\n",
    "                except:\n",
    "                    record_issues += 1\n",
    "            \n",
    "            # Check for unrealistic quantities\n",
    "            if 'quantity' in row.index and pd.notna(row['quantity']):\n",
    "                try:\n",
    "                    quantity = int(row['quantity'])\n",
    "                    if quantity > 100 or quantity < 1:  # Unrealistic quantity\n",
    "                        record_issues += 1\n",
    "                except:\n",
    "                    record_issues += 1\n",
    "            \n",
    "            if record_issues > 0:\n",
    "                accuracy_issues += 1\n",
    "        \n",
    "        accuracy_score = ((total_records - accuracy_issues) / total_records) * 100\n",
    "        return max(0.0, accuracy_score)\n",
    "    \n",
    "    def _calculate_uniqueness_score(self, df: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate uniqueness score based on duplicates\"\"\"\n",
    "        if df.empty:\n",
    "            return 0.0\n",
    "        \n",
    "        total_records = len(df)\n",
    "        \n",
    "        # Check for exact duplicates\n",
    "        exact_duplicates = df.duplicated().sum()\n",
    "        \n",
    "        # Check for key field duplicates (order_id)\n",
    "        key_duplicates = 0\n",
    "        if 'order_id' in df.columns:\n",
    "            key_duplicates = df[df['order_id'].notna()].duplicated(subset=['order_id']).sum()\n",
    "        \n",
    "        total_duplicates = max(exact_duplicates, key_duplicates)\n",
    "        uniqueness_score = ((total_records - total_duplicates) / total_records) * 100\n",
    "        \n",
    "        return max(0.0, uniqueness_score)\n",
    "    \n",
    "    def _determine_quality_level(self, score: float) -> str:\n",
    "        \"\"\"Determine quality level based on score\"\"\"\n",
    "        if score >= 95:\n",
    "            return 'Excellent'\n",
    "        elif score >= 85:\n",
    "            return 'Good'\n",
    "        elif score >= 70:\n",
    "            return 'Fair'\n",
    "        elif score >= 50:\n",
    "            return 'Poor'\n",
    "        else:\n",
    "            return 'Critical'\n",
    "    \n",
    "    def _generate_recommendations(self, scores: Dict[str, float]) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on quality scores\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if scores['completeness'] < 90:\n",
    "            recommendations.append(\"Improve data completeness by reducing missing values\")\n",
    "        \n",
    "        if scores['validity'] < 85:\n",
    "            recommendations.append(\"Fix validation errors to improve data validity\")\n",
    "        \n",
    "        if scores['consistency'] < 80:\n",
    "            recommendations.append(\"Standardize data formats for better consistency\")\n",
    "        \n",
    "        if scores['accuracy'] < 85:\n",
    "            recommendations.append(\"Review and clean suspicious or test data\")\n",
    "        \n",
    "        if scores['uniqueness'] < 95:\n",
    "            recommendations.append(\"Remove duplicate records to improve uniqueness\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"Data quality is excellent! Continue monitoring.\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Test data quality scoring\n",
    "quality_scorer = DataQualityScorer()\n",
    "quality_results = quality_scorer.calculate_quality_score(df_orders, business_result)\n",
    "\n",
    "print(f\"\\n📊 Data Quality Score Results:\")\n",
    "print(f\"  Overall Score: {quality_results['overall_score']:.1f}/100\")\n",
    "print(f\"  Quality Level: {quality_results['quality_level']}\")\n",
    "\n",
    "print(f\"\\n📈 Dimension Scores:\")\n",
    "for dimension, score in quality_results['dimension_scores'].items():\n",
    "    weight = quality_results['weights'][dimension]\n",
    "    weighted_score = score * weight\n",
    "    print(f\"  {dimension.title()}: {score:.1f}/100 (weight: {weight:.2f}, contribution: {weighted_score:.1f})\")\n",
    "\n",
    "print(f\"\\n💡 Recommendations:\")\n",
    "for i, recommendation in enumerate(quality_results['recommendations'], 1):\n",
    "    print(f\"  {i}. {recommendation}\")\n",
    "\n",
    "# Create visualization of quality scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Dimension scores radar chart (simplified as bar chart)\n",
    "dimensions = list(quality_results['dimension_scores'].keys())\n",
    "scores = list(quality_results['dimension_scores'].values())\n",
    "\n",
    "bars = ax1.bar(dimensions, scores, alpha=0.7)\n",
    "ax1.set_title('Data Quality Dimension Scores')\n",
    "ax1.set_ylabel('Score (0-100)')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Color bars based on score\n",
    "for bar, score in zip(bars, scores):\n",
    "    if score >= 85:\n",
    "        bar.set_color('green')\n",
    "    elif score >= 70:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "# Add score labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{score:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Overall quality gauge (simplified as pie chart)\n",
    "overall_score = quality_results['overall_score']\n",
    "remaining_score = 100 - overall_score\n",
    "\n",
    "colors = ['green' if overall_score >= 85 else 'orange' if overall_score >= 70 else 'red', 'lightgray']\n",
    "ax2.pie([overall_score, remaining_score], labels=['Quality Score', 'Gap'], \n",
    "        colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title(f'Overall Quality Score: {overall_score:.1f}/100\\n({quality_results[\"quality_level\"]})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Validation Report Generation\n",
    "\n",
    "Let's create a comprehensive validation report that summarizes all our findings and provides actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive validation report generator\n",
    "print(\"📋 Validation Report Generation\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "class ValidationReportGenerator:\n",
    "    \"\"\"Generate comprehensive validation reports\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(f\"📋 Validation report generator initialized\")\n",
    "    \n",
    "    def generate_comprehensive_report(self, df: pd.DataFrame, \n",
    "                                    schema_result: Dict[str, Any],\n",
    "                                    business_result: ValidationResult,\n",
    "                                    quality_results: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive validation report\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Original dataset\n",
    "            schema_result (Dict): Schema validation results\n",
    "            business_result (ValidationResult): Business rule validation results\n",
    "            quality_results (Dict): Quality scoring results\n",
    "        \n",
    "        Returns:\n",
    "            str: Formatted validation report\n",
    "        \"\"\"\n",
    "        report = []\n",
    "        report.append(\"# 📊 Data Validation Report\")\n",
    "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        report.append(\"## 🎯 Executive Summary\")\n",
    "        report.append(\"\")\n",
    "        overall_status = \"✅ PASSED\" if business_result.is_valid else \"❌ FAILED\"\n",
    "        report.append(f\"**Validation Status:** {overall_status}\")\n",
    "        report.append(f\"**Overall Quality Score:** {quality_results['overall_score']:.1f}/100 ({quality_results['quality_level']})\")\n",
    "        report.append(f\"**Records Analyzed:** {len(df):,}\")\n",
    "        report.append(f\"**Valid Records:** {business_result.valid_records:,} ({(business_result.valid_records/business_result.total_records)*100:.1f}%)\")\n",
    "        report.append(f\"**Total Issues:** {len(business_result.errors)} errors, {len(business_result.warnings)} warnings\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Dataset Overview\n",
    "        report.append(\"## 📊 Dataset Overview\")\n",
    "        report.append(\"\")\n",
    "        report.append(f\"- **Rows:** {len(df):,}\")\n",
    "        report.append(f\"- **Columns:** {len(df.columns)}\")\n",
    "        report.append(f\"- **Data Types:** {df.dtypes.value_counts().to_dict()}\")\n",
    "        report.append(f\"- **Memory Usage:** {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Schema Validation Results\n",
    "        report.append(\"## 📋 Schema Validation\")\n",
    "        report.append(\"\")\n",
    "        schema_status = \"✅ PASSED\" if schema_result['is_valid'] else \"❌ FAILED\"\n",
    "        report.append(f\"**Status:** {schema_status}\")\n",
    "        \n",
    "        if schema_result['missing_columns']:\n",
    "            report.append(f\"**Missing Required Columns:** {', '.join(schema_result['missing_columns'])}\")\n",
    "        \n",
    "        if schema_result['extra_columns']:\n",
    "            report.append(f\"**Extra Columns:** {', '.join(schema_result['extra_columns'])}\")\n",
    "        \n",
    "        if schema_result['type_issues']:\n",
    "            report.append(f\"**Type Issues:** {len(schema_result['type_issues'])}\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Column Analysis\n",
    "        report.append(\"### 📈 Column Analysis\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"| Column | Type | Null % | Unique % | Issues |\")\n",
    "        report.append(\"|--------|------|--------|----------|--------|\")\n",
    "        \n",
    "        for col_name, analysis in schema_result['column_analysis'].items():\n",
    "            null_pct = analysis['null_percentage']\n",
    "            unique_pct = analysis['unique_percentage']\n",
    "            dtype = analysis['dtype']\n",
    "            \n",
    "            # Identify issues\n",
    "            issues = []\n",
    "            if null_pct > 20:\n",
    "                issues.append(\"High nulls\")\n",
    "            if unique_pct < 10 and col_name not in ['status', 'product_category']:\n",
    "                issues.append(\"Low uniqueness\")\n",
    "            \n",
    "            issues_str = \", \".join(issues) if issues else \"None\"\n",
    "            report.append(f\"| {col_name} | {dtype} | {null_pct:.1f}% | {unique_pct:.1f}% | {issues_str} |\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Business Rule Validation\n",
    "        report.append(\"## 🎯 Business Rule Validation\")\n",
    "        report.append(\"\")\n",
    "        business_status = \"✅ PASSED\" if business_result.is_valid else \"❌ FAILED\"\n",
    "        report.append(f\"**Status:** {business_status}\")\n",
    "        report.append(f\"**Quality Score:** {business_result.quality_score:.1f}%\")\n",
    "        report.append(f\"**Validation Time:** {business_result.validation_time:.3f} seconds\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Error Summary\n",
    "        if business_result.errors:\n",
    "            report.append(\"### ❌ Error Summary\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Group errors by type\n",
    "            error_summary = {}\n",
    "            for error in business_result.errors:\n",
    "                error_type = error['type']\n",
    "                severity = error.get('severity', 'medium')\n",
    "                key = f\"{error_type} ({severity})\"\n",
    "                error_summary[key] = error_summary.get(key, 0) + 1\n",
    "            \n",
    "            for error_type, count in sorted(error_summary.items(), key=lambda x: x[1], reverse=True):\n",
    "                report.append(f\"- **{error_type.replace('_', ' ').title()}:** {count} occurrences\")\n",
    "            \n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Top errors\n",
    "            report.append(\"### 🔍 Top Errors (First 10)\")\n",
    "            report.append(\"\")\n",
    "            for i, error in enumerate(business_result.errors[:10], 1):\n",
    "                row_num = error.get('record_index', 'N/A')\n",
    "                field = error.get('field', 'N/A')\n",
    "                message = error['message']\n",
    "                severity = error.get('severity', 'medium')\n",
    "                report.append(f\"{i}. **Row {row_num}, {field}** ({severity}): {message}\")\n",
    "            \n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Quality Dimension Analysis\n",
    "        report.append(\"## 📊 Quality Dimension Analysis\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        for dimension, score in quality_results['dimension_scores'].items():\n",
    "            weight = quality_results['weights'][dimension]\n",
    "            contribution = score * weight\n",
    "            \n",
    "            # Determine status icon\n",
    "            if score >= 85:\n",
    "                status_icon = \"✅\"\n",
    "            elif score >= 70:\n",
    "                status_icon = \"⚠️\"\n",
    "            else:\n",
    "                status_icon = \"❌\"\n",
    "            \n",
    "            report.append(f\"### {status_icon} {dimension.title()}\")\n",
    "            report.append(f\"- **Score:** {score:.1f}/100\")\n",
    "            report.append(f\"- **Weight:** {weight:.1%}\")\n",
    "            report.append(f\"- **Contribution:** {contribution:.1f} points\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(\"## 💡 Recommendations\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Priority recommendations based on scores\n",
    "        priority_recommendations = []\n",
    "        \n",
    "        if quality_results['dimension_scores']['validity'] < 70:\n",
    "            priority_recommendations.append(\"🔥 **HIGH PRIORITY:** Fix critical validation errors immediately\")\n",
    "        \n",
    "        if quality_results['dimension_scores']['completeness'] < 80:\n",
    "            priority_recommendations.append(\"🔥 **HIGH PRIORITY:** Address missing data issues\")\n",
    "        \n",
    "        if quality_results['dimension_scores']['uniqueness'] < 90:\n",
    "            priority_recommendations.append(\"⚠️ **MEDIUM PRIORITY:** Remove duplicate records\")\n",
    "        \n",
    "        if quality_results['dimension_scores']['consistency'] < 80:\n",
    "            priority_recommendations.append(\"⚠️ **MEDIUM PRIORITY:** Standardize data formats\")\n",
    "        \n",
    "        if quality_results['dimension_scores']['accuracy'] < 85:\n",
    "            priority_recommendations.append(\"💡 **LOW PRIORITY:** Review and clean suspicious data\")\n",
    "        \n",
    "        if priority_recommendations:\n",
    "            for rec in priority_recommendations:\n",
    "                report.append(f\"- {rec}\")\n",
    "        else:\n",
    "            report.append(\"- ✅ **Data quality is excellent!** Continue monitoring and maintain current standards.\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Detailed recommendations\n",
    "        report.append(\"### 📋 Detailed Action Items\")\n",
    "        report.append(\"\")\n",
    "        for i, recommendation in enumerate(quality_results['recommendations'], 1):\n",
    "            report.append(f\"{i}. {recommendation}\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Next Steps\n",
    "        report.append(\"## 🚀 Next Steps\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"1. **Address Critical Issues:** Fix all high-severity validation errors\")\n",
    "        report.append(\"2. **Implement Data Cleaning:** Apply transformations to improve quality scores\")\n",
    "        report.append(\"3. **Set Up Monitoring:** Establish ongoing data quality monitoring\")\n",
    "        report.append(\"4. **Update Processes:** Improve data collection processes at the source\")\n",
    "        report.append(\"5. **Regular Reviews:** Schedule periodic data quality assessments\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Footer\n",
    "        report.append(\"---\")\n",
    "        report.append(f\"*Report generated by Data Validation System v1.0*\")\n",
    "        report.append(f\"*For questions or support, contact the Data Engineering team*\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Generate comprehensive validation report\n",
    "report_generator = ValidationReportGenerator()\n",
    "validation_report = report_generator.generate_comprehensive_report(\n",
    "    df_orders, schema_result, business_result, quality_results\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 Comprehensive Validation Report Generated\")\n",
    "print(f\"Report length: {len(validation_report):,} characters\")\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(validation_report)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Building a Complete Validation System\n",
    "\n",
    "Let's put everything together into a complete, reusable validation system that you can use in your own projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete validation system\n",
    "print(\"🛠️ Complete Data Validation System\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "class DataValidationSystem:\n",
    "    \"\"\"\n",
    "    Complete data validation system combining all validation components\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the validation system\n",
    "        \n",
    "        Args:\n",
    "            config (Dict, optional): Configuration for validation rules\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        \n",
    "        # Initialize validators\n",
    "        self.schema_validator = SchemaValidator()\n",
    "        self.business_validator = BusinessRuleValidator()\n",
    "        self.quality_scorer = DataQualityScorer()\n",
    "        self.report_generator = ValidationReportGenerator()\n",
    "        \n",
    "        # Validation history\n",
    "        self.validation_history = []\n",
    "        \n",
    "        print(f\"🛠️ Complete validation system initialized\")\n",
    "        print(f\"  Components: Schema, Business Rules, Quality Scoring, Reporting\")\n",
    "    \n",
    "    def validate_dataset(self, df: pd.DataFrame, \n",
    "                        dataset_name: str = \"Unknown\",\n",
    "                        generate_report: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform complete validation of a dataset\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataset to validate\n",
    "            dataset_name (str): Name of the dataset\n",
    "            generate_report (bool): Whether to generate a detailed report\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete validation results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(f\"🔍 Starting validation of dataset: {dataset_name}\")\n",
    "        print(f\"📊 Dataset shape: {df.shape}\")\n",
    "        \n",
    "        validation_results = {\n",
    "            'dataset_name': dataset_name,\n",
    "            'validation_timestamp': start_time.isoformat(),\n",
    "            'dataset_info': {\n",
    "                'rows': len(df),\n",
    "                'columns': len(df.columns),\n",
    "                'column_names': list(df.columns),\n",
    "                'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Schema Validation\n",
    "            print(\"  📋 Running schema validation...\")\n",
    "            schema_result = self.schema_validator.validate_schema(df)\n",
    "            validation_results['schema_validation'] = schema_result\n",
    "            \n",
    "            # Step 2: Business Rule Validation\n",
    "            print(\"  🎯 Running business rule validation...\")\n",
    "            business_result = self.business_validator.validate_business_rules(df)\n",
    "            validation_results['business_validation'] = {\n",
    "                'is_valid': business_result.is_valid,\n",
    "                'errors': business_result.errors,\n",
    "                'warnings': business_result.warnings,\n",
    "                'valid_records': business_result.valid_records,\n",
    "                'total_records': business_result.total_records,\n",
    "                'quality_score': business_result.quality_score,\n",
    "                'validation_time': business_result.validation_time\n",
    "            }\n",
    "            \n",
    "            # Step 3: Quality Scoring\n",
    "            print(\"  📊 Calculating quality scores...\")\n",
    "            quality_results = self.quality_scorer.calculate_quality_score(df, business_result)\n",
    "            validation_results['quality_assessment'] = quality_results\n",
    "            \n",
    "            # Step 4: Generate Report\n",
    "            if generate_report:\n",
    "                print(\"  📋 Generating validation report...\")\n",
    "                report = self.report_generator.generate_comprehensive_report(\n",
    "                    df, schema_result, business_result, quality_results\n",
    "                )\n",
    "                validation_results['detailed_report'] = report\n",
    "            \n",
    "            # Calculate overall status\n",
    "            overall_valid = (\n",
    "                schema_result['is_valid'] and \n",
    "                business_result.is_valid and \n",
    "                quality_results['overall_score'] >= 70  # Minimum acceptable quality\n",
    "            )\n",
    "            \n",
    "            validation_results['overall_status'] = {\n",
    "                'is_valid': overall_valid,\n",
    "                'quality_level': quality_results['quality_level'],\n",
    "                'overall_score': quality_results['overall_score'],\n",
    "                'total_errors': len(business_result.errors),\n",
    "                'total_warnings': len(business_result.warnings)\n",
    "            }\n",
    "            \n",
    "            # Calculate execution time\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            validation_results['execution_time'] = execution_time\n",
    "            \n",
    "            # Add to history\n",
    "            self.validation_history.append({\n",
    "                'dataset_name': dataset_name,\n",
    "                'timestamp': start_time.isoformat(),\n",
    "                'overall_score': quality_results['overall_score'],\n",
    "                'is_valid': overall_valid,\n",
    "                'execution_time': execution_time\n",
    "            })\n",
    "            \n",
    "            print(f\"  ✅ Validation completed in {execution_time:.2f} seconds\")\n",
    "            print(f\"  📊 Overall Score: {quality_results['overall_score']:.1f}/100 ({quality_results['quality_level']})\")\n",
    "            print(f\"  🎯 Status: {'✅ PASSED' if overall_valid else '❌ FAILED'}\")\n",
    "            \n",
    "            return validation_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            error_result = {\n",
    "                'dataset_name': dataset_name,\n",
    "                'validation_timestamp': start_time.isoformat(),\n",
    "                'execution_time': execution_time,\n",
    "                'overall_status': {\n",
    "                    'is_valid': False,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"  ❌ Validation failed: {str(e)}\")\n",
    "            return error_result\n",
    "    \n",
    "    def get_validation_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get summary of all validations performed\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Any]: Validation history summary\n",
    "        \"\"\"\n",
    "        if not self.validation_history:\n",
    "            return {'message': 'No validations performed yet'}\n",
    "        \n",
    "        total_validations = len(self.validation_history)\n",
    "        successful_validations = sum(1 for v in self.validation_history if v['is_valid'])\n",
    "        avg_score = np.mean([v['overall_score'] for v in self.validation_history])\n",
    "        avg_time = np.mean([v['execution_time'] for v in self.validation_history])\n",
    "        \n",
    "        return {\n",
    "            'total_validations': total_validations,\n",
    "            'successful_validations': successful_validations,\n",
    "            'success_rate': (successful_validations / total_validations) * 100,\n",
    "            'average_quality_score': avg_score,\n",
    "            'average_execution_time': avg_time,\n",
    "            'recent_validations': self.validation_history[-5:]  # Last 5\n",
    "        }\n",
    "    \n",
    "    def save_validation_results(self, results: Dict[str, Any], \n",
    "                              output_path: str = \"validation_results.json\"):\n",
    "        \"\"\"\n",
    "        Save validation results to file\n",
    "        \n",
    "        Args:\n",
    "            results (Dict): Validation results\n",
    "            output_path (str): Output file path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert any non-serializable objects\n",
    "            serializable_results = self._make_serializable(results)\n",
    "            \n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(serializable_results, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"💾 Validation results saved to: {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to save results: {str(e)}\")\n",
    "    \n",
    "    def _make_serializable(self, obj):\n",
    "        \"\"\"Convert objects to JSON-serializable format\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: self._make_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._make_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.floating)):\n",
    "            return obj.item()\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "# Test the complete validation system\n",
    "print(\"\\n🧪 Testing Complete Validation System\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize the system\n",
    "validation_system = DataValidationSystem()\n",
    "\n",
    "# Validate our sample dataset\n",
    "results = validation_system.validate_dataset(\n",
    "    df_orders, \n",
    "    dataset_name=\"Sample E-commerce Orders\",\n",
    "    generate_report=True\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Validation Results Summary:\")\n",
    "print(f\"  Dataset: {results['dataset_name']}\")\n",
    "print(f\"  Overall Valid: {'✅ Yes' if results['overall_status']['is_valid'] else '❌ No'}\")\n",
    "print(f\"  Quality Score: {results['overall_status']['overall_score']:.1f}/100\")\n",
    "print(f\"  Quality Level: {results['overall_status']['quality_level']}\")\n",
    "print(f\"  Total Errors: {results['overall_status']['total_errors']}\")\n",
    "print(f\"  Total Warnings: {results['overall_status']['total_warnings']}\")\n",
    "print(f\"  Execution Time: {results['execution_time']:.2f} seconds\")\n",
    "\n",
    "# Get validation history summary\n",
    "summary = validation_system.get_validation_summary()\n",
    "print(f\"\\n📈 Validation History:\")\n",
    "print(f\"  Total Validations: {summary['total_validations']}\")\n",
    "print(f\"  Success Rate: {summary['success_rate']:.1f}%\")\n",
    "print(f\"  Average Quality Score: {summary['average_quality_score']:.1f}\")\n",
    "print(f\"  Average Execution Time: {summary['average_execution_time']:.2f}s\")\n",
    "\n",
    "# Save results (optional)\n",
    "save_results = input(\"\\nDo you want to save the validation results to a file? (y/n): \").lower().strip()\n",
    "if save_results == 'y':\n",
    "    validation_system.save_validation_results(results, \"sample_validation_results.json\")\n",
    "else:\n",
    "    print(\"Results not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "Congratulations! You've completed the data validation tutorial. Here's what you've mastered:\n",
    "\n",
    "### ✅ **Core Validation Skills**\n",
    "- **📋 Schema Validation**: Ensuring data structure and types are correct\n",
    "- **🎯 Business Rule Validation**: Checking data against business logic\n",
    "- **📊 Quality Scoring**: Quantifying data quality across multiple dimensions\n",
    "- **📋 Report Generation**: Creating comprehensive validation reports\n",
    "- **🛠️ System Integration**: Building complete validation workflows\n",
    "\n",
    "### ✅ **Quality Dimensions Mastered**\n",
    "- **Completeness**: Identifying and handling missing data\n",
    "- **Validity**: Ensuring data conforms to defined rules\n",
    "- **Consistency**: Checking for uniform data formats\n",
    "- **Accuracy**: Detecting suspicious or incorrect data\n",
    "- **Uniqueness**: Finding and handling duplicates\n",
    "\n",
    "### ✅ **Production-Ready Features**\n",
    "- **Comprehensive Error Handling**: Graceful failure management\n",
    "- **Detailed Reporting**: Actionable insights and recommendations\n",
    "- **Performance Monitoring**: Tracking validation execution times\n",
    "- **Historical Tracking**: Maintaining validation history\n",
    "- **Configurable Rules**: Flexible validation criteria\n",
    "\n",
    "### ✅ **Real-World Applications**\n",
    "- **Data Pipeline Quality Gates**: Preventing bad data from entering systems\n",
    "- **Regulatory Compliance**: Ensuring data meets legal requirements\n",
    "- **Business Intelligence**: Validating data before analysis\n",
    "- **Data Migration**: Ensuring data quality during transfers\n",
    "- **API Data Validation**: Checking incoming data quality\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What's Next?\n",
    "\n",
    "In the next tutorial, **\"05_data_transformation.ipynb\"**, you'll learn:\n",
    "- 🧹 How to clean and standardize messy data\n",
    "- ➕ Data enrichment and calculated fields\n",
    "- 📏 Data normalization and formatting\n",
    "- 🔄 Advanced transformation techniques\n",
    "- 🎯 Building transformation pipelines\n",
    "\n",
    "### 🎯 **Practice Exercise**\n",
    "\n",
    "Before moving to the next tutorial, try this exercise:\n",
    "\n",
    "1. **Create your own dataset** with intentional quality issues\n",
    "2. **Define custom business rules** for your domain\n",
    "3. **Use the validation system** we built to validate your data\n",
    "4. **Analyze the validation report** and identify improvement areas\n",
    "5. **Set quality thresholds** appropriate for your use case\n",
    "6. **Create a monitoring dashboard** to track quality over time\n",
    "\n",
    "### 💡 **Advanced Validation Ideas:**\n",
    "- **Statistical Validation**: Detect outliers using statistical methods\n",
    "- **Cross-Field Validation**: Validate relationships between fields\n",
    "- **Time-Series Validation**: Check for temporal consistency\n",
    "- **Reference Data Validation**: Validate against master data\n",
    "- **Machine Learning Validation**: Use ML to detect anomalies\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work mastering data validation! 🎉**\n",
    "\n",
    "You now have the skills to ensure data quality in any system. Data validation is the foundation of reliable analytics and business intelligence - you're building systems that organizations can trust.\n",
    "\n",
    "**Happy Validating! 🔍**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}