{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Pipeline Monitoring and Operations\n",
    "\n",
    "Welcome to the **final tutorial** in our Data Ingestion Pipeline series! In this comprehensive notebook, you'll learn how to monitor, operate, and maintain production data pipelines with enterprise-grade observability and alerting.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ‚úÖ Implement comprehensive pipeline monitoring and observability\n",
    "- ‚úÖ Create real-time dashboards and visualizations\n",
    "- ‚úÖ Set up automated alerting and notification systems\n",
    "- ‚úÖ Perform pipeline performance analysis and optimization\n",
    "- ‚úÖ Establish operational procedures and best practices\n",
    "- ‚úÖ Build a complete monitoring and alerting framework\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Why Pipeline Monitoring Matters\n",
    "\n",
    "In production environments, data pipelines are critical business infrastructure. Without proper monitoring, you risk:\n",
    "\n",
    "### ‚ùå **Without Monitoring**\n",
    "- **Silent Failures**: Pipelines fail without anyone knowing\n",
    "- **Data Quality Issues**: Bad data propagates downstream\n",
    "- **Performance Degradation**: Slow pipelines impact business operations\n",
    "- **Resource Waste**: Inefficient resource utilization\n",
    "- **Compliance Risks**: Missing SLA requirements\n",
    "\n",
    "### ‚úÖ **With Proper Monitoring**\n",
    "- **Proactive Issue Detection**: Catch problems before they impact users\n",
    "- **Data Quality Assurance**: Continuous quality monitoring\n",
    "- **Performance Optimization**: Identify and fix bottlenecks\n",
    "- **Resource Efficiency**: Optimize compute and storage usage\n",
    "- **Compliance Confidence**: Meet SLA and regulatory requirements\n",
    "\n",
    "### üìä **Key Monitoring Dimensions**\n",
    "```\n",
    "üîÑ OPERATIONAL METRICS     üìä DATA QUALITY METRICS     üí∞ BUSINESS METRICS\n",
    "‚îú‚îÄ‚îÄ Pipeline Success Rate  ‚îú‚îÄ‚îÄ Data Completeness       ‚îú‚îÄ‚îÄ Records Processed\n",
    "‚îú‚îÄ‚îÄ Execution Time         ‚îú‚îÄ‚îÄ Data Accuracy           ‚îú‚îÄ‚îÄ Revenue Impact\n",
    "‚îú‚îÄ‚îÄ Resource Usage         ‚îú‚îÄ‚îÄ Schema Compliance       ‚îú‚îÄ‚îÄ Customer Impact\n",
    "‚îú‚îÄ‚îÄ Error Rates            ‚îú‚îÄ‚îÄ Freshness               ‚îú‚îÄ‚îÄ SLA Compliance\n",
    "‚îî‚îÄ‚îÄ Throughput             ‚îî‚îÄ‚îÄ Consistency             ‚îî‚îÄ‚îÄ Cost Efficiency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "# Import our pipeline components\n",
    "try:\n",
    "    from pipeline.pipeline_manager import PipelineManager\n",
    "    from storage.database_manager import DatabaseManager\n",
    "    from utils.config import config\n",
    "    from utils.helpers import ensure_directory_exists, format_duration\n",
    "    print(\"‚úÖ Pipeline components imported successfully!\")\nexcept ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Import error: {e}\")\n",
    "    print(\"üìù Note: Some components may not be available in this demo environment\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"üìà Pipeline monitoring environment ready!\")\n",
    "print(f\"üïê Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Monitoring Infrastructure Setup\n",
    "\n",
    "Let's build a comprehensive monitoring system that tracks all aspects of our pipeline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Pipeline Monitor Class\n",
    "class PipelineMonitor:\n",
    "    \"\"\"Comprehensive pipeline monitoring and observability system\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path=\"../data/monitoring.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.setup_monitoring_database()\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Monitoring configuration\n",
    "        self.metrics_config = {\n",
    "            'collection_interval': 30,  # seconds\n",
    "            'retention_days': 30,\n",
    "            'alert_thresholds': {\n",
    "                'error_rate': 0.05,  # 5%\n",
    "                'execution_time': 300,  # 5 minutes\n",
    "                'quality_score': 80,  # 80%\n",
    "                'throughput_min': 10  # records/second\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"üìà Pipeline Monitor initialized: {self.db_path}\")\n",
    "    \n",
    "    def setup_monitoring_database(self):\n",
    "        \"\"\"Setup monitoring database tables\"\"\"\n",
    "        ensure_directory_exists(os.path.dirname(self.db_path))\n",
    "        \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Pipeline execution metrics\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS pipeline_metrics (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    pipeline_id TEXT NOT NULL,\n",
    "                    timestamp TEXT NOT NULL,\n",
    "                    metric_name TEXT NOT NULL,\n",
    "                    metric_value REAL NOT NULL,\n",
    "                    metric_type TEXT NOT NULL,\n",
    "                    stage TEXT,\n",
    "                    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Data quality metrics\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS quality_metrics (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    pipeline_id TEXT NOT NULL,\n",
    "                    timestamp TEXT NOT NULL,\n",
    "                    total_records INTEGER NOT NULL,\n",
    "                    valid_records INTEGER NOT NULL,\n",
    "                    quality_score REAL NOT NULL,\n",
    "                    data_source TEXT,\n",
    "                    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Alert history\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS alerts (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    pipeline_id TEXT NOT NULL,\n",
    "                    timestamp TEXT NOT NULL,\n",
    "                    alert_type TEXT NOT NULL,\n",
    "                    severity TEXT NOT NULL,\n",
    "                    message TEXT NOT NULL,\n",
    "                    resolved BOOLEAN DEFAULT FALSE,\n",
    "                    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # System resource metrics\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS resource_metrics (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    timestamp TEXT NOT NULL,\n",
    "                    cpu_usage REAL,\n",
    "                    memory_usage REAL,\n",
    "                    disk_usage REAL,\n",
    "                    network_io REAL,\n",
    "                    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            conn.commit()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup structured logging for monitoring\"\"\"\n",
    "        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        \n",
    "        # Create logger\n",
    "        self.logger = logging.getLogger('PipelineMonitor')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter(log_format))\n",
    "        self.logger.addHandler(console_handler)\n",
    "        \n",
    "        # File handler\n",
    "        ensure_directory_exists('../logs')\n",
    "        file_handler = logging.FileHandler('../logs/monitoring.log')\n",
    "        file_handler.setFormatter(logging.Formatter(log_format))\n",
    "        self.logger.addHandler(file_handler)\n",
    "    \n",
    "    def record_pipeline_metric(self, pipeline_id: str, metric_name: str, \n",
    "                             metric_value: float, metric_type: str, \n",
    "                             stage: str = None):\n",
    "        \"\"\"Record a pipeline performance metric\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('''\n",
    "                INSERT INTO pipeline_metrics \n",
    "                (pipeline_id, timestamp, metric_name, metric_value, metric_type, stage)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (pipeline_id, timestamp, metric_name, metric_value, metric_type, stage))\n",
    "            conn.commit()\n",
    "        \n",
    "        self.logger.info(f\"üìä Metric recorded: {metric_name}={metric_value} ({metric_type})\")\n",
    "    \n",
    "    def record_quality_metric(self, pipeline_id: str, total_records: int, \n",
    "                            valid_records: int, quality_score: float, \n",
    "                            data_source: str = None):\n",
    "        \"\"\"Record data quality metrics\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('''\n",
    "                INSERT INTO quality_metrics \n",
    "                (pipeline_id, timestamp, total_records, valid_records, quality_score, data_source)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (pipeline_id, timestamp, total_records, valid_records, quality_score, data_source))\n",
    "            conn.commit()\n",
    "        \n",
    "        self.logger.info(f\"üìä Quality metric recorded: {quality_score:.1f}% ({valid_records}/{total_records})\")\n",
    "    \n",
    "    def create_alert(self, pipeline_id: str, alert_type: str, \n",
    "                    severity: str, message: str):\n",
    "        \"\"\"Create an alert for pipeline issues\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('''\n",
    "                INSERT INTO alerts \n",
    "                (pipeline_id, timestamp, alert_type, severity, message)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            ''', (pipeline_id, timestamp, alert_type, severity, message))\n",
    "            conn.commit()\n",
    "        \n",
    "        severity_icon = {'LOW': 'üü°', 'MEDIUM': 'üü†', 'HIGH': 'üî¥', 'CRITICAL': 'üö®'}\n",
    "        icon = severity_icon.get(severity, '‚ö†Ô∏è')\n",
    "        \n",
    "        self.logger.warning(f\"{icon} ALERT [{severity}]: {alert_type} - {message}\")\n",
    "        \n",
    "        # In production, this would trigger notifications (email, Slack, etc.)\n",
    "        self.send_notification(alert_type, severity, message)\n",
    "    \n",
    "    def send_notification(self, alert_type: str, severity: str, message: str):\n",
    "        \"\"\"Send notification for alerts (simulated)\"\"\"\n",
    "        # In production, implement actual notification logic\n",
    "        notification_methods = {\n",
    "            'CRITICAL': ['email', 'sms', 'slack'],\n",
    "            'HIGH': ['email', 'slack'],\n",
    "            'MEDIUM': ['slack'],\n",
    "            'LOW': ['log_only']\n",
    "        }\n",
    "        \n",
    "        methods = notification_methods.get(severity, ['log_only'])\n",
    "        self.logger.info(f\"üì¢ Notification sent via: {', '.join(methods)}\")\n",
    "    \n",
    "    def check_alert_conditions(self, pipeline_id: str, metrics: Dict[str, Any]):\n",
    "        \"\"\"Check if any alert conditions are met\"\"\"\n",
    "        thresholds = self.metrics_config['alert_thresholds']\n",
    "        \n",
    "        # Check error rate\n",
    "        if 'error_rate' in metrics and metrics['error_rate'] > thresholds['error_rate']:\n",
    "            self.create_alert(\n",
    "                pipeline_id, 'HIGH_ERROR_RATE', 'HIGH',\n",
    "                f\"Error rate {metrics['error_rate']:.1%} exceeds threshold {thresholds['error_rate']:.1%}\"\n",
    "            )\n",
    "        \n",
    "        # Check execution time\n",
    "        if 'execution_time' in metrics and metrics['execution_time'] > thresholds['execution_time']:\n",
    "            self.create_alert(\n",
    "                pipeline_id, 'SLOW_EXECUTION', 'MEDIUM',\n",
    "                f\"Execution time {metrics['execution_time']:.1f}s exceeds threshold {thresholds['execution_time']}s\"\n",
    "            )\n",
    "        \n",
    "        # Check data quality\n",
    "        if 'quality_score' in metrics and metrics['quality_score'] < thresholds['quality_score']:\n",
    "            self.create_alert(\n",
    "                pipeline_id, 'LOW_DATA_QUALITY', 'HIGH',\n",
    "                f\"Data quality {metrics['quality_score']:.1f}% below threshold {thresholds['quality_score']}%\"\n",
    "            )\n",
    "        \n",
    "        # Check throughput\n",
    "        if 'throughput' in metrics and metrics['throughput'] < thresholds['throughput_min']:\n",
    "            self.create_alert(\n",
    "                pipeline_id, 'LOW_THROUGHPUT', 'MEDIUM',\n",
    "                f\"Throughput {metrics['throughput']:.1f} records/s below threshold {thresholds['throughput_min']}\"\n",
    "            )\n",
    "\n",
    "# Initialize monitoring system\n",
    "monitor = PipelineMonitor()\n",
    "\n",
    "print(\"‚úÖ Pipeline monitoring system initialized!\")\n",
    "print(f\"üìä Monitoring database: {monitor.db_path}\")\n",
    "print(f\"‚öôÔ∏è Alert thresholds configured: {monitor.metrics_config['alert_thresholds']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 2: Monitored Pipeline Execution\n",
    "\n",
    "Let's create a pipeline wrapper that automatically collects monitoring data during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitored Pipeline Wrapper\n",
    "class MonitoredPipeline:\n",
    "    \"\"\"Pipeline wrapper with comprehensive monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor: PipelineMonitor):\n",
    "        self.monitor = monitor\n",
    "        self.pipeline_id = f\"MONITORED-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.execution_start = None\n",
    "        self.stage_times = {}\n",
    "        \n",
    "    def execute_monitored_pipeline(self, sample_data_size: int = 100):\n",
    "        \"\"\"Execute pipeline with comprehensive monitoring\"\"\"\n",
    "        self.execution_start = datetime.now()\n",
    "        self.monitor.logger.info(f\"üöÄ Starting monitored pipeline: {self.pipeline_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Generate sample data for demonstration\n",
    "            sample_data = self.generate_sample_data(sample_data_size)\n",
    "            \n",
    "            # Stage 1: Data Ingestion (Monitored)\n",
    "            ingested_data = self.monitored_ingestion_stage(sample_data)\n",
    "            \n",
    "            # Stage 2: Data Validation (Monitored)\n",
    "            validated_data = self.monitored_validation_stage(ingested_data)\n",
    "            \n",
    "            # Stage 3: Data Transformation (Monitored)\n",
    "            transformed_data = self.monitored_transformation_stage(validated_data)\n",
    "            \n",
    "            # Stage 4: Data Storage (Monitored)\n",
    "            storage_result = self.monitored_storage_stage(transformed_data)\n",
    "            \n",
    "            # Calculate overall metrics\n",
    "            total_time = (datetime.now() - self.execution_start).total_seconds()\n",
    "            throughput = len(transformed_data) / total_time if total_time > 0 else 0\n",
    "            \n",
    "            # Record overall pipeline metrics\n",
    "            self.monitor.record_pipeline_metric(\n",
    "                self.pipeline_id, 'total_execution_time', total_time, 'duration'\n",
    "            )\n",
    "            self.monitor.record_pipeline_metric(\n",
    "                self.pipeline_id, 'throughput', throughput, 'rate'\n",
    "            )\n",
    "            self.monitor.record_pipeline_metric(\n",
    "                self.pipeline_id, 'records_processed', len(transformed_data), 'count'\n",
    "            )\n",
    "            \n",
    "            # Check alert conditions\n",
    "            pipeline_metrics = {\n",
    "                'execution_time': total_time,\n",
    "                'throughput': throughput,\n",
    "                'error_rate': 0.0,  # No errors in this demo\n",
    "                'quality_score': transformed_data['quality_score'].mean() if 'quality_score' in transformed_data.columns else 100\n",
    "            }\n",
    "            \n",
    "            self.monitor.check_alert_conditions(self.pipeline_id, pipeline_metrics)\n",
    "            \n",
    "            self.monitor.logger.info(f\"‚úÖ Pipeline completed successfully in {total_time:.2f}s\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'pipeline_id': self.pipeline_id,\n",
    "                'execution_time': total_time,\n",
    "                'records_processed': len(transformed_data),\n",
    "                'throughput': throughput,\n",
    "                'stage_times': self.stage_times,\n",
    "                'final_data': transformed_data\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_time = (datetime.now() - self.execution_start).total_seconds()\n",
    "            \n",
    "            # Record failure metrics\n",
    "            self.monitor.record_pipeline_metric(\n",
    "                self.pipeline_id, 'execution_failed', 1, 'boolean'\n",
    "            )\n",
    "            \n",
    "            # Create critical alert\n",
    "            self.monitor.create_alert(\n",
    "                self.pipeline_id, 'PIPELINE_FAILURE', 'CRITICAL',\n",
    "                f\"Pipeline execution failed: {str(e)}\"\n",
    "            )\n",
    "            \n",
    "            self.monitor.logger.error(f\"‚ùå Pipeline failed after {total_time:.2f}s: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                'success': False,\n",
    "                'pipeline_id': self.pipeline_id,\n",
    "                'execution_time': total_time,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def generate_sample_data(self, size: int) -> pd.DataFrame:\n",
    "        \"\"\"Generate sample data for monitoring demonstration\"\"\"\n",
    "        np.random.seed(42)  # For reproducible results\n",
    "        \n",
    "        # Generate realistic e-commerce data\n",
    "        products = ['iPhone 15', 'MacBook Pro', 'AirPods Pro', 'iPad Air', 'Apple Watch', \n",
    "                   'Nintendo Switch', 'PlayStation 5', 'Xbox Series X', 'Kindle Paperwhite']\n",
    "        \n",
    "        customers = [f\"Customer_{i:03d}\" for i in range(1, min(size//2, 50) + 1)]\n",
    "        sources = ['website', 'mobile_app', 'store', 'phone']\n",
    "        locations = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia']\n",
    "        \n",
    "        data = {\n",
    "            'order_id': [f\"ORD-2024-{i:06d}\" for i in range(1, size + 1)],\n",
    "            'customer_name': np.random.choice(customers, size),\n",
    "            'product': np.random.choice(products, size),\n",
    "            'quantity': np.random.randint(1, 5, size),\n",
    "            'price': np.round(np.random.uniform(50, 2000, size), 2),\n",
    "            'order_date': pd.date_range(start='2024-01-01', periods=size, freq='H'),\n",
    "            'source': np.random.choice(sources, size),\n",
    "            'store_location': np.random.choice(locations, size),\n",
    "            'customer_email': [f\"customer{i}@example.com\" for i in np.random.randint(1, 100, size)]\n",
    "        }\n",
    "        \n",
    "        # Add some data quality issues for realistic monitoring\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Introduce some missing values (5%)\n",
    "        missing_indices = np.random.choice(df.index, size=int(size * 0.05), replace=False)\n",
    "        df.loc[missing_indices, 'customer_email'] = None\n",
    "        \n",
    "        # Introduce some invalid prices (2%)\n",
    "        invalid_indices = np.random.choice(df.index, size=int(size * 0.02), replace=False)\n",
    "        df.loc[invalid_indices, 'price'] = -1\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def monitored_ingestion_stage(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Ingestion stage with monitoring\"\"\"\n",
    "        stage_start = time.time()\n",
    "        self.monitor.logger.info(\"üì• Starting ingestion stage\")\n",
    "        \n",
    "        # Simulate ingestion processing\n",
    "        time.sleep(0.1)  # Simulate processing time\n",
    "        \n",
    "        # Add ingestion metadata\n",
    "        data['ingested_at'] = datetime.now().isoformat()\n",
    "        data['pipeline_id'] = self.pipeline_id\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        self.stage_times['ingestion'] = stage_time\n",
    "        \n",
    "        # Record stage metrics\n",
    "        self.monitor.record_pipeline_metric(\n",
    "            self.pipeline_id, 'ingestion_time', stage_time, 'duration', 'ingestion'\n",
    "        )\n",
    "        self.monitor.record_pipeline_metric(\n",
    "            self.pipeline_id, 'ingestion_records', len(data), 'count', 'ingestion'\n",
    "        )\n",
    "        \n",
    "        self.monitor.logger.info(f\"‚úÖ Ingestion completed: {len(data)} records in {stage_time:.2f}s\")\n",
    "        return data\n",
    "    \n",
    "    def monitored_validation_stage(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Validation stage with monitoring\"\"\"\n",
    "        stage_start = time.time()\n",
    "        self.monitor.logger.info(\"üîç Starting validation stage\")\n",
    "        \n",
    "        # Perform validation\n",
    "        total_records = len(data)\n",
    "        \n",
    "        # Check for invalid prices\n",
    "        valid_prices = data['price'] > 0\n",
    "        invalid_price_count = (~valid_prices).sum()\n",
    "        \n",
    "        # Check for missing emails\n",
    "        missing_email_count = data['customer_email'].isnull().sum()\n",
    "        \n",
    "        # Calculate quality score\n",
    "        quality_issues = invalid_price_count + missing_email_count\n",
    "        quality_score = ((total_records - quality_issues) / total_records) * 100\n",
    "        valid_records = total_records - quality_issues\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        self.stage_times['validation'] = stage_time\n",
    "        \n",
    "        # Record validation metrics\n",
    "        self.monitor.record_pipeline_metric(\n",
    "            self.pipeline_id, 'validation_time', stage_time, 'duration', 'validation'\n",
    "        )\n",
    "        self.monitor.record_quality_metric(\n",
    "            self.pipeline_id, total_records, valid_records, quality_score\n",
    "        )\n",
    "        \n",
    "        # Add validation results to data\n",
    "        data['is_valid'] = valid_prices & data['customer_email'].notna()\n",
    "        data['validation_score'] = data['is_valid'].astype(int) * 100\n",
    "        \n",
    "        self.monitor.logger.info(f\"‚úÖ Validation completed: {quality_score:.1f}% quality in {stage_time:.2f}s\")\n",
    "        \n",
    "        if quality_score < 90:\n",
    "            self.monitor.logger.warning(f\"‚ö†Ô∏è Data quality below 90%: {quality_score:.1f}%\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def monitored_transformation_stage(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transformation stage with monitoring\"\"\"\n",
    "        stage_start = time.time()\n",
    "        self.monitor.logger.info(\"üßπ Starting transformation stage\")\n",
    "        \n",
    "        # Clean invalid data\n",
    "        original_count = len(data)\n",
    "        data = data[data['price'] > 0].copy()  # Remove invalid prices\n",
    "        cleaned_count = len(data)\n",
    "        \n",
    "        # Add transformations\n",
    "        data['total_amount'] = data['quantity'] * data['price']\n",
    "        data['order_size'] = pd.cut(\n",
    "            data['total_amount'],\n",
    "            bins=[0, 100, 500, 1000, 2000, float('inf')],\n",
    "            labels=['XSmall', 'Small', 'Medium', 'Large', 'XLarge']\n",
    "        )\n",
    "        \n",
    "        # Customer segmentation\n",
    "        def categorize_customer(amount):\n",
    "            if amount >= 1500: return 'VIP'\n",
    "            elif amount >= 800: return 'Premium'\n",
    "            elif amount >= 300: return 'Standard'\n",
    "            else: return 'Budget'\n",
    "        \n",
    "        data['customer_segment'] = data['total_amount'].apply(categorize_customer)\n",
    "        \n",
    "        # Add quality score\n",
    "        data['quality_score'] = data['validation_score']  # Use validation score\n",
    "        \n",
    "        # Add transformation metadata\n",
    "        data['transformed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        self.stage_times['transformation'] = stage_time\n",
    "        \n",
    "        # Record transformation metrics\n",
    "        self.monitor.record_pipeline_metric(\n",
    "            self.pipeline_id, 'transformation_time', stage_time, 'duration', 'transformation'\n",
    "        )\n",
    "        self.monitor.record_pipeline_metric(\n",
    "            self.pipeline_id, 'records_cleaned', original_count - cleaned_count, 'count', 'transformation'\n",
    "        )\n",
    "        self.monitor.record_pipeline_metric(\n",
    "            self.pipeline_id, 'transformation_records', cleaned_count, 'count', 'transformation'\n",
    "        )\n",
    "        \n",
    "        self.monitor.logger.info(f\"‚úÖ Transformation completed: {cleaned_count} records in {stage_time:.2f}s\")\n",
    "        \n",
    "        if original_count != cleaned_count:\n",
    "            removed = original_count - cleaned_count\n",
    "            self.monitor.logger.warning(f\"‚ö†Ô∏è Removed {removed} invalid records during transformation\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def monitored_storage_stage(self, data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Storage stage with monitoring\"\"\"\n",
    "        stage_start = time.time()\n",
    "        self.monitor.logger.info(\"üíæ Starting storage stage\")\n",
    "        \n",
    "        # Simulate storage operations\n",
    "        time.sleep(0.05)  # Simulate storage time\n",
    "        \n",
    "        # In production, this would save to actual storage\n",
    "        storage_operations = [\n",
    "            f\"Saved {len(data)} records to database\",\n",
    "            f\"Exported data to CSV file\",\n",
    "            f\"Created backup copy\"\n",
    "        ]\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        self.stage_times['storage'] = stage_time\n",
    "        \n",
    "        # Record storage metrics\n",
    "        self.monitor.record_pipeline_metric(\n",
    "            self.pipeline_id, 'storage_time', stage_time, 'duration', 'storage'\n",
    "        )\n",
    "        self.monitor.record_pipeline_metric(\n",
    "            self.pipeline_id, 'records_stored', len(data), 'count', 'storage'\n",
    "        )\n",
    "        \n",
    "        self.monitor.logger.info(f\"‚úÖ Storage completed: {len(data)} records in {stage_time:.2f}s\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'records_stored': len(data),\n",
    "            'operations': storage_operations,\n",
    "            'storage_time': stage_time\n",
    "        }\n",
    "\n",
    "# Initialize monitored pipeline\n",
    "monitored_pipeline = MonitoredPipeline(monitor)\n",
    "print(f\"üîÑ Monitored pipeline initialized: {monitored_pipeline.pipeline_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Execute Monitored Pipeline\n",
    "\n",
    "Let's run our pipeline with comprehensive monitoring and see the metrics in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute monitored pipeline with different scenarios\n",
    "print(\"üöÄ EXECUTING MONITORED PIPELINE SCENARIOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario 1: Normal execution\n",
    "print(\"\\nüìä Scenario 1: Normal Pipeline Execution\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result1 = monitored_pipeline.execute_monitored_pipeline(sample_data_size=150)\n",
    "\n",
    "print(f\"\\nüìà Execution Results:\")\n",
    "print(f\"  Success: {'‚úÖ Yes' if result1['success'] else '‚ùå No'}\")\n",
    "print(f\"  Pipeline ID: {result1['pipeline_id']}\")\n",
    "print(f\"  Execution Time: {result1['execution_time']:.2f}s\")\n",
    "print(f\"  Records Processed: {result1['records_processed']:,}\")\n",
    "print(f\"  Throughput: {result1['throughput']:.1f} records/second\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Stage Breakdown:\")\n",
    "for stage, time_taken in result1['stage_times'].items():\n",
    "    percentage = (time_taken / result1['execution_time']) * 100\n",
    "    print(f\"  {stage.title()}: {time_taken:.3f}s ({percentage:.1f}%)\")\n",
    "\n",
    "# Scenario 2: Large dataset (stress test)\n",
    "print(\"\\n\\nüìä Scenario 2: Large Dataset Processing\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "large_pipeline = MonitoredPipeline(monitor)\n",
    "result2 = large_pipeline.execute_monitored_pipeline(sample_data_size=1000)\n",
    "\n",
    "print(f\"\\nüìà Large Dataset Results:\")\n",
    "print(f\"  Success: {'‚úÖ Yes' if result2['success'] else '‚ùå No'}\")\n",
    "print(f\"  Pipeline ID: {result2['pipeline_id']}\")\n",
    "print(f\"  Execution Time: {result2['execution_time']:.2f}s\")\n",
    "print(f\"  Records Processed: {result2['records_processed']:,}\")\n",
    "print(f\"  Throughput: {result2['throughput']:.1f} records/second\")\n",
    "\n",
    "# Scenario 3: Small dataset (fast execution)\n",
    "print(\"\\n\\nüìä Scenario 3: Small Dataset Processing\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "small_pipeline = MonitoredPipeline(monitor)\n",
    "result3 = small_pipeline.execute_monitored_pipeline(sample_data_size=50)\n",
    "\n",
    "print(f\"\\nüìà Small Dataset Results:\")\n",
    "print(f\"  Success: {'‚úÖ Yes' if result3['success'] else '‚ùå No'}\")\n",
    "print(f\"  Pipeline ID: {result3['pipeline_id']}\")\n",
    "print(f\"  Execution Time: {result3['execution_time']:.2f}s\")\n",
    "print(f\"  Records Processed: {result3['records_processed']:,}\")\n",
    "print(f\"  Throughput: {result3['throughput']:.1f} records/second\")\n",
    "\n",
    "# Store results for analysis\n",
    "execution_results = [result1, result2, result3]\n",
    "\n",
    "print(f\"\\n‚úÖ Completed {len(execution_results)} monitored pipeline executions\")\n",
    "print(f\"üìä All metrics have been recorded in the monitoring database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Real-Time Monitoring Dashboard\n",
    "\n",
    "Let's create comprehensive dashboards to visualize our pipeline performance and health metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring Dashboard Generator\n",
    "class MonitoringDashboard:\n",
    "    \"\"\"Generate comprehensive monitoring dashboards\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor: PipelineMonitor):\n",
    "        self.monitor = monitor\n",
    "    \n",
    "    def get_pipeline_metrics(self, hours_back: int = 24) -> pd.DataFrame:\n",
    "        \"\"\"Get pipeline metrics from the last N hours\"\"\"\n",
    "        cutoff_time = (datetime.now() - timedelta(hours=hours_back)).isoformat()\n",
    "        \n",
    "        with sqlite3.connect(self.monitor.db_path) as conn:\n",
    "            query = '''\n",
    "                SELECT * FROM pipeline_metrics \n",
    "                WHERE timestamp >= ? \n",
    "                ORDER BY timestamp DESC\n",
    "            '''\n",
    "            return pd.read_sql_query(query, conn, params=[cutoff_time])\n",
    "    \n",
    "    def get_quality_metrics(self, hours_back: int = 24) -> pd.DataFrame:\n",
    "        \"\"\"Get quality metrics from the last N hours\"\"\"\n",
    "        cutoff_time = (datetime.now() - timedelta(hours=hours_back)).isoformat()\n",
    "        \n",
    "        with sqlite3.connect(self.monitor.db_path) as conn:\n",
    "            query = '''\n",
    "                SELECT * FROM quality_metrics \n",
    "                WHERE timestamp >= ? \n",
    "                ORDER BY timestamp DESC\n",
    "            '''\n",
    "            return pd.read_sql_query(query, conn, params=[cutoff_time])\n",
    "    \n",
    "    def get_alerts(self, hours_back: int = 24) -> pd.DataFrame:\n",
    "        \"\"\"Get alerts from the last N hours\"\"\"\n",
    "        cutoff_time = (datetime.now() - timedelta(hours=hours_back)).isoformat()\n",
    "        \n",
    "        with sqlite3.connect(self.monitor.db_path) as conn:\n",
    "            query = '''\n",
    "                SELECT * FROM alerts \n",
    "                WHERE timestamp >= ? \n",
    "                ORDER BY timestamp DESC\n",
    "            '''\n",
    "            return pd.read_sql_query(query, conn, params=[cutoff_time])\n",
    "    \n",
    "    def create_performance_dashboard(self):\n",
    "        \"\"\"Create comprehensive performance dashboard\"\"\"\n",
    "        # Get data\n",
    "        pipeline_metrics = self.get_pipeline_metrics()\n",
    "        quality_metrics = self.get_quality_metrics()\n",
    "        alerts = self.get_alerts()\n",
    "        \n",
    "        if pipeline_metrics.empty:\n",
    "            print(\"‚ö†Ô∏è No pipeline metrics available for dashboard\")\n",
    "            return\n",
    "        \n",
    "        # Create dashboard\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        \n",
    "        # 1. Pipeline Execution Times\n",
    "        ax1 = plt.subplot(3, 3, 1)\n",
    "        execution_times = pipeline_metrics[pipeline_metrics['metric_name'] == 'total_execution_time']\n",
    "        if not execution_times.empty:\n",
    "            execution_times['timestamp'] = pd.to_datetime(execution_times['timestamp'])\n",
    "            ax1.plot(execution_times['timestamp'], execution_times['metric_value'], \n",
    "                    marker='o', linewidth=2, markersize=6)\n",
    "            ax1.set_title('Pipeline Execution Times', fontsize=14, fontweight='bold')\n",
    "            ax1.set_ylabel('Time (seconds)')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Throughput Over Time\n",
    "        ax2 = plt.subplot(3, 3, 2)\n",
    "        throughput_data = pipeline_metrics[pipeline_metrics['metric_name'] == 'throughput']\n",
    "        if not throughput_data.empty:\n",
    "            throughput_data['timestamp'] = pd.to_datetime(throughput_data['timestamp'])\n",
    "            ax2.plot(throughput_data['timestamp'], throughput_data['metric_value'], \n",
    "                    marker='s', linewidth=2, markersize=6, color='green')\n",
    "            ax2.set_title('Pipeline Throughput', fontsize=14, fontweight='bold')\n",
    "            ax2.set_ylabel('Records/Second')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Records Processed\n",
    "        ax3 = plt.subplot(3, 3, 3)\n",
    "        records_data = pipeline_metrics[pipeline_metrics['metric_name'] == 'records_processed']\n",
    "        if not records_data.empty:\n",
    "            records_data['timestamp'] = pd.to_datetime(records_data['timestamp'])\n",
    "            bars = ax3.bar(range(len(records_data)), records_data['metric_value'], \n",
    "                          color='skyblue', alpha=0.7)\n",
    "            ax3.set_title('Records Processed per Run', fontsize=14, fontweight='bold')\n",
    "            ax3.set_ylabel('Number of Records')\n",
    "            ax3.set_xlabel('Pipeline Run')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for i, bar in enumerate(bars):\n",
    "                height = bar.get_height()\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                        f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 4. Data Quality Trends\n",
    "        ax4 = plt.subplot(3, 3, 4)\n",
    "        if not quality_metrics.empty:\n",
    "            quality_metrics['timestamp'] = pd.to_datetime(quality_metrics['timestamp'])\n",
    "            ax4.plot(quality_metrics['timestamp'], quality_metrics['quality_score'], \n",
    "                    marker='D', linewidth=2, markersize=6, color='orange')\n",
    "            ax4.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Threshold (80%)')\n",
    "            ax4.set_title('Data Quality Score Trends', fontsize=14, fontweight='bold')\n",
    "            ax4.set_ylabel('Quality Score (%)')\n",
    "            ax4.set_ylim(0, 100)\n",
    "            ax4.tick_params(axis='x', rotation=45)\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            ax4.legend()\n",
    "        \n",
    "        # 5. Stage Performance Breakdown\n",
    "        ax5 = plt.subplot(3, 3, 5)\n",
    "        stage_metrics = pipeline_metrics[pipeline_metrics['stage'].notna()]\n",
    "        if not stage_metrics.empty:\n",
    "            stage_avg = stage_metrics.groupby('stage')['metric_value'].mean()\n",
    "            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "            bars = ax5.bar(stage_avg.index, stage_avg.values, color=colors[:len(stage_avg)])\n",
    "            ax5.set_title('Average Stage Performance', fontsize=14, fontweight='bold')\n",
    "            ax5.set_ylabel('Average Time (seconds)')\n",
    "            ax5.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax5.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                        f'{height:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 6. Alert Summary\n",
    "        ax6 = plt.subplot(3, 3, 6)\n",
    "        if not alerts.empty:\n",
    "            alert_counts = alerts['severity'].value_counts()\n",
    "            colors_alert = {'LOW': 'yellow', 'MEDIUM': 'orange', 'HIGH': 'red', 'CRITICAL': 'darkred'}\n",
    "            pie_colors = [colors_alert.get(severity, 'gray') for severity in alert_counts.index]\n",
    "            \n",
    "            wedges, texts, autotexts = ax6.pie(alert_counts.values, labels=alert_counts.index, \n",
    "                                              autopct='%1.0f%%', colors=pie_colors, startangle=90)\n",
    "            ax6.set_title('Alert Distribution by Severity', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('white')\n",
    "                autotext.set_fontweight('bold')\n",
    "        else:\n",
    "            ax6.text(0.5, 0.5, '‚úÖ No Alerts', ha='center', va='center', \n",
    "                    transform=ax6.transAxes, fontsize=16, fontweight='bold', color='green')\n",
    "            ax6.set_title('Alert Status', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 7. Pipeline Success Rate\n",
    "        ax7 = plt.subplot(3, 3, 7)\n",
    "        total_runs = len(pipeline_metrics['pipeline_id'].unique())\n",
    "        failed_runs = len(pipeline_metrics[pipeline_metrics['metric_name'] == 'execution_failed']['pipeline_id'].unique())\n",
    "        success_runs = total_runs - failed_runs\n",
    "        success_rate = (success_runs / total_runs) * 100 if total_runs > 0 else 100\n",
    "        \n",
    "        # Create gauge-like visualization\n",
    "        theta = np.linspace(0, 2*np.pi, 100)\n",
    "        r = np.ones_like(theta)\n",
    "        \n",
    "        # Background circle\n",
    "        ax7.fill_between(theta, 0, r, alpha=0.3, color='lightgray')\n",
    "        \n",
    "        # Success rate arc\n",
    "        success_theta = theta[:int(success_rate)]\n",
    "        success_r = r[:int(success_rate)]\n",
    "        color = 'green' if success_rate >= 95 else 'orange' if success_rate >= 80 else 'red'\n",
    "        ax7.fill_between(success_theta, 0, success_r, alpha=0.8, color=color)\n",
    "        \n",
    "        ax7.text(0, 0, f'{success_rate:.1f}%', ha='center', va='center', \n",
    "                fontsize=20, fontweight='bold')\n",
    "        ax7.set_title('Pipeline Success Rate', fontsize=14, fontweight='bold')\n",
    "        ax7.set_xlim(-1.2, 1.2)\n",
    "        ax7.set_ylim(-1.2, 1.2)\n",
    "        ax7.axis('off')\n",
    "        \n",
    "        # 8. Resource Utilization (Simulated)\n",
    "        ax8 = plt.subplot(3, 3, 8)\n",
    "        # Simulate resource metrics\n",
    "        resources = ['CPU', 'Memory', 'Disk I/O', 'Network']\n",
    "        utilization = [65, 45, 30, 25]  # Simulated percentages\n",
    "        colors_resource = ['red' if u > 80 else 'orange' if u > 60 else 'green' for u in utilization]\n",
    "        \n",
    "        bars = ax8.barh(resources, utilization, color=colors_resource, alpha=0.7)\n",
    "        ax8.set_title('Resource Utilization', fontsize=14, fontweight='bold')\n",
    "        ax8.set_xlabel('Utilization (%)')\n",
    "        ax8.set_xlim(0, 100)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, (bar, util) in enumerate(zip(bars, utilization)):\n",
    "            ax8.text(util + 2, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{util}%', va='center', fontweight='bold')\n",
    "        \n",
    "        # 9. Recent Pipeline Activity\n",
    "        ax9 = plt.subplot(3, 3, 9)\n",
    "        recent_pipelines = pipeline_metrics['pipeline_id'].unique()[-10:]  # Last 10 pipelines\n",
    "        pipeline_status = []\n",
    "        \n",
    "        for pid in recent_pipelines:\n",
    "            pipeline_data = pipeline_metrics[pipeline_metrics['pipeline_id'] == pid]\n",
    "            has_failure = 'execution_failed' in pipeline_data['metric_name'].values\n",
    "            pipeline_status.append('Failed' if has_failure else 'Success')\n",
    "        \n",
    "        # Create timeline visualization\n",
    "        y_pos = range(len(recent_pipelines))\n",
    "        colors_status = ['red' if status == 'Failed' else 'green' for status in pipeline_status]\n",
    "        \n",
    "        ax9.barh(y_pos, [1]*len(recent_pipelines), color=colors_status, alpha=0.7)\n",
    "        ax9.set_yticks(y_pos)\n",
    "        ax9.set_yticklabels([f'Run {i+1}' for i in range(len(recent_pipelines))])\n",
    "        ax9.set_title('Recent Pipeline Runs', fontsize=14, fontweight='bold')\n",
    "        ax9.set_xlabel('Status')\n",
    "        ax9.set_xlim(0, 1.2)\n",
    "        \n",
    "        # Add status labels\n",
    "        for i, status in enumerate(pipeline_status):\n",
    "            ax9.text(0.5, i, status, ha='center', va='center', \n",
    "                    fontweight='bold', color='white')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('üìä Pipeline Monitoring Dashboard', fontsize=20, fontweight='bold', y=0.98)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        self.print_dashboard_summary(pipeline_metrics, quality_metrics, alerts)\n",
    "    \n",
    "    def print_dashboard_summary(self, pipeline_metrics: pd.DataFrame, \n",
    "                               quality_metrics: pd.DataFrame, alerts: pd.DataFrame):\n",
    "        \"\"\"Print dashboard summary statistics\"\"\"\n",
    "        print(\"\\nüìä DASHBOARD SUMMARY STATISTICS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Pipeline statistics\n",
    "        total_runs = len(pipeline_metrics['pipeline_id'].unique())\n",
    "        avg_execution_time = pipeline_metrics[pipeline_metrics['metric_name'] == 'total_execution_time']['metric_value'].mean()\n",
    "        total_records = pipeline_metrics[pipeline_metrics['metric_name'] == 'records_processed']['metric_value'].sum()\n",
    "        avg_throughput = pipeline_metrics[pipeline_metrics['metric_name'] == 'throughput']['metric_value'].mean()\n",
    "        \n",
    "        print(f\"üîÑ Pipeline Performance:\")\n",
    "        print(f\"  Total Runs: {total_runs}\")\n",
    "        print(f\"  Average Execution Time: {avg_execution_time:.2f}s\")\n",
    "        print(f\"  Total Records Processed: {total_records:,.0f}\")\n",
    "        print(f\"  Average Throughput: {avg_throughput:.1f} records/second\")\n",
    "        \n",
    "        # Quality statistics\n",
    "        if not quality_metrics.empty:\n",
    "            avg_quality = quality_metrics['quality_score'].mean()\n",
    "            min_quality = quality_metrics['quality_score'].min()\n",
    "            max_quality = quality_metrics['quality_score'].max()\n",
    "            \n",
    "            print(f\"\\nüìä Data Quality:\")\n",
    "            print(f\"  Average Quality Score: {avg_quality:.1f}%\")\n",
    "            print(f\"  Quality Range: {min_quality:.1f}% - {max_quality:.1f}%\")\n",
    "        \n",
    "        # Alert statistics\n",
    "        if not alerts.empty:\n",
    "            alert_counts = alerts['severity'].value_counts()\n",
    "            print(f\"\\nüö® Alerts:\")\n",
    "            for severity, count in alert_counts.items():\n",
    "                print(f\"  {severity}: {count}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No alerts in the monitoring period\")\n",
    "        \n",
    "        # Performance insights\n",
    "        print(f\"\\nüí° Performance Insights:\")\n",
    "        if avg_throughput > 50:\n",
    "            print(f\"  ‚úÖ Excellent throughput performance\")\n",
    "        elif avg_throughput > 20:\n",
    "            print(f\"  ‚ö° Good throughput performance\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Consider optimizing pipeline performance\")\n",
    "        \n",
    "        if not quality_metrics.empty and avg_quality >= 95:\n",
    "            print(f\"  ‚úÖ Excellent data quality maintained\")\n",
    "        elif not quality_metrics.empty and avg_quality >= 80:\n",
    "            print(f\"  ‚ö° Good data quality\")\n",
    "        elif not quality_metrics.empty:\n",
    "            print(f\"  ‚ö†Ô∏è Data quality needs attention\")\n",
    "\n",
    "# Create and display monitoring dashboard\n",
    "dashboard = MonitoringDashboard(monitor)\n",
    "dashboard.create_performance_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Step 5: Advanced Alerting System\n",
    "\n",
    "Let's implement a sophisticated alerting system that can detect various types of issues and send appropriate notifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Alerting System\n",
    "class AdvancedAlertingSystem:\n",
    "    \"\"\"Sophisticated alerting system with multiple notification channels\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor: PipelineMonitor):\n",
    "        self.monitor = monitor\n",
    "        self.alert_rules = self.setup_alert_rules()\n",
    "        self.notification_channels = self.setup_notification_channels()\n",
    "        self.alert_history = []\n",
    "        \n",
    "    def setup_alert_rules(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Setup comprehensive alert rules\"\"\"\n",
    "        return {\n",
    "            'pipeline_failure': {\n",
    "                'condition': lambda metrics: metrics.get('success', True) == False,\n",
    "                'severity': 'CRITICAL',\n",
    "                'message_template': 'Pipeline {pipeline_id} failed: {error}',\n",
    "                'cooldown_minutes': 5,\n",
    "                'escalation_minutes': 15\n",
    "            },\n",
    "            'high_execution_time': {\n",
    "                'condition': lambda metrics: metrics.get('execution_time', 0) > 300,\n",
    "                'severity': 'HIGH',\n",
    "                'message_template': 'Pipeline {pipeline_id} execution time {execution_time:.1f}s exceeds threshold',\n",
    "                'cooldown_minutes': 10,\n",
    "                'escalation_minutes': 30\n",
    "            },\n",
    "            'low_throughput': {\n",
    "                'condition': lambda metrics: metrics.get('throughput', float('inf')) < 10,\n",
    "                'severity': 'MEDIUM',\n",
    "                'message_template': 'Pipeline {pipeline_id} throughput {throughput:.1f} records/s is below threshold',\n",
    "                'cooldown_minutes': 15,\n",
    "                'escalation_minutes': 60\n",
    "            },\n",
    "            'data_quality_degradation': {\n",
    "                'condition': lambda metrics: metrics.get('quality_score', 100) < 80,\n",
    "                'severity': 'HIGH',\n",
    "                'message_template': 'Data quality {quality_score:.1f}% below acceptable threshold',\n",
    "                'cooldown_minutes': 5,\n",
    "                'escalation_minutes': 20\n",
    "            },\n",
    "            'no_data_processed': {\n",
    "                'condition': lambda metrics: metrics.get('records_processed', 1) == 0,\n",
    "                'severity': 'HIGH',\n",
    "                'message_template': 'Pipeline {pipeline_id} processed zero records',\n",
    "                'cooldown_minutes': 5,\n",
    "                'escalation_minutes': 15\n",
    "            },\n",
    "            'resource_exhaustion': {\n",
    "                'condition': lambda metrics: (\n",
    "                    metrics.get('cpu_usage', 0) > 90 or \n",
    "                    metrics.get('memory_usage', 0) > 90 or\n",
    "                    metrics.get('disk_usage', 0) > 95\n",
    "                ),\n",
    "                'severity': 'CRITICAL',\n",
    "                'message_template': 'Resource exhaustion detected: CPU {cpu_usage}%, Memory {memory_usage}%, Disk {disk_usage}%',\n",
    "                'cooldown_minutes': 2,\n",
    "                'escalation_minutes': 10\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def setup_notification_channels(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Setup notification channels configuration\"\"\"\n",
    "        return {\n",
    "            'email': {\n",
    "                'enabled': True,\n",
    "                'recipients': ['admin@company.com', 'data-team@company.com'],\n",
    "                'severity_threshold': 'MEDIUM'\n",
    "            },\n",
    "            'slack': {\n",
    "                'enabled': True,\n",
    "                'webhook_url': 'https://hooks.slack.com/services/...',\n",
    "                'channel': '#data-alerts',\n",
    "                'severity_threshold': 'LOW'\n",
    "            },\n",
    "            'sms': {\n",
    "                'enabled': True,\n",
    "                'phone_numbers': ['+1234567890'],\n",
    "                'severity_threshold': 'CRITICAL'\n",
    "            },\n",
    "            'pagerduty': {\n",
    "                'enabled': False,\n",
    "                'integration_key': 'your-pagerduty-key',\n",
    "                'severity_threshold': 'CRITICAL'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def evaluate_alerts(self, pipeline_id: str, metrics: Dict[str, Any]):\n",
    "        \"\"\"Evaluate all alert rules against current metrics\"\"\"\n",
    "        triggered_alerts = []\n",
    "        \n",
    "        for rule_name, rule_config in self.alert_rules.items():\n",
    "            try:\n",
    "                if rule_config['condition'](metrics):\n",
    "                    # Check cooldown period\n",
    "                    if self._is_in_cooldown(rule_name, pipeline_id):\n",
    "                        continue\n",
    "                    \n",
    "                    # Create alert\n",
    "                    alert = self._create_alert(\n",
    "                        pipeline_id, rule_name, rule_config, metrics\n",
    "                    )\n",
    "                    triggered_alerts.append(alert)\n",
    "                    \n",
    "                    # Send notifications\n",
    "                    self._send_notifications(alert)\n",
    "                    \n",
    "                    # Record in history\n",
    "                    self.alert_history.append(alert)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.monitor.logger.error(f\"Error evaluating alert rule {rule_name}: {e}\")\n",
    "        \n",
    "        return triggered_alerts\n",
    "    \n",
    "    def _is_in_cooldown(self, rule_name: str, pipeline_id: str) -> bool:\n",
    "        \"\"\"Check if alert is in cooldown period\"\"\"\n",
    "        cooldown_minutes = self.alert_rules[rule_name]['cooldown_minutes']\n",
    "        cutoff_time = datetime.now() - timedelta(minutes=cooldown_minutes)\n",
    "        \n",
    "        # Check recent alerts for this rule and pipeline\n",
    "        recent_alerts = [\n",
    "            alert for alert in self.alert_history\n",
    "            if (alert['rule_name'] == rule_name and \n",
    "                alert['pipeline_id'] == pipeline_id and\n",
    "                datetime.fromisoformat(alert['timestamp']) > cutoff_time)\n",
    "        ]\n",
    "        \n",
    "        return len(recent_alerts) > 0\n",
    "    \n",
    "    def _create_alert(self, pipeline_id: str, rule_name: str, \n",
    "                     rule_config: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Create alert object\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Format message with metrics\n",
    "        message_data = {'pipeline_id': pipeline_id, **metrics}\n",
    "        message = rule_config['message_template'].format(**message_data)\n",
    "        \n",
    "        alert = {\n",
    "            'id': f\"ALERT-{timestamp}-{rule_name}\",\n",
    "            'timestamp': timestamp,\n",
    "            'pipeline_id': pipeline_id,\n",
    "            'rule_name': rule_name,\n",
    "            'severity': rule_config['severity'],\n",
    "            'message': message,\n",
    "            'metrics': metrics,\n",
    "            'resolved': False\n",
    "        }\n",
    "        \n",
    "        # Save to monitoring database\n",
    "        self.monitor.create_alert(\n",
    "            pipeline_id, rule_name, rule_config['severity'], message\n",
    "        )\n",
    "        \n",
    "        return alert\n",
    "    \n",
    "    def _send_notifications(self, alert: Dict[str, Any]):\n",
    "        \"\"\"Send notifications through configured channels\"\"\"\n",
    "        severity_levels = {'LOW': 1, 'MEDIUM': 2, 'HIGH': 3, 'CRITICAL': 4}\n",
    "        alert_severity_level = severity_levels.get(alert['severity'], 0)\n",
    "        \n",
    "        for channel_name, channel_config in self.notification_channels.items():\n",
    "            if not channel_config['enabled']:\n",
    "                continue\n",
    "            \n",
    "            threshold_level = severity_levels.get(channel_config['severity_threshold'], 0)\n",
    "            \n",
    "            if alert_severity_level >= threshold_level:\n",
    "                self._send_notification(channel_name, channel_config, alert)\n",
    "    \n",
    "    def _send_notification(self, channel_name: str, channel_config: Dict[str, Any], \n",
    "                          alert: Dict[str, Any]):\n",
    "        \"\"\"Send notification to specific channel (simulated)\"\"\"\n",
    "        severity_icons = {\n",
    "            'LOW': 'üü°',\n",
    "            'MEDIUM': 'üü†', \n",
    "            'HIGH': 'üî¥',\n",
    "            'CRITICAL': 'üö®'\n",
    "        }\n",
    "        \n",
    "        icon = severity_icons.get(alert['severity'], '‚ö†Ô∏è')\n",
    "        \n",
    "        if channel_name == 'email':\n",
    "            self._send_email_notification(channel_config, alert, icon)\n",
    "        elif channel_name == 'slack':\n",
    "            self._send_slack_notification(channel_config, alert, icon)\n",
    "        elif channel_name == 'sms':\n",
    "            self._send_sms_notification(channel_config, alert, icon)\n",
    "        elif channel_name == 'pagerduty':\n",
    "            self._send_pagerduty_notification(channel_config, alert, icon)\n",
    "    \n",
    "    def _send_email_notification(self, config: Dict[str, Any], \n",
    "                                alert: Dict[str, Any], icon: str):\n",
    "        \"\"\"Send email notification (simulated)\"\"\"\n",
    "        subject = f\"{icon} [{alert['severity']}] Pipeline Alert: {alert['rule_name']}\"\n",
    "        body = f\"\"\"\n",
    "        Alert Details:\n",
    "        - Timestamp: {alert['timestamp']}\n",
    "        - Pipeline ID: {alert['pipeline_id']}\n",
    "        - Severity: {alert['severity']}\n",
    "        - Message: {alert['message']}\n",
    "        \n",
    "        Metrics:\n",
    "        {json.dumps(alert['metrics'], indent=2)}\n",
    "        \n",
    "        Please investigate and take appropriate action.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.monitor.logger.info(f\"üìß EMAIL sent to {config['recipients']}: {subject}\")\n",
    "        # In production: send actual email using SMTP\n",
    "    \n",
    "    def _send_slack_notification(self, config: Dict[str, Any], \n",
    "                                alert: Dict[str, Any], icon: str):\n",
    "        \"\"\"Send Slack notification (simulated)\"\"\"\n",
    "        message = f\"{icon} *[{alert['severity']}]* Pipeline Alert\\n\" \\\n",
    "                 f\"*Rule:* {alert['rule_name']}\\n\" \\\n",
    "                 f\"*Pipeline:* {alert['pipeline_id']}\\n\" \\\n",
    "                 f\"*Message:* {alert['message']}\"\n",
    "        \n",
    "        self.monitor.logger.info(f\"üí¨ SLACK sent to {config['channel']}: {alert['rule_name']}\")\n",
    "        # In production: send to Slack webhook\n",
    "    \n",
    "    def _send_sms_notification(self, config: Dict[str, Any], \n",
    "                              alert: Dict[str, Any], icon: str):\n",
    "        \"\"\"Send SMS notification (simulated)\"\"\"\n",
    "        message = f\"{icon} [{alert['severity']}] {alert['rule_name']}: {alert['message'][:100]}...\"\n",
    "        \n",
    "        self.monitor.logger.info(f\"üì± SMS sent to {config['phone_numbers']}: {alert['rule_name']}\")\n",
    "        # In production: send SMS using Twilio or similar service\n",
    "    \n",
    "    def _send_pagerduty_notification(self, config: Dict[str, Any], \n",
    "                                    alert: Dict[str, Any], icon: str):\n",
    "        \"\"\"Send PagerDuty notification (simulated)\"\"\"\n",
    "        self.monitor.logger.info(f\"üìü PAGERDUTY incident created: {alert['rule_name']}\")\n",
    "        # In production: create PagerDuty incident\n",
    "    \n",
    "    def get_alert_summary(self, hours_back: int = 24) -> Dict[str, Any]:\n",
    "        \"\"\"Get alert summary for the specified time period\"\"\"\n",
    "        cutoff_time = datetime.now() - timedelta(hours=hours_back)\n",
    "        \n",
    "        recent_alerts = [\n",
    "            alert for alert in self.alert_history\n",
    "            if datetime.fromisoformat(alert['timestamp']) > cutoff_time\n",
    "        ]\n",
    "        \n",
    "        # Group by severity\n",
    "        severity_counts = {}\n",
    "        rule_counts = {}\n",
    "        \n",
    "        for alert in recent_alerts:\n",
    "            severity = alert['severity']\n",
    "            rule_name = alert['rule_name']\n",
    "            \n",
    "            severity_counts[severity] = severity_counts.get(severity, 0) + 1\n",
    "            rule_counts[rule_name] = rule_counts.get(rule_name, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            'total_alerts': len(recent_alerts),\n",
    "            'severity_breakdown': severity_counts,\n",
    "            'rule_breakdown': rule_counts,\n",
    "            'recent_alerts': recent_alerts[-10:],  # Last 10 alerts\n",
    "            'time_period_hours': hours_back\n",
    "        }\n",
    "\n",
    "# Initialize advanced alerting system\n",
    "alerting_system = AdvancedAlertingSystem(monitor)\n",
    "\n",
    "print(\"üö® Advanced Alerting System initialized!\")\n",
    "print(f\"üìã Alert Rules: {len(alerting_system.alert_rules)}\")\n",
    "print(f\"üì¢ Notification Channels: {len(alerting_system.notification_channels)}\")\n",
    "\n",
    "# Test alerting system with various scenarios\n",
    "print(\"\\nüß™ Testing Alert Scenarios:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Scenario 1: Normal metrics (no alerts)\n",
    "normal_metrics = {\n",
    "    'success': True,\n",
    "    'execution_time': 45.2,\n",
    "    'throughput': 25.5,\n",
    "    'quality_score': 95.2,\n",
    "    'records_processed': 150\n",
    "}\n",
    "\n",
    "alerts1 = alerting_system.evaluate_alerts('TEST-001', normal_metrics)\n",
    "print(f\"Normal metrics: {len(alerts1)} alerts triggered\")\n",
    "\n",
    "# Scenario 2: Slow execution\n",
    "slow_metrics = {\n",
    "    'success': True,\n",
    "    'execution_time': 350.0,  # Exceeds threshold\n",
    "    'throughput': 15.2,\n",
    "    'quality_score': 88.5,\n",
    "    'records_processed': 200\n",
    "}\n",
    "\n",
    "alerts2 = alerting_system.evaluate_alerts('TEST-002', slow_metrics)\n",
    "print(f\"Slow execution: {len(alerts2)} alerts triggered\")\n",
    "\n",
    "# Scenario 3: Poor data quality\n",
    "quality_metrics = {\n",
    "    'success': True,\n",
    "    'execution_time': 65.3,\n",
    "    'throughput': 22.1,\n",
    "    'quality_score': 75.0,  # Below threshold\n",
    "    'records_processed': 180\n",
    "}\n",
    "\n",
    "alerts3 = alerting_system.evaluate_alerts('TEST-003', quality_metrics)\n",
    "print(f\"Poor quality: {len(alerts3)} alerts triggered\")\n",
    "\n",
    "# Scenario 4: Pipeline failure\n",
    "failure_metrics = {\n",
    "    'success': False,  # Pipeline failed\n",
    "    'execution_time': 120.0,\n",
    "    'error': 'Database connection timeout',\n",
    "    'records_processed': 0\n",
    "}\n",
    "\n",
    "alerts4 = alerting_system.evaluate_alerts('TEST-004', failure_metrics)\n",
    "print(f\"Pipeline failure: {len(alerts4)} alerts triggered\")\n",
    "\n",
    "# Get alert summary\n",
    "alert_summary = alerting_system.get_alert_summary()\n",
    "print(f\"\\nüìä Alert Summary (24h):\")\n",
    "print(f\"  Total Alerts: {alert_summary['total_alerts']}\")\n",
    "print(f\"  By Severity: {alert_summary['severity_breakdown']}\")\n",
    "print(f\"  By Rule: {alert_summary['rule_breakdown']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 6: Performance Analysis and Optimization\n",
    "\n",
    "Let's analyze pipeline performance and identify optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Analysis and Optimization\n",
    "class PerformanceAnalyzer:\n",
    "    \"\"\"Analyze pipeline performance and suggest optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor: PipelineMonitor):\n",
    "        self.monitor = monitor\n",
    "    \n",
    "    def analyze_performance_trends(self, days_back: int = 7) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze performance trends over time\"\"\"\n",
    "        cutoff_time = (datetime.now() - timedelta(days=days_back)).isoformat()\n",
    "        \n",
    "        with sqlite3.connect(self.monitor.db_path) as conn:\n",
    "            # Get pipeline metrics\n",
    "            pipeline_query = '''\n",
    "                SELECT * FROM pipeline_metrics \n",
    "                WHERE timestamp >= ? \n",
    "                ORDER BY timestamp\n",
    "            '''\n",
    "            pipeline_metrics = pd.read_sql_query(pipeline_query, conn, params=[cutoff_time])\n",
    "            \n",
    "            # Get quality metrics\n",
    "            quality_query = '''\n",
    "                SELECT * FROM quality_metrics \n",
    "                WHERE timestamp >= ? \n",
    "                ORDER BY timestamp\n",
    "            '''\n",
    "            quality_metrics = pd.read_sql_query(quality_query, conn, params=[cutoff_time])\n",
    "        \n",
    "        if pipeline_metrics.empty:\n",
    "            return {'error': 'No performance data available'}\n",
    "        \n",
    "        # Convert timestamps\n",
    "        pipeline_metrics['timestamp'] = pd.to_datetime(pipeline_metrics['timestamp'])\n",
    "        if not quality_metrics.empty:\n",
    "            quality_metrics['timestamp'] = pd.to_datetime(quality_metrics['timestamp'])\n",
    "        \n",
    "        # Analyze execution times\n",
    "        execution_times = pipeline_metrics[pipeline_metrics['metric_name'] == 'total_execution_time']\n",
    "        throughput_data = pipeline_metrics[pipeline_metrics['metric_name'] == 'throughput']\n",
    "        records_data = pipeline_metrics[pipeline_metrics['metric_name'] == 'records_processed']\n",
    "        \n",
    "        analysis = {\n",
    "            'time_period_days': days_back,\n",
    "            'total_pipeline_runs': len(pipeline_metrics['pipeline_id'].unique()),\n",
    "            'metrics_collected': len(pipeline_metrics)\n",
    "        }\n",
    "        \n",
    "        # Execution time analysis\n",
    "        if not execution_times.empty:\n",
    "            analysis['execution_time'] = {\n",
    "                'mean': execution_times['metric_value'].mean(),\n",
    "                'median': execution_times['metric_value'].median(),\n",
    "                'std': execution_times['metric_value'].std(),\n",
    "                'min': execution_times['metric_value'].min(),\n",
    "                'max': execution_times['metric_value'].max(),\n",
    "                'trend': self._calculate_trend(execution_times)\n",
    "            }\n",
    "        \n",
    "        # Throughput analysis\n",
    "        if not throughput_data.empty:\n",
    "            analysis['throughput'] = {\n",
    "                'mean': throughput_data['metric_value'].mean(),\n",
    "                'median': throughput_data['metric_value'].median(),\n",
    "                'std': throughput_data['metric_value'].std(),\n",
    "                'min': throughput_data['metric_value'].min(),\n",
    "                'max': throughput_data['metric_value'].max(),\n",
    "                'trend': self._calculate_trend(throughput_data)\n",
    "            }\n",
    "        \n",
    "        # Records processed analysis\n",
    "        if not records_data.empty:\n",
    "            analysis['records_processed'] = {\n",
    "                'total': records_data['metric_value'].sum(),\n",
    "                'mean_per_run': records_data['metric_value'].mean(),\n",
    "                'median_per_run': records_data['metric_value'].median(),\n",
    "                'trend': self._calculate_trend(records_data)\n",
    "            }\n",
    "        \n",
    "        # Quality analysis\n",
    "        if not quality_metrics.empty:\n",
    "            analysis['data_quality'] = {\n",
    "                'mean_quality_score': quality_metrics['quality_score'].mean(),\n",
    "                'median_quality_score': quality_metrics['quality_score'].median(),\n",
    "                'min_quality_score': quality_metrics['quality_score'].min(),\n",
    "                'max_quality_score': quality_metrics['quality_score'].max(),\n",
    "                'quality_trend': self._calculate_trend(quality_metrics, 'quality_score')\n",
    "            }\n",
    "        \n",
    "        # Stage performance analysis\n",
    "        stage_analysis = self._analyze_stage_performance(pipeline_metrics)\n",
    "        analysis['stage_performance'] = stage_analysis\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _calculate_trend(self, data: pd.DataFrame, value_col: str = 'metric_value') -> str:\n",
    "        \"\"\"Calculate trend direction (improving, degrading, stable)\"\"\"\n",
    "        if len(data) < 2:\n",
    "            return 'insufficient_data'\n",
    "        \n",
    "        # Simple linear trend calculation\n",
    "        x = np.arange(len(data))\n",
    "        y = data[value_col].values\n",
    "        \n",
    "        # Calculate correlation coefficient\n",
    "        correlation = np.corrcoef(x, y)[0, 1]\n",
    "        \n",
    "        if abs(correlation) < 0.1:\n",
    "            return 'stable'\n",
    "        elif correlation > 0:\n",
    "            return 'increasing'\n",
    "        else:\n",
    "            return 'decreasing'\n",
    "    \n",
    "    def _analyze_stage_performance(self, pipeline_metrics: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze performance by pipeline stage\"\"\"\n",
    "        stage_metrics = pipeline_metrics[pipeline_metrics['stage'].notna()]\n",
    "        \n",
    "        if stage_metrics.empty:\n",
    "            return {'error': 'No stage-specific metrics available'}\n",
    "        \n",
    "        stage_analysis = {}\n",
    "        \n",
    "        for stage in stage_metrics['stage'].unique():\n",
    "            stage_data = stage_metrics[stage_metrics['stage'] == stage]\n",
    "            \n",
    "            stage_analysis[stage] = {\n",
    "                'mean_time': stage_data['metric_value'].mean(),\n",
    "                'median_time': stage_data['metric_value'].median(),\n",
    "                'std_time': stage_data['metric_value'].std(),\n",
    "                'min_time': stage_data['metric_value'].min(),\n",
    "                'max_time': stage_data['metric_value'].max(),\n",
    "                'execution_count': len(stage_data),\n",
    "                'trend': self._calculate_trend(stage_data)\n",
    "            }\n",
    "        \n",
    "        return stage_analysis\n",
    "    \n",
    "    def generate_optimization_recommendations(self, analysis: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate optimization recommendations based on performance analysis\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Execution time recommendations\n",
    "        if 'execution_time' in analysis:\n",
    "            exec_time = analysis['execution_time']\n",
    "            \n",
    "            if exec_time['mean'] > 300:  # 5 minutes\n",
    "                recommendations.append({\n",
    "                    'category': 'Performance',\n",
    "                    'priority': 'High',\n",
    "                    'issue': 'Long execution times',\n",
    "                    'recommendation': f\"Average execution time is {exec_time['mean']:.1f}s. Consider parallel processing, data partitioning, or infrastructure scaling.\",\n",
    "                    'impact': 'Reduce pipeline execution time by 30-50%'\n",
    "                })\n",
    "            \n",
    "            if exec_time['std'] > exec_time['mean'] * 0.5:  # High variability\n",
    "                recommendations.append({\n",
    "                    'category': 'Consistency',\n",
    "                    'priority': 'Medium',\n",
    "                    'issue': 'Inconsistent execution times',\n",
    "                    'recommendation': f\"High variability in execution times (std: {exec_time['std']:.1f}s). Investigate resource contention or data size variations.\",\n",
    "                    'impact': 'Improve pipeline predictability'\n",
    "                })\n",
    "            \n",
    "            if exec_time['trend'] == 'increasing':\n",
    "                recommendations.append({\n",
    "                    'category': 'Performance',\n",
    "                    'priority': 'High',\n",
    "                    'issue': 'Degrading performance trend',\n",
    "                    'recommendation': 'Execution times are increasing over time. Review recent changes and consider performance optimization.',\n",
    "                    'impact': 'Prevent further performance degradation'\n",
    "                })\n",
    "        \n",
    "        # Throughput recommendations\n",
    "        if 'throughput' in analysis:\n",
    "            throughput = analysis['throughput']\n",
    "            \n",
    "            if throughput['mean'] < 20:  # Low throughput\n",
    "                recommendations.append({\n",
    "                    'category': 'Performance',\n",
    "                    'priority': 'Medium',\n",
    "                    'issue': 'Low throughput',\n",
    "                    'recommendation': f\"Average throughput is {throughput['mean']:.1f} records/s. Consider batch size optimization or parallel processing.\",\n",
    "                    'impact': 'Increase data processing rate'\n",
    "                })\n",
    "            \n",
    "            if throughput['trend'] == 'decreasing':\n",
    "                recommendations.append({\n",
    "                    'category': 'Performance',\n",
    "                    'priority': 'High',\n",
    "                    'issue': 'Declining throughput',\n",
    "                    'recommendation': 'Throughput is decreasing over time. Investigate data growth, resource constraints, or code changes.',\n",
    "                    'impact': 'Maintain processing efficiency'\n",
    "                })\n",
    "        \n",
    "        # Data quality recommendations\n",
    "        if 'data_quality' in analysis:\n",
    "            quality = analysis['data_quality']\n",
    "            \n",
    "            if quality['mean_quality_score'] < 90:\n",
    "                recommendations.append({\n",
    "                    'category': 'Data Quality',\n",
    "                    'priority': 'High',\n",
    "                    'issue': 'Low data quality',\n",
    "                    'recommendation': f\"Average data quality is {quality['mean_quality_score']:.1f}%. Review data sources and validation rules.\",\n",
    "                    'impact': 'Improve downstream data reliability'\n",
    "                })\n",
    "            \n",
    "            if quality['quality_trend'] == 'decreasing':\n",
    "                recommendations.append({\n",
    "                    'category': 'Data Quality',\n",
    "                    'priority': 'Critical',\n",
    "                    'issue': 'Degrading data quality',\n",
    "                    'recommendation': 'Data quality is declining over time. Immediate investigation required for data sources.',\n",
    "                    'impact': 'Prevent data quality deterioration'\n",
    "                })\n",
    "        \n",
    "        # Stage-specific recommendations\n",
    "        if 'stage_performance' in analysis and 'error' not in analysis['stage_performance']:\n",
    "            stages = analysis['stage_performance']\n",
    "            \n",
    "            # Find slowest stage\n",
    "            slowest_stage = max(stages.keys(), key=lambda x: stages[x]['mean_time'])\n",
    "            slowest_time = stages[slowest_stage]['mean_time']\n",
    "            \n",
    "            if slowest_time > 60:  # More than 1 minute\n",
    "                recommendations.append({\n",
    "                    'category': 'Performance',\n",
    "                    'priority': 'Medium',\n",
    "                    'issue': f'Slow {slowest_stage} stage',\n",
    "                    'recommendation': f\"The {slowest_stage} stage takes {slowest_time:.1f}s on average. Consider optimizing this stage specifically.\",\n",
    "                    'impact': f'Reduce {slowest_stage} stage execution time'\n",
    "                })\n",
    "            \n",
    "            # Check for stages with increasing trends\n",
    "            for stage, metrics in stages.items():\n",
    "                if metrics['trend'] == 'increasing':\n",
    "                    recommendations.append({\n",
    "                        'category': 'Performance',\n",
    "                        'priority': 'Medium',\n",
    "                        'issue': f'{stage} stage performance degrading',\n",
    "                        'recommendation': f\"The {stage} stage is taking longer over time. Review recent changes to this stage.\",\n",
    "                        'impact': f'Stabilize {stage} stage performance'\n",
    "                    })\n",
    "        \n",
    "        # General recommendations if no specific issues found\n",
    "        if not recommendations:\n",
    "            recommendations.append({\n",
    "                'category': 'Optimization',\n",
    "                'priority': 'Low',\n",
    "                'issue': 'Proactive optimization',\n",
    "                'recommendation': 'Pipeline performance is good. Consider proactive optimizations like caching, indexing, or resource scaling for future growth.',\n",
    "                'impact': 'Prepare for increased data volumes'\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_performance_report(self, analysis: Dict[str, Any], \n",
    "                                 recommendations: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Create comprehensive performance analysis report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"# üìä Pipeline Performance Analysis Report\")\n",
    "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(f\"Analysis Period: {analysis.get('time_period_days', 'N/A')} days\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        report.append(\"## üéØ Executive Summary\")\n",
    "        report.append(f\"- **Total Pipeline Runs**: {analysis.get('total_pipeline_runs', 0):,}\")\n",
    "        report.append(f\"- **Metrics Collected**: {analysis.get('metrics_collected', 0):,}\")\n",
    "        \n",
    "        if 'execution_time' in analysis:\n",
    "            exec_time = analysis['execution_time']\n",
    "            report.append(f\"- **Average Execution Time**: {exec_time['mean']:.2f}s\")\n",
    "            report.append(f\"- **Execution Time Range**: {exec_time['min']:.2f}s - {exec_time['max']:.2f}s\")\n",
    "        \n",
    "        if 'throughput' in analysis:\n",
    "            throughput = analysis['throughput']\n",
    "            report.append(f\"- **Average Throughput**: {throughput['mean']:.1f} records/second\")\n",
    "        \n",
    "        if 'data_quality' in analysis:\n",
    "            quality = analysis['data_quality']\n",
    "            report.append(f\"- **Average Data Quality**: {quality['mean_quality_score']:.1f}%\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Performance Trends\n",
    "        report.append(\"## üìà Performance Trends\")\n",
    "        \n",
    "        if 'execution_time' in analysis:\n",
    "            trend = analysis['execution_time']['trend']\n",
    "            trend_icon = {'increasing': 'üìà', 'decreasing': 'üìâ', 'stable': '‚û°Ô∏è'}.get(trend, '‚ùì')\n",
    "            report.append(f\"- **Execution Time Trend**: {trend_icon} {trend.title()}\")\n",
    "        \n",
    "        if 'throughput' in analysis:\n",
    "            trend = analysis['throughput']['trend']\n",
    "            trend_icon = {'increasing': 'üìà', 'decreasing': 'üìâ', 'stable': '‚û°Ô∏è'}.get(trend, '‚ùì')\n",
    "            report.append(f\"- **Throughput Trend**: {trend_icon} {trend.title()}\")\n",
    "        \n",
    "        if 'data_quality' in analysis:\n",
    "            trend = analysis['data_quality']['quality_trend']\n",
    "            trend_icon = {'increasing': 'üìà', 'decreasing': 'üìâ', 'stable': '‚û°Ô∏è'}.get(trend, '‚ùì')\n",
    "            report.append(f\"- **Data Quality Trend**: {trend_icon} {trend.title()}\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Stage Performance\n",
    "        if 'stage_performance' in analysis and 'error' not in analysis['stage_performance']:\n",
    "            report.append(\"## ‚ö° Stage Performance Analysis\")\n",
    "            stages = analysis['stage_performance']\n",
    "            \n",
    "            for stage, metrics in stages.items():\n",
    "                report.append(f\"### {stage.title()} Stage\")\n",
    "                report.append(f\"- **Average Time**: {metrics['mean_time']:.3f}s\")\n",
    "                report.append(f\"- **Time Range**: {metrics['min_time']:.3f}s - {metrics['max_time']:.3f}s\")\n",
    "                report.append(f\"- **Executions**: {metrics['execution_count']}\")\n",
    "                trend_icon = {'increasing': 'üìà', 'decreasing': 'üìâ', 'stable': '‚û°Ô∏è'}.get(metrics['trend'], '‚ùì')\n",
    "                report.append(f\"- **Trend**: {trend_icon} {metrics['trend'].title()}\")\n",
    "                report.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(\"## üí° Optimization Recommendations\")\n",
    "        \n",
    "        if recommendations:\n",
    "            # Group by priority\n",
    "            priority_order = ['Critical', 'High', 'Medium', 'Low']\n",
    "            for priority in priority_order:\n",
    "                priority_recs = [r for r in recommendations if r['priority'] == priority]\n",
    "                if priority_recs:\n",
    "                    priority_icon = {'Critical': 'üö®', 'High': 'üî¥', 'Medium': 'üü†', 'Low': 'üü°'}.get(priority, '‚ö™')\n",
    "                    report.append(f\"### {priority_icon} {priority} Priority\")\n",
    "                    \n",
    "                    for i, rec in enumerate(priority_recs, 1):\n",
    "                        report.append(f\"**{i}. {rec['issue']}**\")\n",
    "                        report.append(f\"- **Category**: {rec['category']}\")\n",
    "                        report.append(f\"- **Recommendation**: {rec['recommendation']}\")\n",
    "                        report.append(f\"- **Expected Impact**: {rec['impact']}\")\n",
    "                        report.append(\"\")\n",
    "        else:\n",
    "            report.append(\"‚úÖ No critical performance issues identified.\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Next Steps\n",
    "        report.append(\"## üöÄ Next Steps\")\n",
    "        report.append(\"1. **Review High Priority Recommendations**: Address critical and high priority items first\")\n",
    "        report.append(\"2. **Implement Monitoring**: Set up automated alerts for performance degradation\")\n",
    "        report.append(\"3. **Regular Analysis**: Schedule weekly performance reviews\")\n",
    "        report.append(\"4. **Capacity Planning**: Plan for future data growth and scaling needs\")\n",
    "        report.append(\"5. **Performance Testing**: Implement load testing for pipeline validation\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        report.append(\"---\")\n",
    "        report.append(\"*Report generated by Pipeline Performance Analyzer*\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Initialize performance analyzer\n",
    "performance_analyzer = PerformanceAnalyzer(monitor)\n",
    "\n",
    "print(\"üîß Performance Analyzer initialized!\")\n",
    "print(\"üìä Analyzing pipeline performance...\")\n",
    "\n",
    "# Perform performance analysis\n",
    "analysis = performance_analyzer.analyze_performance_trends(days_back=1)  # Last 24 hours\n",
    "\n",
    "if 'error' not in analysis:\n",
    "    # Generate recommendations\n",
    "    recommendations = performance_analyzer.generate_optimization_recommendations(analysis)\n",
    "    \n",
    "    # Create performance report\n",
    "    performance_report = performance_analyzer.create_performance_report(analysis, recommendations)\n",
    "    \n",
    "    print(\"\\nüìã PERFORMANCE ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(performance_report)\n",
    "    \n",
    "    # Save report\n",
    "    report_filename = f\"../data/output/performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    try:\n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write(performance_report)\n",
    "        print(f\"\\nüíæ Performance report saved to: {report_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Could not save performance report: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Summary: {len(recommendations)} optimization recommendations generated\")\n",
    "    \n",
    "    # Show top recommendations\n",
    "    high_priority = [r for r in recommendations if r['priority'] in ['Critical', 'High']]\n",
    "    if high_priority:\n",
    "        print(f\"\\nüö® High Priority Actions:\")\n",
    "        for i, rec in enumerate(high_priority[:3], 1):\n",
    "            print(f\"  {i}. {rec['issue']}: {rec['recommendation'][:100]}...\")\nelse:\n",
    "    print(f\"\\n‚ö†Ô∏è Performance analysis error: {analysis['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 7: Operational Procedures and Best Practices\n",
    "\n",
    "Let's establish comprehensive operational procedures for maintaining production data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational Procedures and Best Practices\n",
    "class OperationalProcedures:\n",
    "    \"\"\"Comprehensive operational procedures for production pipelines\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor: PipelineMonitor):\n",
    "        self.monitor = monitor\n",
    "        self.procedures = self.define_procedures()\n",
    "        self.checklists = self.create_checklists()\n",
    "        self.runbooks = self.create_runbooks()\n",
    "    \n",
    "    def define_procedures(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Define standard operational procedures\"\"\"\n",
    "        return {\n",
    "            'daily_health_check': {\n",
    "                'frequency': 'daily',\n",
    "                'description': 'Daily pipeline health assessment',\n",
    "                'steps': [\n",
    "                    'Check pipeline execution status for last 24 hours',\n",
    "                    'Review data quality metrics',\n",
    "                    'Verify alert status and resolve any issues',\n",
    "                    'Check resource utilization trends',\n",
    "                    'Validate data freshness and completeness'\n",
    "                ],\n",
    "                'success_criteria': [\n",
    "                    'All pipelines executed successfully',\n",
    "                    'Data quality > 95%',\n",
    "                    'No unresolved critical alerts',\n",
    "                    'Resource utilization < 80%'\n",
    "                ]\n",
    "            },\n",
    "            'weekly_performance_review': {\n",
    "                'frequency': 'weekly',\n",
    "                'description': 'Weekly performance analysis and optimization',\n",
    "                'steps': [\n",
    "                    'Generate performance analysis report',\n",
    "                    'Review execution time trends',\n",
    "                    'Analyze throughput patterns',\n",
    "                    'Identify optimization opportunities',\n",
    "                    'Plan performance improvements'\n",
    "                ],\n",
    "                'success_criteria': [\n",
    "                    'Performance report generated',\n",
    "                    'Trends analyzed and documented',\n",
    "                    'Action items identified for next week'\n",
    "                ]\n",
    "            },\n",
    "            'monthly_capacity_planning': {\n",
    "                'frequency': 'monthly',\n",
    "                'description': 'Monthly capacity and scaling assessment',\n",
    "                'steps': [\n",
    "                    'Analyze data volume growth trends',\n",
    "                    'Review resource utilization patterns',\n",
    "                    'Forecast future capacity needs',\n",
    "                    'Plan infrastructure scaling',\n",
    "                    'Update capacity planning documentation'\n",
    "                ],\n",
    "                'success_criteria': [\n",
    "                    'Capacity forecast completed',\n",
    "                    'Scaling plan documented',\n",
    "                    'Budget requirements identified'\n",
    "                ]\n",
    "            },\n",
    "            'incident_response': {\n",
    "                'frequency': 'as_needed',\n",
    "                'description': 'Response to pipeline incidents and failures',\n",
    "                'steps': [\n",
    "                    'Assess incident severity and impact',\n",
    "                    'Implement immediate containment measures',\n",
    "                    'Investigate root cause',\n",
    "                    'Implement fix and validate resolution',\n",
    "                    'Document incident and lessons learned'\n",
    "                ],\n",
    "                'success_criteria': [\n",
    "                    'Incident resolved within SLA',\n",
    "                    'Root cause identified',\n",
    "                    'Prevention measures implemented'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_checklists(self) -> Dict[str, List[Dict[str, str]]]:\n",
    "        \"\"\"Create operational checklists\"\"\"\n",
    "        return {\n",
    "            'pipeline_deployment': [\n",
    "                {'task': 'Code review completed and approved', 'type': 'verification'},\n",
    "                {'task': 'Unit tests passing (>95% coverage)', 'type': 'testing'},\n",
    "                {'task': 'Integration tests passing', 'type': 'testing'},\n",
    "                {'task': 'Performance tests completed', 'type': 'testing'},\n",
    "                {'task': 'Security scan completed', 'type': 'security'},\n",
    "                {'task': 'Configuration validated', 'type': 'configuration'},\n",
    "                {'task': 'Database migrations applied', 'type': 'database'},\n",
    "                {'task': 'Monitoring and alerts configured', 'type': 'monitoring'},\n",
    "                {'task': 'Rollback plan prepared', 'type': 'contingency'},\n",
    "                {'task': 'Stakeholders notified', 'type': 'communication'}\n",
    "            ],\n",
    "            'incident_response': [\n",
    "                {'task': 'Incident severity assessed', 'type': 'assessment'},\n",
    "                {'task': 'Incident commander assigned', 'type': 'organization'},\n",
    "                {'task': 'Stakeholders notified', 'type': 'communication'},\n",
    "                {'task': 'Initial containment implemented', 'type': 'containment'},\n",
    "                {'task': 'Root cause investigation started', 'type': 'investigation'},\n",
    "                {'task': 'Fix implemented and tested', 'type': 'resolution'},\n",
    "                {'task': 'Service restored and validated', 'type': 'validation'},\n",
    "                {'task': 'Post-incident review scheduled', 'type': 'follow_up'},\n",
    "                {'task': 'Documentation updated', 'type': 'documentation'}\n",
    "            ],\n",
    "            'data_quality_investigation': [\n",
    "                {'task': 'Quality issue scope identified', 'type': 'assessment'},\n",
    "                {'task': 'Affected data sources identified', 'type': 'investigation'},\n",
    "                {'task': 'Data lineage traced', 'type': 'investigation'},\n",
    "                {'task': 'Validation rules reviewed', 'type': 'analysis'},\n",
    "                {'task': 'Source data quality verified', 'type': 'verification'},\n",
    "                {'task': 'Transformation logic validated', 'type': 'verification'},\n",
    "                {'task': 'Corrective actions implemented', 'type': 'resolution'},\n",
    "                {'task': 'Data quality monitoring enhanced', 'type': 'prevention'}\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def create_runbooks(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Create detailed runbooks for common scenarios\"\"\"\n",
    "        return {\n",
    "            'pipeline_failure': {\n",
    "                'title': 'Pipeline Execution Failure Response',\n",
    "                'severity': 'HIGH',\n",
    "                'description': 'Steps to diagnose and resolve pipeline execution failures',\n",
    "                'steps': [\n",
    "                    {\n",
    "                        'step': 1,\n",
    "                        'action': 'Check pipeline logs',\n",
    "                        'details': 'Review execution logs for error messages and stack traces',\n",
    "                        'commands': ['tail -f /logs/pipeline.log', 'grep ERROR /logs/pipeline.log']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 2,\n",
    "                        'action': 'Verify data source availability',\n",
    "                        'details': 'Check if all data sources are accessible and contain expected data',\n",
    "                        'commands': ['curl -I <api_endpoint>', 'ls -la /data/input/']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 3,\n",
    "                        'action': 'Check system resources',\n",
    "                        'details': 'Verify CPU, memory, and disk space availability',\n",
    "                        'commands': ['top', 'df -h', 'free -m']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 4,\n",
    "                        'action': 'Restart pipeline with debug mode',\n",
    "                        'details': 'Attempt restart with verbose logging enabled',\n",
    "                        'commands': ['python scripts/run_pipeline.py --log-level DEBUG']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 5,\n",
    "                        'action': 'Escalate if unresolved',\n",
    "                        'details': 'Contact senior engineer if issue persists after 30 minutes',\n",
    "                        'commands': []\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            'data_quality_degradation': {\n",
    "                'title': 'Data Quality Issue Response',\n",
    "                'severity': 'MEDIUM',\n",
    "                'description': 'Steps to investigate and resolve data quality issues',\n",
    "                'steps': [\n",
    "                    {\n",
    "                        'step': 1,\n",
    "                        'action': 'Identify affected data',\n",
    "                        'details': 'Determine which datasets and time periods are affected',\n",
    "                        'commands': ['python scripts/data_quality_check.py --date-range']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 2,\n",
    "                        'action': 'Check source data quality',\n",
    "                        'details': 'Verify if quality issues originate from source systems',\n",
    "                        'commands': ['python scripts/source_validation.py']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 3,\n",
    "                        'action': 'Review validation rules',\n",
    "                        'details': 'Check if validation rules need updates or fixes',\n",
    "                        'commands': ['python scripts/validate_rules.py']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 4,\n",
    "                        'action': 'Implement data fixes',\n",
    "                        'details': 'Apply data corrections and re-run affected pipelines',\n",
    "                        'commands': ['python scripts/data_correction.py', 'python scripts/reprocess_data.py']\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            'performance_degradation': {\n",
    "                'title': 'Performance Degradation Response',\n",
    "                'severity': 'MEDIUM',\n",
    "                'description': 'Steps to diagnose and resolve performance issues',\n",
    "                'steps': [\n",
    "                    {\n",
    "                        'step': 1,\n",
    "                        'action': 'Analyze performance metrics',\n",
    "                        'details': 'Review execution times, throughput, and resource usage',\n",
    "                        'commands': ['python scripts/performance_analysis.py']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 2,\n",
    "                        'action': 'Identify bottlenecks',\n",
    "                        'details': 'Determine which pipeline stages are causing delays',\n",
    "                        'commands': ['python scripts/bottleneck_analysis.py']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 3,\n",
    "                        'action': 'Check resource constraints',\n",
    "                        'details': 'Verify if system resources are limiting performance',\n",
    "                        'commands': ['htop', 'iotop', 'nethogs']\n",
    "                    },\n",
    "                    {\n",
    "                        'step': 4,\n",
    "                        'action': 'Apply performance optimizations',\n",
    "                        'details': 'Implement immediate performance improvements',\n",
    "                        'commands': ['python scripts/optimize_pipeline.py']\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def execute_health_check(self) -> Dict[str, Any]:\n",
    "        \"\"\"Execute comprehensive pipeline health check\"\"\"\n",
    "        health_check = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'overall_status': 'HEALTHY',\n",
    "            'checks': {},\n",
    "            'issues': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check 1: Recent pipeline executions\n",
    "            recent_pipelines = self._check_recent_executions()\n",
    "            health_check['checks']['recent_executions'] = recent_pipelines\n",
    "            \n",
    "            # Check 2: Data quality status\n",
    "            quality_status = self._check_data_quality()\n",
    "            health_check['checks']['data_quality'] = quality_status\n",
    "            \n",
    "            # Check 3: Alert status\n",
    "            alert_status = self._check_alert_status()\n",
    "            health_check['checks']['alerts'] = alert_status\n",
    "            \n",
    "            # Check 4: Resource utilization\n",
    "            resource_status = self._check_resource_utilization()\n",
    "            health_check['checks']['resources'] = resource_status\n",
    "            \n",
    "            # Check 5: Data freshness\n",
    "            freshness_status = self._check_data_freshness()\n",
    "            health_check['checks']['data_freshness'] = freshness_status\n",
    "            \n",
    "            # Determine overall status\n",
    "            failed_checks = [name for name, check in health_check['checks'].items() \n",
    "                           if check['status'] == 'FAILED']\n",
    "            warning_checks = [name for name, check in health_check['checks'].items() \n",
    "                            if check['status'] == 'WARNING']\n",
    "            \n",
    "            if failed_checks:\n",
    "                health_check['overall_status'] = 'UNHEALTHY'\n",
    "                health_check['issues'].extend([f\"Failed check: {check}\" for check in failed_checks])\n",
    "            elif warning_checks:\n",
    "                health_check['overall_status'] = 'WARNING'\n",
    "                health_check['issues'].extend([f\"Warning in check: {check}\" for check in warning_checks])\n",
    "            \n",
    "            # Generate recommendations\n",
    "            health_check['recommendations'] = self._generate_health_recommendations(health_check)\n",
    "            \n",
    "        except Exception as e:\n",
    "            health_check['overall_status'] = 'ERROR'\n",
    "            health_check['issues'].append(f\"Health check error: {str(e)}\")\n",
    "        \n",
    "        return health_check\n",
    "    \n",
    "    def _check_recent_executions(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check recent pipeline executions\"\"\"\n",
    "        try:\n",
    "            cutoff_time = (datetime.now() - timedelta(hours=24)).isoformat()\n",
    "            \n",
    "            with sqlite3.connect(self.monitor.db_path) as conn:\n",
    "                query = '''\n",
    "                    SELECT pipeline_id, COUNT(*) as execution_count\n",
    "                    FROM pipeline_metrics \n",
    "                    WHERE timestamp >= ? AND metric_name = 'total_execution_time'\n",
    "                    GROUP BY pipeline_id\n",
    "                '''\n",
    "                results = pd.read_sql_query(query, conn, params=[cutoff_time])\n",
    "            \n",
    "            if results.empty:\n",
    "                return {\n",
    "                    'status': 'WARNING',\n",
    "                    'message': 'No pipeline executions in the last 24 hours',\n",
    "                    'details': {'execution_count': 0}\n",
    "                }\n",
    "            \n",
    "            total_executions = results['execution_count'].sum()\n",
    "            unique_pipelines = len(results)\n",
    "            \n",
    "            return {\n",
    "                'status': 'PASSED',\n",
    "                'message': f'{total_executions} pipeline executions across {unique_pipelines} pipelines',\n",
    "                'details': {\n",
    "                    'total_executions': int(total_executions),\n",
    "                    'unique_pipelines': unique_pipelines\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'FAILED',\n",
    "                'message': f'Error checking recent executions: {str(e)}',\n",
    "                'details': {}\n",
    "            }\n",
    "    \n",
    "    def _check_data_quality(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check recent data quality metrics\"\"\"\n",
    "        try:\n",
    "            cutoff_time = (datetime.now() - timedelta(hours=24)).isoformat()\n",
    "            \n",
    "            with sqlite3.connect(self.monitor.db_path) as conn:\n",
    "                query = '''\n",
    "                    SELECT AVG(quality_score) as avg_quality, MIN(quality_score) as min_quality\n",
    "                    FROM quality_metrics \n",
    "                    WHERE timestamp >= ?\n",
    "                '''\n",
    "                result = pd.read_sql_query(query, conn, params=[cutoff_time])\n",
    "            \n",
    "            if result.empty or result['avg_quality'].iloc[0] is None:\n",
    "                return {\n",
    "                    'status': 'WARNING',\n",
    "                    'message': 'No quality metrics available',\n",
    "                    'details': {}\n",
    "                }\n",
    "            \n",
    "            avg_quality = result['avg_quality'].iloc[0]\n",
    "            min_quality = result['min_quality'].iloc[0]\n",
    "            \n",
    "            if avg_quality < 80:\n",
    "                status = 'FAILED'\n",
    "                message = f'Poor average data quality: {avg_quality:.1f}%'\n",
    "            elif min_quality < 70:\n",
    "                status = 'WARNING'\n",
    "                message = f'Some low quality data detected (min: {min_quality:.1f}%)'\n",
    "            else:\n",
    "                status = 'PASSED'\n",
    "                message = f'Good data quality (avg: {avg_quality:.1f}%)'\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'message': message,\n",
    "                'details': {\n",
    "                    'average_quality': round(avg_quality, 1),\n",
    "                    'minimum_quality': round(min_quality, 1)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'FAILED',\n",
    "                'message': f'Error checking data quality: {str(e)}',\n",
    "                'details': {}\n",
    "            }\n",
    "    \n",
    "    def _check_alert_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check recent alert status\"\"\"\n",
    "        try:\n",
    "            cutoff_time = (datetime.now() - timedelta(hours=24)).isoformat()\n",
    "            \n",
    "            with sqlite3.connect(self.monitor.db_path) as conn:\n",
    "                query = '''\n",
    "                    SELECT severity, COUNT(*) as count\n",
    "                    FROM alerts \n",
    "                    WHERE timestamp >= ? AND resolved = 0\n",
    "                    GROUP BY severity\n",
    "                '''\n",
    "                results = pd.read_sql_query(query, conn, params=[cutoff_time])\n",
    "            \n",
    "            if results.empty:\n",
    "                return {\n",
    "                    'status': 'PASSED',\n",
    "                    'message': 'No unresolved alerts',\n",
    "                    'details': {'unresolved_alerts': 0}\n",
    "                }\n",
    "            \n",
    "            alert_counts = dict(zip(results['severity'], results['count']))\n",
    "            total_alerts = results['count'].sum()\n",
    "            \n",
    "            if alert_counts.get('CRITICAL', 0) > 0:\n",
    "                status = 'FAILED'\n",
    "                message = f'{alert_counts[\"CRITICAL\"]} critical alerts unresolved'\n",
    "            elif alert_counts.get('HIGH', 0) > 0:\n",
    "                status = 'WARNING'\n",
    "                message = f'{alert_counts[\"HIGH\"]} high severity alerts unresolved'\n",
    "            else:\n",
    "                status = 'WARNING'\n",
    "                message = f'{total_alerts} alerts need attention'\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'message': message,\n",
    "                'details': {\n",
    "                    'unresolved_alerts': int(total_alerts),\n",
    "                    'by_severity': alert_counts\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'FAILED',\n",
    "                'message': f'Error checking alerts: {str(e)}',\n",
    "                'details': {}\n",
    "            }\n",
    "    \n",
    "    def _check_resource_utilization(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check system resource utilization (simulated)\"\"\"\n",
    "        # In production, this would check actual system resources\n",
    "        import random\n",
    "        \n",
    "        cpu_usage = random.uniform(20, 85)\n",
    "        memory_usage = random.uniform(30, 75)\n",
    "        disk_usage = random.uniform(40, 90)\n",
    "        \n",
    "        max_usage = max(cpu_usage, memory_usage, disk_usage)\n",
    "        \n",
    "        if max_usage > 90:\n",
    "            status = 'FAILED'\n",
    "            message = 'Critical resource utilization detected'\n",
    "        elif max_usage > 80:\n",
    "            status = 'WARNING'\n",
    "            message = 'High resource utilization'\n",
    "        else:\n",
    "            status = 'PASSED'\n",
    "            message = 'Resource utilization normal'\n",
    "        \n",
    "        return {\n",
    "            'status': status,\n",
    "            'message': message,\n",
    "            'details': {\n",
    "                'cpu_usage': round(cpu_usage, 1),\n",
    "                'memory_usage': round(memory_usage, 1),\n",
    "                'disk_usage': round(disk_usage, 1)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _check_data_freshness(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check data freshness\"\"\"\n",
    "        try:\n",
    "            # Check when the last pipeline ran\n",
    "            with sqlite3.connect(self.monitor.db_path) as conn:\n",
    "                query = '''\n",
    "                    SELECT MAX(timestamp) as last_execution\n",
    "                    FROM pipeline_metrics \n",
    "                    WHERE metric_name = 'total_execution_time'\n",
    "                '''\n",
    "                result = pd.read_sql_query(query, conn)\n",
    "            \n",
    "            if result.empty or result['last_execution'].iloc[0] is None:\n",
    "                return {\n",
    "                    'status': 'FAILED',\n",
    "                    'message': 'No recent pipeline executions found',\n",
    "                    'details': {}\n",
    "                }\n",
    "            \n",
    "            last_execution = pd.to_datetime(result['last_execution'].iloc[0])\n",
    "            hours_since = (datetime.now() - last_execution).total_seconds() / 3600\n",
    "            \n",
    "            if hours_since > 24:\n",
    "                status = 'FAILED'\n",
    "                message = f'Data is stale (last update: {hours_since:.1f} hours ago)'\n",
    "            elif hours_since > 12:\n",
    "                status = 'WARNING'\n",
    "                message = f'Data is getting stale (last update: {hours_since:.1f} hours ago)'\n",
    "            else:\n",
    "                status = 'PASSED'\n",
    "                message = f'Data is fresh (last update: {hours_since:.1f} hours ago)'\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'message': message,\n",
    "                'details': {\n",
    "                    'last_execution': last_execution.isoformat(),\n",
    "                    'hours_since_last_update': round(hours_since, 1)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'FAILED',\n",
    "                'message': f'Error checking data freshness: {str(e)}',\n",
    "                'details': {}\n",
    "            }\n",
    "    \n",
    "    def _generate_health_recommendations(self, health_check: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on health check results\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        for check_name, check_result in health_check['checks'].items():\n",
    "            if check_result['status'] == 'FAILED':\n",
    "                if check_name == 'recent_executions':\n",
    "                    recommendations.append('Investigate why pipelines are not executing')\n",
    "                elif check_name == 'data_quality':\n",
    "                    recommendations.append('Review data sources and validation rules')\n",
    "                elif check_name == 'alerts':\n",
    "                    recommendations.append('Resolve critical alerts immediately')\n",
    "                elif check_name == 'resources':\n",
    "                    recommendations.append('Scale infrastructure or optimize resource usage')\n",
    "                elif check_name == 'data_freshness':\n",
    "                    recommendations.append('Check pipeline scheduling and execution')\n",
    "            \n",
    "            elif check_result['status'] == 'WARNING':\n",
    "                if check_name == 'data_quality':\n",
    "                    recommendations.append('Monitor data quality trends closely')\n",
    "                elif check_name == 'alerts':\n",
    "                    recommendations.append('Review and resolve pending alerts')\n",
    "                elif check_name == 'resources':\n",
    "                    recommendations.append('Plan for resource scaling')\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append('System is healthy - continue regular monitoring')\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def generate_operational_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive operational report\"\"\"\n",
    "        health_check = self.execute_health_check()\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"# üìã Pipeline Operational Status Report\")\n",
    "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Overall Status\n",
    "        status_icon = {\n",
    "            'HEALTHY': '‚úÖ',\n",
    "            'WARNING': '‚ö†Ô∏è',\n",
    "            'UNHEALTHY': '‚ùå',\n",
    "            'ERROR': 'üö®'\n",
    "        }.get(health_check['overall_status'], '‚ùì')\n",
    "        \n",
    "        report.append(f\"## {status_icon} Overall Status: {health_check['overall_status']}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Health Check Results\n",
    "        report.append(\"## üîç Health Check Results\")\n",
    "        \n",
    "        for check_name, check_result in health_check['checks'].items():\n",
    "            check_icon = {\n",
    "                'PASSED': '‚úÖ',\n",
    "                'WARNING': '‚ö†Ô∏è',\n",
    "                'FAILED': '‚ùå'\n",
    "            }.get(check_result['status'], '‚ùì')\n",
    "            \n",
    "            report.append(f\"### {check_icon} {check_name.replace('_', ' ').title()}\")\n",
    "            report.append(f\"- **Status**: {check_result['status']}\")\n",
    "            report.append(f\"- **Message**: {check_result['message']}\")\n",
    "            \n",
    "            if check_result['details']:\n",
    "                report.append(f\"- **Details**: {check_result['details']}\")\n",
    "            \n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Issues\n",
    "        if health_check['issues']:\n",
    "            report.append(\"## üö® Issues Identified\")\n",
    "            for i, issue in enumerate(health_check['issues'], 1):\n",
    "                report.append(f\"{i}. {issue}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(\"## üí° Recommendations\")\n",
    "        for i, recommendation in enumerate(health_check['recommendations'], 1):\n",
    "            report.append(f\"{i}. {recommendation}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Operational Procedures\n",
    "        report.append(\"## üìã Available Procedures\")\n",
    "        for proc_name, proc_config in self.procedures.items():\n",
    "            report.append(f\"### {proc_name.replace('_', ' ').title()}\")\n",
    "            report.append(f\"- **Frequency**: {proc_config['frequency']}\")\n",
    "            report.append(f\"- **Description**: {proc_config['description']}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Runbooks\n",
    "        report.append(\"## üìñ Available Runbooks\")\n",
    "        for runbook_name, runbook_config in self.runbooks.items():\n",
    "            severity_icon = {\n",
    "                'HIGH': 'üî¥',\n",
    "                'MEDIUM': 'üü†',\n",
    "                'LOW': 'üü°'\n",
    "            }.get(runbook_config['severity'], '‚ö™')\n",
    "            \n",
    "            report.append(f\"### {severity_icon} {runbook_config['title']}\")\n",
    "            report.append(f\"- **Severity**: {runbook_config['severity']}\")\n",
    "            report.append(f\"- **Description**: {runbook_config['description']}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        report.append(\"---\")\n",
    "        report.append(\"*Report generated by Pipeline Operational Procedures*\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Initialize operational procedures\n",
    "ops_procedures = OperationalProcedures(monitor)\n",
    "\n",
    "print(\"üìã Operational Procedures initialized!\")\n",
    "print(f\"üìù Procedures defined: {len(ops_procedures.procedures)}\")\n",
    "print(f\"‚úÖ Checklists created: {len(ops_procedures.checklists)}\")\n",
    "print(f\"üìñ Runbooks available: {len(ops_procedures.runbooks)}\")\n",
    "\n",
    "# Execute health check\n",
    "print(\"\\nüîç Executing comprehensive health check...\")\n",
    "health_result = ops_procedures.execute_health_check()\n",
    "\n",
    "print(f\"\\nüìä Health Check Results:\")\n",
    "print(f\"  Overall Status: {health_result['overall_status']}\")\n",
    "print(f\"  Checks Performed: {len(health_result['checks'])}\")\n",
    "print(f\"  Issues Found: {len(health_result['issues'])}\")\n",
    "print(f\"  Recommendations: {len(health_result['recommendations'])}\")\n",
    "\n",
    "# Generate operational report\n",
    "operational_report = ops_procedures.generate_operational_report()\n",
    "\n",
    "print(\"\\nüìã OPERATIONAL STATUS REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(operational_report)\n",
    "\n",
    "# Save operational report\n",
    "ops_report_filename = f\"../data/output/operational_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "try:\n",
    "    with open(ops_report_filename, 'w') as f:\n",
    "        f.write(operational_report)\n",
    "    print(f\"\\nüíæ Operational report saved to: {ops_report_filename}\")\nexcept Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Could not save operational report: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "Congratulations! You've completed the comprehensive **Pipeline Monitoring and Operations** tutorial. You've built a production-ready monitoring and alerting system that provides:\n",
    "\n",
    "### ‚úÖ **Monitoring Infrastructure**\n",
    "- **üìä Comprehensive Metrics Collection**: Pipeline performance, data quality, and system health\n",
    "- **üóÑÔ∏è Monitoring Database**: Structured storage for all monitoring data\n",
    "- **üìà Real-time Dashboards**: Visual monitoring with multiple chart types\n",
    "- **üìã Performance Analysis**: Trend analysis and bottleneck identification\n",
    "\n",
    "### ‚úÖ **Advanced Alerting System**\n",
    "- **üö® Multi-level Alerts**: Critical, High, Medium, and Low severity levels\n",
    "- **üì¢ Multiple Notification Channels**: Email, Slack, SMS, and PagerDuty integration\n",
    "- **‚è∞ Smart Cooldown**: Prevents alert spam with configurable cooldown periods\n",
    "- **üîÑ Escalation Logic**: Automatic escalation for unresolved issues\n",
    "\n",
    "### ‚úÖ **Performance Optimization**\n",
    "- **üìä Trend Analysis**: Identify performance patterns and degradation\n",
    "- **üéØ Bottleneck Detection**: Pinpoint slow stages and optimization opportunities\n",
    "- **üí° Automated Recommendations**: AI-driven optimization suggestions\n",
    "- **üìà Capacity Planning**: Forecast future resource needs\n",
    "\n",
    "### ‚úÖ **Operational Excellence**\n",
    "- **üìã Standard Procedures**: Daily, weekly, and monthly operational tasks\n",
    "- **‚úÖ Operational Checklists**: Ensure consistent execution of procedures\n",
    "- **üìñ Detailed Runbooks**: Step-by-step incident response procedures\n",
    "- **üîç Health Checks**: Automated system health assessment\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ What You've Accomplished\n",
    "\n",
    "Through this 7-part tutorial series, you've built a **complete, enterprise-grade data ingestion pipeline** with:\n",
    "\n",
    "### üéì **Technical Skills Mastered**\n",
    "1. **üì• Data Ingestion**: Multi-source data collection (CSV, JSON, APIs, databases)\n",
    "2. **üîç Data Validation**: Comprehensive quality scoring and business rule validation\n",
    "3. **üßπ Data Transformation**: Cleaning, standardization, and enrichment\n",
    "4. **üíæ Data Storage**: Database operations and file management\n",
    "5. **üîÑ Pipeline Orchestration**: End-to-end workflow management\n",
    "6. **üìà Monitoring & Alerting**: Production-ready observability\n",
    "7. **üõ†Ô∏è Operations**: Incident response and maintenance procedures\n",
    "\n",
    "### üèóÔ∏è **Architecture Patterns Learned**\n",
    "- **Modular Design**: Loosely coupled, reusable components\n",
    "- **Error Handling**: Graceful failure management and recovery\n",
    "- **Observability**: Comprehensive logging, metrics, and tracing\n",
    "- **Scalability**: Horizontal scaling and performance optimization\n",
    "- **Reliability**: Fault tolerance and data consistency\n",
    "\n",
    "### üíº **Business Value Delivered**\n",
    "- **Automated Data Processing**: Hands-off data pipeline operations\n",
    "- **Data Quality Assurance**: Consistent, reliable data for analytics\n",
    "- **Operational Efficiency**: Reduced manual intervention and faster issue resolution\n",
    "- **Scalable Foundation**: Ready for enterprise-scale data volumes\n",
    "- **Compliance Ready**: Audit trails and data lineage tracking\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps for Production\n",
    "\n",
    "To deploy this pipeline in production, consider these enhancements:\n",
    "\n",
    "### üîí **Security & Compliance**\n",
    "- **Data Encryption**: Encrypt data at rest and in transit\n",
    "- **Access Control**: Implement role-based access control (RBAC)\n",
    "- **Audit Logging**: Comprehensive audit trails for compliance\n",
    "- **Data Privacy**: GDPR/CCPA compliance features\n",
    "\n",
    "### ‚òÅÔ∏è **Cloud Deployment**\n",
    "- **Container Orchestration**: Deploy with Docker and Kubernetes\n",
    "- **Cloud Services**: Leverage AWS/GCP/Azure managed services\n",
    "- **Auto-scaling**: Implement horizontal and vertical scaling\n",
    "- **Multi-region**: Deploy across multiple regions for reliability\n",
    "\n",
    "### üîÑ **Advanced Features**\n",
    "- **Stream Processing**: Real-time data processing with Kafka/Kinesis\n",
    "- **Machine Learning**: Automated anomaly detection and data profiling\n",
    "- **Data Lineage**: Complete data lineage tracking and visualization\n",
    "- **Schema Evolution**: Automatic schema migration and compatibility\n",
    "\n",
    "### üìä **Enterprise Integration**\n",
    "- **Data Catalog**: Integrate with enterprise data catalogs\n",
    "- **Workflow Orchestration**: Use Apache Airflow or Prefect\n",
    "- **BI Integration**: Connect to Tableau, PowerBI, or Looker\n",
    "- **API Gateway**: Expose pipeline APIs for external consumption\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Recommended Learning Path\n",
    "\n",
    "Continue your data engineering journey with these topics:\n",
    "\n",
    "### üéØ **Immediate Next Steps**\n",
    "1. **Apache Airflow**: Workflow orchestration and scheduling\n",
    "2. **Apache Kafka**: Stream processing and event-driven architecture\n",
    "3. **Docker & Kubernetes**: Containerization and orchestration\n",
    "4. **Cloud Platforms**: AWS/GCP/Azure data services\n",
    "\n",
    "### üèóÔ∏è **Advanced Topics**\n",
    "1. **Apache Spark**: Big data processing and analytics\n",
    "2. **Data Mesh**: Decentralized data architecture\n",
    "3. **MLOps**: Machine learning pipeline operations\n",
    "4. **Data Governance**: Enterprise data management\n",
    "\n",
    "### üìñ **Recommended Resources**\n",
    "- **Books**: \"Designing Data-Intensive Applications\" by Martin Kleppmann\n",
    "- **Courses**: Data Engineering courses on Coursera, Udacity, or Pluralsight\n",
    "- **Certifications**: AWS/GCP/Azure data engineering certifications\n",
    "- **Communities**: Join data engineering Slack communities and forums\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed a comprehensive data engineering tutorial series and built a production-ready data ingestion pipeline with enterprise-grade monitoring and operations capabilities.\n",
    "\n",
    "**You're now equipped with the skills and knowledge to:**\n",
    "- ‚úÖ Design and implement scalable data pipelines\n",
    "- ‚úÖ Ensure data quality and reliability\n",
    "- ‚úÖ Monitor and operate production data systems\n",
    "- ‚úÖ Handle incidents and optimize performance\n",
    "- ‚úÖ Build career-ready data engineering solutions\n",
    "\n",
    "**Keep building, keep learning, and welcome to the world of Data Engineering! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "*Thank you for completing the Data Ingestion Pipeline Tutorial Series!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}