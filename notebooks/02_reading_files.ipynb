{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÅ Reading Files - Mastering File-Based Data Ingestion\n",
    "\n",
    "Welcome to the second tutorial in our **Data Ingestion Pipeline** series! In this notebook, you'll master the art of reading and processing different file formats - the foundation of most data ingestion pipelines.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ‚úÖ Read CSV, JSON, and Excel files with confidence\n",
    "- ‚úÖ Handle different encodings and file formats\n",
    "- ‚úÖ Process large files efficiently\n",
    "- ‚úÖ Implement robust error handling\n",
    "- ‚úÖ Automate file monitoring and processing\n",
    "- ‚úÖ Apply best practices for file-based ingestion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Imports\n",
    "\n",
    "Let's start by importing the libraries we'll need and setting up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for file processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File monitoring (we'll simulate this)\n",
    "import hashlib\n",
    "import shutil\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")\n",
    "print(f\"üêç Python version: {sys.version.split()[0]}\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Setting Up Our File Structure\n",
    "\n",
    "Let's create a proper directory structure for our file processing examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure for our examples\n",
    "base_dir = Path(\"tutorial_data\")\n",
    "directories = [\n",
    "    base_dir / \"input\" / \"csv\",\n",
    "    base_dir / \"input\" / \"json\", \n",
    "    base_dir / \"input\" / \"excel\",\n",
    "    base_dir / \"processed\",\n",
    "    base_dir / \"output\",\n",
    "    base_dir / \"archive\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"üìÅ Created directory: {directory}\")\n",
    "\n",
    "print(\"\\n‚úÖ Directory structure created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Creating Sample Data Files\n",
    "\n",
    "Before we can read files, let's create some realistic sample data files to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample CSV data\n",
    "def create_sample_csv_files():\n",
    "    \"\"\"Create sample CSV files with different characteristics\"\"\"\n",
    "    \n",
    "    # Sample 1: Clean, well-formatted data\n",
    "    clean_data = {\n",
    "        'order_id': ['ORD-2024-001', 'ORD-2024-002', 'ORD-2024-003', 'ORD-2024-004'],\n",
    "        'customer_name': ['John Doe', 'Jane Smith', 'Bob Wilson', 'Alice Johnson'],\n",
    "        'product': ['iPhone 15', 'MacBook Pro', 'AirPods Pro', 'iPad Air'],\n",
    "        'quantity': [1, 1, 2, 1],\n",
    "        'price': [999.99, 1999.99, 249.99, 599.99],\n",
    "        'order_date': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18'],\n",
    "        'store_location': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "    }\n",
    "    \n",
    "    df_clean = pd.DataFrame(clean_data)\n",
    "    df_clean.to_csv(base_dir / \"input\" / \"csv\" / \"orders_clean.csv\", index=False)\n",
    "    \n",
    "    # Sample 2: Data with issues (missing values, inconsistent formats)\n",
    "    messy_data = {\n",
    "        'order_id': ['ORD-2024-005', '', 'ORD-2024-007', 'ORD-2024-008'],\n",
    "        'customer_name': ['mary johnson', 'DAVID BROWN', '', 'Sarah Connor'],\n",
    "        'product': ['samsung galaxy s24', 'Dell XPS 13', 'Sony WH-1000XM4', 'Nintendo Switch'],\n",
    "        'quantity': [1, 0, 2, 1],\n",
    "        'price': [899.99, 1299.99, 349.99, 299.99],\n",
    "        'order_date': ['01/19/2024', '2024-01-20', 'Jan 21, 2024', '2024-01-22'],\n",
    "        'store_location': ['miami', 'SEATTLE', 'Phoenix', 'Boston']\n",
    "    }\n",
    "    \n",
    "    df_messy = pd.DataFrame(messy_data)\n",
    "    df_messy.to_csv(base_dir / \"input\" / \"csv\" / \"orders_messy.csv\", index=False)\n",
    "    \n",
    "    # Sample 3: Large dataset (for performance testing)\n",
    "    np.random.seed(42)\n",
    "    large_data = {\n",
    "        'order_id': [f'ORD-2024-{i:06d}' for i in range(1000, 2000)],\n",
    "        'customer_name': [f'Customer {i}' for i in range(1000)],\n",
    "        'product': np.random.choice(['iPhone 15', 'MacBook Pro', 'AirPods Pro', 'iPad Air'], 1000),\n",
    "        'quantity': np.random.randint(1, 5, 1000),\n",
    "        'price': np.random.uniform(99.99, 1999.99, 1000).round(2),\n",
    "        'order_date': pd.date_range('2024-01-01', periods=1000, freq='H').strftime('%Y-%m-%d'),\n",
    "        'store_location': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston'], 1000)\n",
    "    }\n",
    "    \n",
    "    df_large = pd.DataFrame(large_data)\n",
    "    df_large.to_csv(base_dir / \"input\" / \"csv\" / \"orders_large.csv\", index=False)\n",
    "    \n",
    "    return len(df_clean), len(df_messy), len(df_large)\n",
    "\n",
    "# Create the CSV files\n",
    "clean_count, messy_count, large_count = create_sample_csv_files()\n",
    "print(f\"üìä Created CSV files:\")\n",
    "print(f\"  - orders_clean.csv: {clean_count} records\")\n",
    "print(f\"  - orders_messy.csv: {messy_count} records\")\n",
    "print(f\"  - orders_large.csv: {large_count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample JSON files\n",
    "def create_sample_json_files():\n",
    "    \"\"\"Create sample JSON files with different structures\"\"\"\n",
    "    \n",
    "    # Sample 1: Simple JSON structure\n",
    "    simple_json = {\n",
    "        \"metadata\": {\n",
    "            \"source\": \"mobile_app\",\n",
    "            \"version\": \"2.1.0\",\n",
    "            \"timestamp\": \"2024-01-15T12:00:00Z\"\n",
    "        },\n",
    "        \"orders\": [\n",
    "            {\n",
    "                \"order_id\": \"APP-2024-001\",\n",
    "                \"customer\": {\n",
    "                    \"name\": \"Emma Watson\",\n",
    "                    \"email\": \"emma@example.com\",\n",
    "                    \"tier\": \"premium\"\n",
    "                },\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"product\": \"iPhone 15 Pro\",\n",
    "                        \"quantity\": 1,\n",
    "                        \"price\": 1199.99\n",
    "                    }\n",
    "                ],\n",
    "                \"total\": 1199.99,\n",
    "                \"order_date\": \"2024-01-15T11:30:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"order_id\": \"APP-2024-002\",\n",
    "                \"customer\": {\n",
    "                    \"name\": \"Tom Hardy\",\n",
    "                    \"email\": \"tom@example.com\",\n",
    "                    \"tier\": \"standard\"\n",
    "                },\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"product\": \"AirPods Pro\",\n",
    "                        \"quantity\": 2,\n",
    "                        \"price\": 249.99\n",
    "                    }\n",
    "                ],\n",
    "                \"total\": 499.98,\n",
    "                \"order_date\": \"2024-01-15T14:15:00Z\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(base_dir / \"input\" / \"json\" / \"mobile_orders.json\", 'w') as f:\n",
    "        json.dump(simple_json, f, indent=2)\n",
    "    \n",
    "    # Sample 2: Flat JSON structure (array of objects)\n",
    "    flat_json = [\n",
    "        {\n",
    "            \"order_id\": \"WEB-2024-001\",\n",
    "            \"customer_name\": \"Chris Evans\",\n",
    "            \"customer_email\": \"chris@example.com\",\n",
    "            \"product\": \"MacBook Air\",\n",
    "            \"quantity\": 1,\n",
    "            \"price\": 1299.99,\n",
    "            \"order_date\": \"2024-01-16T09:00:00Z\",\n",
    "            \"source\": \"website\"\n",
    "        },\n",
    "        {\n",
    "            \"order_id\": \"WEB-2024-002\",\n",
    "            \"customer_name\": \"Scarlett Johansson\",\n",
    "            \"customer_email\": \"scarlett@example.com\",\n",
    "            \"product\": \"iPad Pro\",\n",
    "            \"quantity\": 1,\n",
    "            \"price\": 1099.99,\n",
    "            \"order_date\": \"2024-01-16T10:30:00Z\",\n",
    "            \"source\": \"website\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    with open(base_dir / \"input\" / \"json\" / \"website_orders.json\", 'w') as f:\n",
    "        json.dump(flat_json, f, indent=2)\n",
    "    \n",
    "    return len(simple_json['orders']), len(flat_json)\n",
    "\n",
    "# Create the JSON files\n",
    "mobile_count, web_count = create_sample_json_files()\n",
    "print(f\"üìÑ Created JSON files:\")\n",
    "print(f\"  - mobile_orders.json: {mobile_count} orders\")\n",
    "print(f\"  - website_orders.json: {web_count} orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Reading CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) files are the most common format for data exchange. Let's explore different ways to read and process them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CSV reading\n",
    "def read_csv_basic():\n",
    "    \"\"\"Demonstrate basic CSV reading\"\"\"\n",
    "    \n",
    "    print(\"üìä Reading CSV Files - Basic Approach\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read the clean CSV file\n",
    "    csv_file = base_dir / \"input\" / \"csv\" / \"orders_clean.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Basic read\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully read {len(df)} records from {csv_file.name}\")\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        print(f\"üìè Shape: {df.shape}\")\n",
    "        print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "        \n",
    "        print(\"\\nüìä First 3 records:\")\n",
    "        display(df.head(3))\n",
    "        \n",
    "        print(\"\\nüìà Data types:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read the basic CSV\n",
    "df_clean = read_csv_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CSV reading with options\n",
    "def read_csv_advanced():\n",
    "    \"\"\"Demonstrate advanced CSV reading with various options\"\"\"\n",
    "    \n",
    "    print(\"üìä Reading CSV Files - Advanced Techniques\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    csv_file = base_dir / \"input\" / \"csv\" / \"orders_messy.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Read with specific data types and options\n",
    "        df = pd.read_csv(\n",
    "            csv_file,\n",
    "            dtype={\n",
    "                'order_id': 'string',\n",
    "                'customer_name': 'string', \n",
    "                'product': 'string',\n",
    "                'quantity': 'Int64',  # Nullable integer\n",
    "                'price': 'float64',\n",
    "                'store_location': 'string'\n",
    "            },\n",
    "            parse_dates=['order_date'],\n",
    "            na_values=['', 'NULL', 'null', 'N/A'],  # Additional NA values\n",
    "            skipinitialspace=True,  # Skip spaces after delimiter\n",
    "            encoding='utf-8'  # Specify encoding\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Successfully read {len(df)} records with advanced options\")\n",
    "        \n",
    "        print(\"\\nüìä Data with issues:\")\n",
    "        display(df)\n",
    "        \n",
    "        print(\"\\nüîç Data quality analysis:\")\n",
    "        print(f\"Missing values per column:\")\n",
    "        missing_counts = df.isnull().sum()\n",
    "        for col, count in missing_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"  {col}: {count} missing ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüìà Improved data types:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read the messy CSV with advanced options\n",
    "df_messy = read_csv_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading large CSV files efficiently\n",
    "def read_csv_large_file():\n",
    "    \"\"\"Demonstrate efficient reading of large CSV files\"\"\"\n",
    "    \n",
    "    print(\"üìä Reading Large CSV Files - Performance Optimization\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    csv_file = base_dir / \"input\" / \"csv\" / \"orders_large.csv\"\n",
    "    \n",
    "    # Method 1: Read in chunks\n",
    "    print(\"üîÑ Method 1: Reading in chunks\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    chunk_size = 100\n",
    "    chunks = []\n",
    "    \n",
    "    try:\n",
    "        for i, chunk in enumerate(pd.read_csv(csv_file, chunksize=chunk_size)):\n",
    "            # Process each chunk (example: add chunk number)\n",
    "            chunk['chunk_number'] = i + 1\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            if i < 3:  # Show first few chunks\n",
    "                print(f\"  Processed chunk {i+1}: {len(chunk)} records\")\n",
    "        \n",
    "        # Combine all chunks\n",
    "        df_chunked = pd.concat(chunks, ignore_index=True)\n",
    "        chunk_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"  ‚úÖ Chunked reading: {len(df_chunked)} records in {chunk_time:.3f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error with chunked reading: {e}\")\n",
    "        df_chunked = None\n",
    "    \n",
    "    # Method 2: Optimized data types\n",
    "    print(\"\\n‚ö° Method 2: Optimized data types\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        df_optimized = pd.read_csv(\n",
    "            csv_file,\n",
    "            dtype={\n",
    "                'order_id': 'category',  # Use category for repeated strings\n",
    "                'customer_name': 'string',\n",
    "                'product': 'category',\n",
    "                'quantity': 'int8',  # Smaller integer type\n",
    "                'price': 'float32',  # Smaller float type\n",
    "                'store_location': 'category'\n",
    "            },\n",
    "            parse_dates=['order_date']\n",
    "        )\n",
    "        \n",
    "        optimized_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"  ‚úÖ Optimized reading: {len(df_optimized)} records in {optimized_time:.3f}s\")\n",
    "        \n",
    "        # Memory comparison\n",
    "        if df_chunked is not None:\n",
    "            chunked_memory = df_chunked.memory_usage(deep=True).sum() / 1024\n",
    "            optimized_memory = df_optimized.memory_usage(deep=True).sum() / 1024\n",
    "            \n",
    "            print(f\"\\nüíæ Memory usage comparison:\")\n",
    "            print(f\"  Chunked approach: {chunked_memory:.2f} KB\")\n",
    "            print(f\"  Optimized types: {optimized_memory:.2f} KB\")\n",
    "            print(f\"  Memory savings: {((chunked_memory - optimized_memory) / chunked_memory * 100):.1f}%\")\n",
    "        \n",
    "        return df_optimized\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error with optimized reading: {e}\")\n",
    "        return df_chunked\n",
    "\n",
    "# Read the large CSV file\n",
    "df_large = read_csv_large_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Reading JSON Files\n",
    "\n",
    "JSON (JavaScript Object Notation) files are flexible and can contain nested structures. Let's explore different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON files\n",
    "def read_json_files():\n",
    "    \"\"\"Demonstrate different approaches to reading JSON files\"\"\"\n",
    "    \n",
    "    print(\"üìÑ Reading JSON Files\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Method 1: Simple JSON structure (flat array)\n",
    "    print(\"üîÑ Method 1: Flat JSON structure\")\n",
    "    json_file1 = base_dir / \"input\" / \"json\" / \"website_orders.json\"\n",
    "    \n",
    "    try:\n",
    "        # Read directly with pandas\n",
    "        df_flat = pd.read_json(json_file1)\n",
    "        \n",
    "        print(f\"  ‚úÖ Read {len(df_flat)} records from flat JSON\")\n",
    "        print(f\"  üìã Columns: {list(df_flat.columns)}\")\n",
    "        \n",
    "        print(\"\\n  üìä Sample data:\")\n",
    "        display(df_flat.head(2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error reading flat JSON: {e}\")\n",
    "        df_flat = None\n",
    "    \n",
    "    # Method 2: Nested JSON structure\n",
    "    print(\"\\nüîÑ Method 2: Nested JSON structure\")\n",
    "    json_file2 = base_dir / \"input\" / \"json\" / \"mobile_orders.json\"\n",
    "    \n",
    "    try:\n",
    "        # Read with Python's json module first\n",
    "        with open(json_file2, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        print(f\"  üìã JSON structure keys: {list(json_data.keys())}\")\n",
    "        print(f\"  üìä Metadata: {json_data['metadata']}\")\n",
    "        \n",
    "        # Extract orders from nested structure\n",
    "        orders = json_data['orders']\n",
    "        \n",
    "        # Flatten nested customer data\n",
    "        flattened_orders = []\n",
    "        for order in orders:\n",
    "            flat_order = {\n",
    "                'order_id': order['order_id'],\n",
    "                'customer_name': order['customer']['name'],\n",
    "                'customer_email': order['customer']['email'],\n",
    "                'customer_tier': order['customer']['tier'],\n",
    "                'product': order['items'][0]['product'],  # Assuming single item\n",
    "                'quantity': order['items'][0]['quantity'],\n",
    "                'price': order['items'][0]['price'],\n",
    "                'total': order['total'],\n",
    "                'order_date': order['order_date'],\n",
    "                'source': json_data['metadata']['source'],\n",
    "                'app_version': json_data['metadata']['version']\n",
    "            }\n",
    "            flattened_orders.append(flat_order)\n",
    "        \n",
    "        df_nested = pd.DataFrame(flattened_orders)\n",
    "        \n",
    "        print(f\"  ‚úÖ Flattened {len(df_nested)} records from nested JSON\")\n",
    "        print(f\"  üìã Columns: {list(df_nested.columns)}\")\n",
    "        \n",
    "        print(\"\\n  üìä Sample flattened data:\")\n",
    "        display(df_nested.head(2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error reading nested JSON: {e}\")\n",
    "        df_nested = None\n",
    "    \n",
    "    return df_flat, df_nested\n",
    "\n",
    "# Read JSON files\n",
    "df_json_flat, df_json_nested = read_json_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced JSON processing with error handling\n",
    "def read_json_with_error_handling():\n",
    "    \"\"\"Demonstrate robust JSON reading with comprehensive error handling\"\"\"\n",
    "    \n",
    "    print(\"üìÑ Advanced JSON Processing with Error Handling\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    def safe_json_read(file_path):\n",
    "        \"\"\"Safely read JSON file with multiple fallback strategies\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Try different encodings\n",
    "            encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']\n",
    "            \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=encoding) as f:\n",
    "                        data = json.load(f)\n",
    "                    print(f\"  ‚úÖ Successfully read with {encoding} encoding\")\n",
    "                    return data, None\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                except json.JSONDecodeError as e:\n",
    "                    return None, f\"JSON decode error: {e}\"\n",
    "            \n",
    "            return None, \"Could not read file with any supported encoding\"\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            return None, f\"File not found: {file_path}\"\n",
    "        except Exception as e:\n",
    "            return None, f\"Unexpected error: {e}\"\n",
    "    \n",
    "    # Test with existing files\n",
    "    json_files = [\n",
    "        base_dir / \"input\" / \"json\" / \"mobile_orders.json\",\n",
    "        base_dir / \"input\" / \"json\" / \"website_orders.json\"\n",
    "    ]\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        print(f\"\\nüîÑ Processing: {json_file.name}\")\n",
    "        \n",
    "        data, error = safe_json_read(json_file)\n",
    "        \n",
    "        if error:\n",
    "            print(f\"  ‚ùå Error: {error}\")\n",
    "            continue\n",
    "        \n",
    "        # Process based on structure\n",
    "        if isinstance(data, list):\n",
    "            # Flat structure\n",
    "            df = pd.DataFrame(data)\n",
    "            df['source_file'] = json_file.name\n",
    "            all_data.append(df)\n",
    "            print(f\"  üìä Processed flat structure: {len(df)} records\")\n",
    "            \n",
    "        elif isinstance(data, dict) and 'orders' in data:\n",
    "            # Nested structure\n",
    "            orders = data['orders']\n",
    "            df = pd.json_normalize(orders)  # Automatically flatten nested data\n",
    "            df['source_file'] = json_file.name\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if 'metadata' in data:\n",
    "                for key, value in data['metadata'].items():\n",
    "                    df[f'metadata_{key}'] = value\n",
    "            \n",
    "            all_data.append(df)\n",
    "            print(f\"  üìä Processed nested structure: {len(df)} records\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Unknown JSON structure in {json_file.name}\")\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True, sort=False)\n",
    "        print(f\"\\n‚úÖ Combined all JSON data: {len(combined_df)} total records\")\n",
    "        print(f\"üìã Combined columns: {list(combined_df.columns)}\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"\\n‚ùå No data could be processed\")\n",
    "        return None\n",
    "\n",
    "# Process JSON files with error handling\n",
    "df_json_combined = read_json_with_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ File Processing Automation\n",
    "\n",
    "In real-world scenarios, you need to automatically monitor directories and process new files as they arrive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File monitoring and processing automation\n",
    "class FileProcessor:\n",
    "    \"\"\"Automated file processor with monitoring capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dir, processed_dir, output_dir):\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.processed_files = set()\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        for directory in [self.processed_dir, self.output_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate MD5 hash of file to detect changes\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                    hash_md5.update(chunk)\n",
    "            return hash_md5.hexdigest()\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error calculating hash for {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def discover_files(self):\n",
    "        \"\"\"Discover new files to process\"\"\"\n",
    "        new_files = []\n",
    "        \n",
    "        # Look for CSV and JSON files\n",
    "        patterns = ['**/*.csv', '**/*.json']\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for file_path in self.input_dir.glob(pattern):\n",
    "                if file_path.is_file():\n",
    "                    file_hash = self.get_file_hash(file_path)\n",
    "                    file_key = f\"{file_path.name}_{file_hash}\"\n",
    "                    \n",
    "                    if file_key not in self.processed_files:\n",
    "                        new_files.append((file_path, file_hash))\n",
    "        \n",
    "        return new_files\n",
    "    \n",
    "    def process_csv_file(self, file_path):\n",
    "        \"\"\"Process a CSV file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add processing metadata\n",
    "            df['source_file'] = file_path.name\n",
    "            df['processed_at'] = datetime.now().isoformat()\n",
    "            df['file_size_kb'] = file_path.stat().st_size / 1024\n",
    "            \n",
    "            return df, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def process_json_file(self, file_path):\n",
    "        \"\"\"Process a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle different JSON structures\n",
    "            if isinstance(data, list):\n",
    "                df = pd.DataFrame(data)\n",
    "            elif isinstance(data, dict) and 'orders' in data:\n",
    "                df = pd.json_normalize(data['orders'])\n",
    "                # Add metadata\n",
    "                if 'metadata' in data:\n",
    "                    for key, value in data['metadata'].items():\n",
    "                        df[f'metadata_{key}'] = value\n",
    "            else:\n",
    "                df = pd.json_normalize(data)\n",
    "            \n",
    "            # Add processing metadata\n",
    "            df['source_file'] = file_path.name\n",
    "            df['processed_at'] = datetime.now().isoformat()\n",
    "            df['file_size_kb'] = file_path.stat().st_size / 1024\n",
    "            \n",
    "            return df, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def process_file(self, file_path, file_hash):\n",
    "        \"\"\"Process a single file based on its extension\"\"\"\n",
    "        print(f\"  üîÑ Processing: {file_path.name}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if file_path.suffix.lower() == '.csv':\n",
    "            df, error = self.process_csv_file(file_path)\n",
    "        elif file_path.suffix.lower() == '.json':\n",
    "            df, error = self.process_json_file(file_path)\n",
    "        else:\n",
    "            return False, f\"Unsupported file type: {file_path.suffix}\"\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        if error:\n",
    "            print(f\"    ‚ùå Error: {error}\")\n",
    "            return False, error\n",
    "        \n",
    "        # Save processed data\n",
    "        output_file = self.output_dir / f\"processed_{file_path.stem}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Move original file to processed directory\n",
    "        processed_file = self.processed_dir / file_path.name\n",
    "        shutil.move(str(file_path), str(processed_file))\n",
    "        \n",
    "        # Mark as processed\n",
    "        file_key = f\"{file_path.name}_{file_hash}\"\n",
    "        self.processed_files.add(file_key)\n",
    "        \n",
    "        print(f\"    ‚úÖ Success: {len(df)} records in {processing_time:.3f}s\")\n",
    "        print(f\"    üìÅ Output: {output_file.name}\")\n",
    "        print(f\"    üì¶ Archived: {processed_file.name}\")\n",
    "        \n",
    "        return True, df\n",
    "    \n",
    "    def run_processing_cycle(self):\n",
    "        \"\"\"Run one cycle of file discovery and processing\"\"\"\n",
    "        print(\"üîÑ Starting file processing cycle...\")\n",
    "        \n",
    "        new_files = self.discover_files()\n",
    "        \n",
    "        if not new_files:\n",
    "            print(\"  üì≠ No new files found\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"  üìÅ Found {len(new_files)} new files\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for file_path, file_hash in new_files:\n",
    "            success, result = self.process_file(file_path, file_hash)\n",
    "            results.append({\n",
    "                'file': file_path.name,\n",
    "                'success': success,\n",
    "                'result': result\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create and test the file processor\n",
    "processor = FileProcessor(\n",
    "    input_dir=base_dir / \"input\",\n",
    "    processed_dir=base_dir / \"processed\",\n",
    "    output_dir=base_dir / \"output\"\n",
    ")\n",
    "\n",
    "print(\"ü§ñ File Processor Initialized\")\n",
    "print(f\"üìÅ Input directory: {processor.input_dir}\")\n",
    "print(f\"üì¶ Processed directory: {processor.processed_dir}\")\n",
    "print(f\"üì§ Output directory: {processor.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the file processing cycle\n",
    "print(\"üöÄ Running File Processing Cycle\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "results = processor.run_processing_cycle()\n",
    "\n",
    "# Summary of results\n",
    "if results:\n",
    "    print(f\"\\nüìä Processing Summary:\")\n",
    "    successful = sum(1 for r in results if r['success'])\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(f\"  ‚úÖ Successful: {successful}\")\n",
    "    print(f\"  ‚ùå Failed: {failed}\")\n",
    "    \n",
    "    if failed > 0:\n",
    "        print(f\"\\n‚ùå Failed files:\")\n",
    "        for result in results:\n",
    "            if not result['success']:\n",
    "                print(f\"  - {result['file']}: {result['result']}\")\n",
    "    \n",
    "    # Show output files\n",
    "    output_files = list(processor.output_dir.glob('*.csv'))\n",
    "    print(f\"\\nüì§ Generated output files:\")\n",
    "    for output_file in output_files:\n",
    "        file_size = output_file.stat().st_size / 1024\n",
    "        print(f\"  - {output_file.name} ({file_size:.2f} KB)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüì≠ No files were processed in this cycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç File Validation and Quality Checks\n",
    "\n",
    "Before processing files, it's important to validate their structure and quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File validation system\n",
    "class FileValidator:\n",
    "    \"\"\"Comprehensive file validation system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define expected schemas\n",
    "        self.schemas = {\n",
    "            'orders': {\n",
    "                'required_columns': ['order_id', 'customer_name', 'product', 'quantity', 'price'],\n",
    "                'optional_columns': ['order_date', 'store_location', 'customer_email'],\n",
    "                'data_types': {\n",
    "                    'quantity': 'numeric',\n",
    "                    'price': 'numeric'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def validate_file_structure(self, file_path):\n",
    "        \"\"\"Validate basic file structure and accessibility\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not file_path.exists():\n",
    "            issues.append(f\"File does not exist: {file_path}\")\n",
    "            return issues\n",
    "        \n",
    "        # Check file size\n",
    "        file_size = file_path.stat().st_size\n",
    "        if file_size == 0:\n",
    "            issues.append(\"File is empty\")\n",
    "        elif file_size > 100 * 1024 * 1024:  # 100MB\n",
    "            issues.append(f\"File is very large: {file_size / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # Check file extension\n",
    "        if file_path.suffix.lower() not in ['.csv', '.json']:\n",
    "            issues.append(f\"Unsupported file type: {file_path.suffix}\")\n",
    "        \n",
    "        # Check file permissions\n",
    "        if not os.access(file_path, os.R_OK):\n",
    "            issues.append(\"File is not readable\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def validate_csv_content(self, file_path, schema_name='orders'):\n",
    "        \"\"\"Validate CSV file content against schema\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        try:\n",
    "            # Try to read the file\n",
    "            df = pd.read_csv(file_path, nrows=100)  # Read first 100 rows for validation\n",
    "            \n",
    "            schema = self.schemas.get(schema_name, {})\n",
    "            \n",
    "            # Check required columns\n",
    "            required_cols = schema.get('required_columns', [])\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                issues.append(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "            # Check data types\n",
    "            data_types = schema.get('data_types', {})\n",
    "            for col, expected_type in data_types.items():\n",
    "                if col in df.columns:\n",
    "                    if expected_type == 'numeric':\n",
    "                        try:\n",
    "                            pd.to_numeric(df[col], errors='raise')\n",
    "                        except (ValueError, TypeError):\n",
    "                            issues.append(f\"Column '{col}' contains non-numeric values\")\n",
    "            \n",
    "            # Check for completely empty columns\n",
    "            empty_cols = df.columns[df.isnull().all()].tolist()\n",
    "            if empty_cols:\n",
    "                issues.append(f\"Completely empty columns: {empty_cols}\")\n",
    "            \n",
    "            # Check data quality\n",
    "            total_rows = len(df)\n",
    "            if total_rows == 0:\n",
    "                issues.append(\"No data rows found\")\n",
    "            else:\n",
    "                # Check missing value percentage\n",
    "                missing_pct = (df.isnull().sum() / total_rows * 100)\n",
    "                high_missing = missing_pct[missing_pct > 50]\n",
    "                if not high_missing.empty:\n",
    "                    issues.append(f\"High missing values (>50%): {high_missing.to_dict()}\")\n",
    "            \n",
    "        except pd.errors.EmptyDataError:\n",
    "            issues.append(\"CSV file is empty or has no data\")\n",
    "        except pd.errors.ParserError as e:\n",
    "            issues.append(f\"CSV parsing error: {e}\")\n",
    "        except Exception as e:\n",
    "            issues.append(f\"Unexpected error reading CSV: {e}\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def validate_json_content(self, file_path):\n",
    "        \"\"\"Validate JSON file content\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check if it's a valid structure for our use case\n",
    "            if isinstance(data, list):\n",
    "                if len(data) == 0:\n",
    "                    issues.append(\"JSON array is empty\")\n",
    "                else:\n",
    "                    # Check if all items have consistent structure\n",
    "                    first_keys = set(data[0].keys()) if data else set()\n",
    "                    for i, item in enumerate(data[1:], 1):\n",
    "                        if set(item.keys()) != first_keys:\n",
    "                            issues.append(f\"Inconsistent structure at item {i}\")\n",
    "                            break\n",
    "            \n",
    "            elif isinstance(data, dict):\n",
    "                if 'orders' in data:\n",
    "                    orders = data['orders']\n",
    "                    if not isinstance(orders, list):\n",
    "                        issues.append(\"'orders' field should be an array\")\n",
    "                    elif len(orders) == 0:\n",
    "                        issues.append(\"No orders found in JSON\")\n",
    "                else:\n",
    "                    issues.append(\"JSON structure not recognized (expected 'orders' field)\")\n",
    "            \n",
    "            else:\n",
    "                issues.append(\"JSON should be an object or array\")\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            issues.append(f\"Invalid JSON format: {e}\")\n",
    "        except Exception as e:\n",
    "            issues.append(f\"Unexpected error reading JSON: {e}\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def validate_file(self, file_path):\n",
    "        \"\"\"Comprehensive file validation\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        print(f\"üîç Validating: {file_path.name}\")\n",
    "        \n",
    "        all_issues = []\n",
    "        \n",
    "        # Structure validation\n",
    "        structure_issues = self.validate_file_structure(file_path)\n",
    "        all_issues.extend(structure_issues)\n",
    "        \n",
    "        # Content validation (only if structure is OK)\n",
    "        if not structure_issues:\n",
    "            if file_path.suffix.lower() == '.csv':\n",
    "                content_issues = self.validate_csv_content(file_path)\n",
    "            elif file_path.suffix.lower() == '.json':\n",
    "                content_issues = self.validate_json_content(file_path)\n",
    "            else:\n",
    "                content_issues = [\"Unsupported file type for content validation\"]\n",
    "            \n",
    "            all_issues.extend(content_issues)\n",
    "        \n",
    "        # Report results\n",
    "        if all_issues:\n",
    "            print(f\"  ‚ùå Validation failed ({len(all_issues)} issues):\")\n",
    "            for issue in all_issues:\n",
    "                print(f\"    - {issue}\")\n",
    "            return False, all_issues\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Validation passed\")\n",
    "            return True, []\n",
    "\n",
    "# Test file validation\n",
    "validator = FileValidator()\n",
    "\n",
    "print(\"üîç File Validation Tests\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test files in the processed directory\n",
    "test_files = list((base_dir / \"processed\").glob('*'))\n",
    "\n",
    "if test_files:\n",
    "    for test_file in test_files[:3]:  # Test first 3 files\n",
    "        is_valid, issues = validator.validate_file(test_file)\n",
    "        print()  # Empty line for readability\nelse:\n",
    "    print(\"üì≠ No files found in processed directory to validate\")\n",
    "    \n",
    "    # Create a test file with issues for demonstration\n",
    "    test_csv = base_dir / \"input\" / \"csv\" / \"test_invalid.csv\"\n",
    "    \n",
    "    # Create CSV with issues\n",
    "    invalid_data = pd.DataFrame({\n",
    "        'order_id': ['ORD-001', '', 'ORD-003'],  # Missing value\n",
    "        'customer_name': ['John', 'Jane', 'Bob'],\n",
    "        'product': ['iPhone', 'MacBook', 'iPad'],\n",
    "        'quantity': ['one', '2', 'three'],  # Non-numeric values\n",
    "        'price': [999.99, 'expensive', 599.99]  # Mixed types\n",
    "    })\n",
    "    \n",
    "    invalid_data.to_csv(test_csv, index=False)\n",
    "    print(f\"üìù Created test file with issues: {test_csv.name}\")\n",
    "    \n",
    "    # Validate the problematic file\n",
    "    is_valid, issues = validator.validate_file(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Optimization Techniques\n",
    "\n",
    "When dealing with large files or high-volume processing, performance optimization becomes crucial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization techniques\n",
    "def demonstrate_performance_techniques():\n",
    "    \"\"\"Show various performance optimization techniques for file processing\"\"\"\n",
    "    \n",
    "    print(\"‚ö° Performance Optimization Techniques\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Create a larger test file for performance testing\n",
    "    large_file = base_dir / \"input\" / \"csv\" / \"performance_test.csv\"\n",
    "    \n",
    "    if not large_file.exists():\n",
    "        print(\"üìù Creating large test file...\")\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        large_data = {\n",
    "            'order_id': [f'ORD-{i:08d}' for i in range(10000)],\n",
    "            'customer_name': [f'Customer {i}' for i in range(10000)],\n",
    "            'product': np.random.choice(['iPhone', 'MacBook', 'iPad', 'AirPods'], 10000),\n",
    "            'category': np.random.choice(['Electronics', 'Accessories'], 10000),\n",
    "            'quantity': np.random.randint(1, 10, 10000),\n",
    "            'price': np.random.uniform(99.99, 1999.99, 10000).round(2),\n",
    "            'order_date': pd.date_range('2024-01-01', periods=10000, freq='min').strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'store_location': np.random.choice(['NY', 'LA', 'Chicago', 'Houston'], 10000)\n",
    "        }\n",
    "        \n",
    "        pd.DataFrame(large_data).to_csv(large_file, index=False)\n",
    "        print(f\"  ‚úÖ Created {large_file.name} with 10,000 records\")\n",
    "    \n",
    "    # Technique 1: Basic reading (baseline)\n",
    "    print(\"\\nüîÑ Technique 1: Basic Reading (Baseline)\")\n",
    "    start_time = time.time()\n",
    "    df_basic = pd.read_csv(large_file)\n",
    "    basic_time = time.time() - start_time\n",
    "    basic_memory = df_basic.memory_usage(deep=True).sum() / 1024 / 1024  # MB\n",
    "    \n",
    "    print(f\"  ‚è±Ô∏è Time: {basic_time:.3f}s\")\n",
    "    print(f\"  üíæ Memory: {basic_memory:.2f} MB\")\n",
    "    print(f\"  üìä Records: {len(df_basic):,}\")\n",
    "    \n",
    "    # Technique 2: Optimized data types\n",
    "    print(\"\\nüîÑ Technique 2: Optimized Data Types\")\n",
    "    start_time = time.time()\n",
    "    df_optimized = pd.read_csv(\n",
    "        large_file,\n",
    "        dtype={\n",
    "            'order_id': 'string',\n",
    "            'customer_name': 'string',\n",
    "            'product': 'category',  # Repeated values\n",
    "            'category': 'category',\n",
    "            'quantity': 'int8',  # Small integers\n",
    "            'price': 'float32',  # Reduced precision\n",
    "            'store_location': 'category'\n",
    "        },\n",
    "        parse_dates=['order_date']\n",
    "    )\n",
    "    optimized_time = time.time() - start_time\n",
    "    optimized_memory = df_optimized.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    \n",
    "    print(f\"  ‚è±Ô∏è Time: {optimized_time:.3f}s ({optimized_time/basic_time:.2f}x)\")\n",
    "    print(f\"  üíæ Memory: {optimized_memory:.2f} MB ({optimized_memory/basic_memory:.2f}x)\")\n",
    "    print(f\"  üìä Records: {len(df_optimized):,}\")\n",
    "    \n",
    "    # Technique 3: Chunked processing\n",
    "    print(\"\\nüîÑ Technique 3: Chunked Processing\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    chunk_results = []\n",
    "    chunk_size = 1000\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(pd.read_csv(large_file, chunksize=chunk_size)):\n",
    "        # Process each chunk (example: calculate summary statistics)\n",
    "        chunk_summary = {\n",
    "            'chunk': chunk_num,\n",
    "            'records': len(chunk),\n",
    "            'total_revenue': chunk['price'].sum(),\n",
    "            'avg_quantity': chunk['quantity'].mean()\n",
    "        }\n",
    "        chunk_results.append(chunk_summary)\n",
    "    \n",
    "    chunked_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ‚è±Ô∏è Time: {chunked_time:.3f}s ({chunked_time/basic_time:.2f}x)\")\n",
    "    print(f\"  üìä Processed {len(chunk_results)} chunks\")\n",
    "    print(f\"  üí∞ Total revenue: ${sum(r['total_revenue'] for r in chunk_results):,.2f}\")\n",
    "    \n",
    "    # Technique 4: Column selection\n",
    "    print(\"\\nüîÑ Technique 4: Column Selection\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Only read columns we need\n",
    "    df_selected = pd.read_csv(\n",
    "        large_file,\n",
    "        usecols=['order_id', 'product', 'quantity', 'price'],\n",
    "        dtype={\n",
    "            'order_id': 'string',\n",
    "            'product': 'category',\n",
    "            'quantity': 'int8',\n",
    "            'price': 'float32'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    selected_time = time.time() - start_time\n",
    "    selected_memory = df_selected.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    \n",
    "    print(f\"  ‚è±Ô∏è Time: {selected_time:.3f}s ({selected_time/basic_time:.2f}x)\")\n",
    "    print(f\"  üíæ Memory: {selected_memory:.2f} MB ({selected_memory/basic_memory:.2f}x)\")\n",
    "    print(f\"  üìä Columns: {len(df_selected.columns)} vs {len(df_basic.columns)}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\nüìä Performance Summary:\")\n",
    "    techniques = [\n",
    "        ('Basic Reading', basic_time, basic_memory),\n",
    "        ('Optimized Types', optimized_time, optimized_memory),\n",
    "        ('Chunked Processing', chunked_time, 0),  # Memory not measured for chunks\n",
    "        ('Column Selection', selected_time, selected_memory)\n",
    "    ]\n",
    "    \n",
    "    for name, time_taken, memory_used in techniques:\n",
    "        time_improvement = f\"{basic_time/time_taken:.1f}x faster\" if time_taken < basic_time else \"baseline\"\n",
    "        memory_improvement = f\"{basic_memory/memory_used:.1f}x less memory\" if memory_used > 0 and memory_used < basic_memory else \"N/A\"\n",
    "        print(f\"  {name:20} | {time_taken:.3f}s ({time_improvement:12}) | {memory_improvement}\")\n",
    "\n",
    "# Run performance demonstration\n",
    "demonstrate_performance_techniques()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Best Practices Summary\n",
    "\n",
    "Let's summarize the key best practices for file-based data ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "best_practices = {\n",
    "    'Category': [\n",
    "        'File Handling',\n",
    "        'Performance',\n",
    "        'Error Handling',\n",
    "        'Data Quality',\n",
    "        'Security',\n",
    "        'Monitoring',\n",
    "        'Maintenance'\n",
    "    ],\n",
    "    'Best Practices': [\n",
    "        'Validate files before processing, Use appropriate encodings, Archive processed files',\n",
    "        'Optimize data types, Process in chunks, Select only needed columns',\n",
    "        'Handle encoding issues, Graceful failure recovery, Comprehensive logging',\n",
    "        'Schema validation, Missing value checks, Data type verification',\n",
    "        'Secure file permissions, Validate file sources, Sanitize file names',\n",
    "        'Track processing metrics, Log all operations, Set up alerts',\n",
    "        'Regular cleanup, Archive old files, Monitor disk space'\n",
    "    ],\n",
    "    'Priority': ['Critical', 'High', 'Critical', 'High', 'Medium', 'High', 'Medium']\n",
    "}\n",
    "\n",
    "df_practices = pd.DataFrame(best_practices)\n",
    "\n",
    "print(\"üí° File Processing Best Practices\")\n",
    "print(\"=\" * 40)\n",
    "display(df_practices)\n",
    "\n",
    "# Create a visual summary\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Priority distribution\n",
    "priority_counts = df_practices['Priority'].value_counts()\n",
    "colors = ['red', 'orange', 'yellow']\n",
    "ax1.pie(priority_counts.values, labels=priority_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "ax1.set_title('Best Practices by Priority')\n",
    "\n",
    "# Category importance (mock data for visualization)\n",
    "categories = df_practices['Category'].tolist()\n",
    "importance_scores = [9, 8, 9, 8, 6, 8, 6]  # Mock importance scores\n",
    "\n",
    "bars = ax2.barh(categories, importance_scores, color='skyblue')\n",
    "ax2.set_xlabel('Importance Score (1-10)')\n",
    "ax2.set_title('Category Importance Scores')\n",
    "ax2.set_xlim(0, 10)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, importance_scores):\n",
    "    ax2.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "             str(score), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Takeaways:\")\n",
    "print(\"  1. Always validate files before processing\")\n",
    "print(\"  2. Optimize for performance with large files\")\n",
    "print(\"  3. Implement robust error handling\")\n",
    "print(\"  4. Monitor and log all operations\")\n",
    "print(\"  5. Maintain clean and organized file systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup and Summary\n",
    "\n",
    "Let's clean up our test files and summarize what we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup function\n",
    "def cleanup_tutorial_files():\n",
    "    \"\"\"Clean up tutorial files and directories\"\"\"\n",
    "    \n",
    "    print(\"üßπ Cleaning up tutorial files...\")\n",
    "    \n",
    "    try:\n",
    "        # Count files before cleanup\n",
    "        total_files = 0\n",
    "        for directory in [\"input\", \"processed\", \"output\"]:\n",
    "            dir_path = base_dir / directory\n",
    "            if dir_path.exists():\n",
    "                files = list(dir_path.rglob('*'))\n",
    "                file_count = len([f for f in files if f.is_file()])\n",
    "                total_files += file_count\n",
    "                print(f\"  üìÅ {directory}: {file_count} files\")\n",
    "        \n",
    "        print(f\"\\nüìä Total files created during tutorial: {total_files}\")\n",
    "        \n",
    "        # Ask user if they want to keep the files\n",
    "        print(\"\\n‚ùì Keep tutorial files for further exploration?\")\n",
    "        print(\"   (Files are in the 'tutorial_data' directory)\")\n",
    "        \n",
    "        # For notebook environment, we'll keep the files by default\n",
    "        keep_files = True\n",
    "        \n",
    "        if keep_files:\n",
    "            print(\"‚úÖ Tutorial files preserved for your exploration!\")\n",
    "            print(f\"üìÅ Location: {base_dir.absolute()}\")\n",
    "        else:\n",
    "            # Remove tutorial directory\n",
    "            import shutil\n",
    "            shutil.rmtree(base_dir)\n",
    "            print(\"üóëÔ∏è Tutorial files cleaned up\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error during cleanup: {e}\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_tutorial_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Tutorial Summary\n",
    "\n",
    "Congratulations! You've completed the **File Reading** tutorial. Here's what you've mastered:\n",
    "\n",
    "### ‚úÖ **Core Skills Learned**\n",
    "\n",
    "1. **üìä CSV File Processing**\n",
    "   - Basic and advanced reading techniques\n",
    "   - Handling different encodings and formats\n",
    "   - Optimizing data types for performance\n",
    "   - Processing large files with chunking\n",
    "\n",
    "2. **üìÑ JSON File Processing**\n",
    "   - Reading flat and nested JSON structures\n",
    "   - Flattening complex nested data\n",
    "   - Error handling for malformed JSON\n",
    "   - Combining multiple JSON sources\n",
    "\n",
    "3. **ü§ñ File Processing Automation**\n",
    "   - Automated file discovery and monitoring\n",
    "   - Batch processing workflows\n",
    "   - File archiving and organization\n",
    "   - Processing status tracking\n",
    "\n",
    "4. **üîç File Validation**\n",
    "   - Structure and accessibility validation\n",
    "   - Content and schema validation\n",
    "   - Data quality assessment\n",
    "   - Error reporting and handling\n",
    "\n",
    "5. **‚ö° Performance Optimization**\n",
    "   - Memory-efficient data types\n",
    "   - Chunked processing for large files\n",
    "   - Column selection optimization\n",
    "   - Performance benchmarking\n",
    "\n",
    "### üõ†Ô∏è **Practical Tools Built**\n",
    "\n",
    "- **FileProcessor**: Automated file processing system\n",
    "- **FileValidator**: Comprehensive validation framework\n",
    "- **Performance Optimizer**: Techniques for handling large files\n",
    "- **Error Handler**: Robust error handling patterns\n",
    "\n",
    "### üìä **Real-World Applications**\n",
    "\n",
    "- **Daily Data Imports**: Process daily sales reports, inventory updates\n",
    "- **Log File Analysis**: Parse and analyze application logs\n",
    "- **Data Migration**: Move data between systems efficiently\n",
    "- **ETL Pipelines**: Extract data from various file sources\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "In the next tutorial, **\"03_calling_apis.ipynb\"**, you'll learn:\n",
    "\n",
    "- üåê **REST API Integration**: Making HTTP requests and handling responses\n",
    "- üîê **Authentication**: API keys, OAuth, and secure access\n",
    "- üîÑ **Rate Limiting**: Handling API limits and throttling\n",
    "- üìä **Data Pagination**: Processing large datasets from APIs\n",
    "- ‚ö†Ô∏è **Error Handling**: Dealing with network issues and API failures\n",
    "- üîÑ **Real-time Data**: Polling and webhook integration\n",
    "\n",
    "### üéØ **Practice Exercise**\n",
    "\n",
    "Before moving to the next tutorial, try this challenge:\n",
    "\n",
    "1. **Create a file monitoring system** that watches a directory for new files\n",
    "2. **Process different file formats** (CSV, JSON, maybe Excel)\n",
    "3. **Implement data validation** with custom business rules\n",
    "4. **Generate processing reports** showing success/failure rates\n",
    "5. **Optimize for performance** with large files\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- **üìñ Pandas Documentation**: [Reading Files](https://pandas.pydata.org/docs/user_guide/io.html)\n",
    "- **üêç Python JSON Module**: [JSON Processing](https://docs.python.org/3/library/json.html)\n",
    "- **‚ö° Performance Tips**: [Pandas Performance](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)\n",
    "- **üîç File Validation**: [Data Validation Patterns](https://github.com/great-expectations/great_expectations)\n",
    "\n",
    "---\n",
    "\n",
    "**Great job! You're now ready to handle any file-based data ingestion challenge! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}