{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÅ Reading Files - Your First Data Ingestion Step\n",
    "\n",
    "Welcome to the second tutorial in our **Data Ingestion Pipeline** series! In this hands-on notebook, you'll learn how to read different file formats and handle real-world file processing challenges.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ‚úÖ Read CSV and JSON files with Python\n",
    "- ‚úÖ Handle different file encodings and formats\n",
    "- ‚úÖ Validate file structure and content\n",
    "- ‚úÖ Process multiple files automatically\n",
    "- ‚úÖ Handle common file processing errors\n",
    "- ‚úÖ Build your first file ingestion function\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Imports\n",
    "\n",
    "Let's start by importing the libraries we'll need and setting up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for file processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")\n",
    "print(f\"üêç Pandas version: {pd.__version__}\")\n",
    "print(f\"üìä Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Creating Sample Data Files\n",
    "\n",
    "Before we learn to read files, let's create some sample data files to work with. This simulates the real-world scenario where you receive data files from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for our sample data\n",
    "data_dir = Path(\"sample_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "csv_dir = data_dir / \"csv\"\n",
    "json_dir = data_dir / \"json\"\n",
    "csv_dir.mkdir(exist_ok=True)\n",
    "json_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Created directories:\")\n",
    "print(f\"  - {csv_dir}\")\n",
    "print(f\"  - {json_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample CSV data (simulating store sales)\n",
    "store_sales_data = {\n",
    "    'order_id': ['ORD-2024-001', 'ORD-2024-002', 'ORD-2024-003', 'ORD-2024-004', 'ORD-2024-005'],\n",
    "    'customer_name': ['John Doe', 'Jane Smith', 'Bob Wilson', 'Alice Johnson', 'Charlie Brown'],\n",
    "    'product': ['iPhone 15', 'MacBook Pro', 'AirPods Pro', 'iPad Air', 'Apple Watch'],\n",
    "    'quantity': [1, 1, 2, 1, 1],\n",
    "    'price': [999.99, 1999.99, 249.99, 599.99, 399.99],\n",
    "    'order_date': ['2024-01-15', '2024-01-15', '2024-01-16', '2024-01-16', '2024-01-17'],\n",
    "    'store_location': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "}\n",
    "\n",
    "# Save as CSV file\n",
    "df_store = pd.DataFrame(store_sales_data)\n",
    "csv_file_path = csv_dir / \"store_sales_2024_01.csv\"\n",
    "df_store.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"üìä Created sample CSV file:\")\n",
    "print(f\"  File: {csv_file_path}\")\n",
    "print(f\"  Records: {len(df_store)}\")\n",
    "print(f\"  Columns: {list(df_store.columns)}\")\n",
    "\n",
    "# Display the data\n",
    "print(\"\\nüìã Sample CSV Data:\")\n",
    "display(df_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample JSON data (simulating mobile app orders)\n",
    "mobile_app_data = {\n",
    "    \"app_info\": {\n",
    "        \"version\": \"2.1.0\",\n",
    "        \"platform\": \"iOS\",\n",
    "        \"upload_timestamp\": \"2024-01-15T12:00:00Z\"\n",
    "    },\n",
    "    \"orders\": [\n",
    "        {\n",
    "            \"order_id\": \"APP-2024-001\",\n",
    "            \"customer_name\": \"Sarah Connor\",\n",
    "            \"customer_email\": \"sarah@example.com\",\n",
    "            \"product\": \"iPhone 15 Pro\",\n",
    "            \"quantity\": 1,\n",
    "            \"price\": 1199.99,\n",
    "            \"order_date\": \"2024-01-15\",\n",
    "            \"device_info\": {\n",
    "                \"model\": \"iPhone 14\",\n",
    "                \"os_version\": \"17.2\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"order_id\": \"APP-2024-002\",\n",
    "            \"customer_name\": \"Mike Johnson\",\n",
    "            \"customer_email\": \"mike@example.com\",\n",
    "            \"product\": \"MacBook Air\",\n",
    "            \"quantity\": 1,\n",
    "            \"price\": 1299.99,\n",
    "            \"order_date\": \"2024-01-15\",\n",
    "            \"device_info\": {\n",
    "                \"model\": \"iPhone 13\",\n",
    "                \"os_version\": \"17.1\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"order_id\": \"APP-2024-003\",\n",
    "            \"customer_name\": \"Emma Davis\",\n",
    "            \"customer_email\": \"emma@example.com\",\n",
    "            \"product\": \"AirPods Max\",\n",
    "            \"quantity\": 1,\n",
    "            \"price\": 549.99,\n",
    "            \"order_date\": \"2024-01-16\",\n",
    "            \"device_info\": {\n",
    "                \"model\": \"iPhone 15\",\n",
    "                \"os_version\": \"17.2\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"summary\": {\n",
    "        \"total_orders\": 3,\n",
    "        \"total_revenue\": 3049.97\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as JSON file\n",
    "json_file_path = json_dir / \"mobile_orders_2024_01.json\"\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(mobile_app_data, f, indent=2)\n",
    "\n",
    "print(\"üì± Created sample JSON file:\")\n",
    "print(f\"  File: {json_file_path}\")\n",
    "print(f\"  Orders: {mobile_app_data['summary']['total_orders']}\")\n",
    "print(f\"  Revenue: ${mobile_app_data['summary']['total_revenue']:,.2f}\")\n",
    "\n",
    "# Display the JSON structure\n",
    "print(\"\\nüìã Sample JSON Structure:\")\n",
    "print(json.dumps(mobile_app_data, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Reading CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) files are the most common format for data exchange. Let's learn how to read them properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CSV reading\n",
    "print(\"üìä Reading CSV File - Basic Method\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Method 1: Simple read\n",
    "df_basic = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(f\"‚úÖ Successfully read CSV file!\")\n",
    "print(f\"üìè Shape: {df_basic.shape} (rows, columns)\")\n",
    "print(f\"üìã Columns: {list(df_basic.columns)}\")\n",
    "print(f\"üî¢ Data types:\")\n",
    "for col, dtype in df_basic.dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "print(\"\\nüìã First 3 rows:\")\n",
    "display(df_basic.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CSV reading with options\n",
    "print(\"üìä Reading CSV File - Advanced Method\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Method 2: With specific options\n",
    "df_advanced = pd.read_csv(\n",
    "    csv_file_path,\n",
    "    encoding='utf-8',           # Specify encoding\n",
    "    parse_dates=['order_date'], # Parse dates automatically\n",
    "    dtype={                     # Specify data types\n",
    "        'order_id': 'string',\n",
    "        'customer_name': 'string',\n",
    "        'product': 'string',\n",
    "        'quantity': 'int64',\n",
    "        'price': 'float64',\n",
    "        'store_location': 'string'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Successfully read CSV with advanced options!\")\n",
    "print(f\"üî¢ Improved data types:\")\n",
    "for col, dtype in df_advanced.dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "print(\"\\nüìä Data Info:\")\n",
    "df_advanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to read CSV files safely\n",
    "def read_csv_safely(file_path, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Safely read a CSV file with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "        encoding (str): File encoding (default: utf-8)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, data_or_error_message, file_info)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return False, f\"File not found: {file_path}\", None\n",
    "        \n",
    "        # Get file info\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        file_modified = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "        \n",
    "        # Try different encodings if utf-8 fails\n",
    "        encodings_to_try = [encoding, 'utf-8', 'latin-1', 'cp1252']\n",
    "        \n",
    "        for enc in encodings_to_try:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=enc)\n",
    "                \n",
    "                file_info = {\n",
    "                    'file_path': str(file_path),\n",
    "                    'file_size_bytes': file_size,\n",
    "                    'file_size_mb': round(file_size / (1024*1024), 2),\n",
    "                    'modified_date': file_modified.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'encoding_used': enc,\n",
    "                    'rows': len(df),\n",
    "                    'columns': len(df.columns),\n",
    "                    'column_names': list(df.columns)\n",
    "                }\n",
    "                \n",
    "                return True, df, file_info\n",
    "                \n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        return False, \"Could not decode file with any supported encoding\", None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading CSV file: {str(e)}\", None\n",
    "\n",
    "# Test our safe CSV reader\n",
    "print(\"üõ°Ô∏è Testing Safe CSV Reader\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "success, data, info = read_csv_safely(csv_file_path)\n",
    "\n",
    "if success:\n",
    "    print(\"‚úÖ File read successfully!\")\n",
    "    print(f\"üìÅ File Info:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüìä Data Preview:\")\n",
    "    display(data.head(2))\n",
    "else:\n",
    "    print(f\"‚ùå Error: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì± Reading JSON Files\n",
    "\n",
    "JSON (JavaScript Object Notation) files are flexible and can contain nested data structures. Let's learn how to handle them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic JSON reading\n",
    "print(\"üì± Reading JSON File - Basic Method\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Method 1: Using json library\n",
    "with open(json_file_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Successfully read JSON file!\")\n",
    "print(f\"üìã Top-level keys: {list(json_data.keys())}\")\n",
    "print(f\"üìä Number of orders: {len(json_data['orders'])}\")\n",
    "print(f\"üí∞ Total revenue: ${json_data['summary']['total_revenue']:,.2f}\")\n",
    "\n",
    "print(\"\\nüìã First order:\")\n",
    "print(json.dumps(json_data['orders'][0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to DataFrame\n",
    "print(\"üìä Converting JSON to DataFrame\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Extract orders from JSON\n",
    "orders_list = json_data['orders']\n",
    "\n",
    "# Method 1: Simple conversion (flattens simple fields)\n",
    "df_json_simple = pd.DataFrame(orders_list)\n",
    "\n",
    "print(\"üìã Simple conversion (nested fields remain as objects):\")\n",
    "display(df_json_simple)\n",
    "\n",
    "print(f\"\\nüî¢ Data types:\")\n",
    "for col, dtype in df_json_simple.dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced JSON processing - Flatten nested data\n",
    "print(\"üìä Advanced JSON Processing - Flattening Nested Data\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def flatten_json_orders(json_data):\n",
    "    \"\"\"\n",
    "    Flatten JSON orders data into a clean DataFrame\n",
    "    \n",
    "    Args:\n",
    "        json_data (dict): JSON data with orders\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Flattened orders data\n",
    "    \"\"\"\n",
    "    flattened_orders = []\n",
    "    \n",
    "    for order in json_data['orders']:\n",
    "        # Create flattened order record\n",
    "        flat_order = {\n",
    "            'order_id': order['order_id'],\n",
    "            'customer_name': order['customer_name'],\n",
    "            'customer_email': order['customer_email'],\n",
    "            'product': order['product'],\n",
    "            'quantity': order['quantity'],\n",
    "            'price': order['price'],\n",
    "            'order_date': order['order_date'],\n",
    "            # Flatten device_info\n",
    "            'device_model': order['device_info']['model'],\n",
    "            'device_os_version': order['device_info']['os_version'],\n",
    "            # Add metadata from app_info\n",
    "            'app_version': json_data['app_info']['version'],\n",
    "            'app_platform': json_data['app_info']['platform'],\n",
    "            'upload_timestamp': json_data['app_info']['upload_timestamp']\n",
    "        }\n",
    "        \n",
    "        flattened_orders.append(flat_order)\n",
    "    \n",
    "    return pd.DataFrame(flattened_orders)\n",
    "\n",
    "# Flatten our JSON data\n",
    "df_json_flat = flatten_json_orders(json_data)\n",
    "\n",
    "print(\"‚úÖ Successfully flattened JSON data!\")\n",
    "print(f\"üìè Shape: {df_json_flat.shape}\")\n",
    "print(f\"üìã Columns: {list(df_json_flat.columns)}\")\n",
    "\n",
    "print(\"\\nüìä Flattened Data:\")\n",
    "display(df_json_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to read JSON files safely\n",
    "def read_json_safely(file_path):\n",
    "    \"\"\"\n",
    "    Safely read a JSON file with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, data_or_error_message, file_info)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return False, f\"File not found: {file_path}\", None\n",
    "        \n",
    "        # Get file info\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        file_modified = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "        \n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # Analyze JSON structure\n",
    "        def analyze_json_structure(data, prefix=\"\"):\n",
    "            structure = {}\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    full_key = f\"{prefix}.{key}\" if prefix else key\n",
    "                    if isinstance(value, (dict, list)):\n",
    "                        structure[full_key] = type(value).__name__\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            structure[f\"{full_key}_length\"] = len(value)\n",
    "                    else:\n",
    "                        structure[full_key] = type(value).__name__\n",
    "            return structure\n",
    "        \n",
    "        structure = analyze_json_structure(json_data)\n",
    "        \n",
    "        file_info = {\n",
    "            'file_path': str(file_path),\n",
    "            'file_size_bytes': file_size,\n",
    "            'file_size_mb': round(file_size / (1024*1024), 2),\n",
    "            'modified_date': file_modified.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'json_structure': structure,\n",
    "            'top_level_keys': list(json_data.keys()) if isinstance(json_data, dict) else 'Not a dict'\n",
    "        }\n",
    "        \n",
    "        return True, json_data, file_info\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return False, f\"Invalid JSON format: {str(e)}\", None\n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading JSON file: {str(e)}\", None\n",
    "\n",
    "# Test our safe JSON reader\n",
    "print(\"üõ°Ô∏è Testing Safe JSON Reader\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "success, data, info = read_json_safely(json_file_path)\n",
    "\n",
    "if success:\n",
    "    print(\"‚úÖ File read successfully!\")\n",
    "    print(f\"üìÅ File Info:\")\n",
    "    for key, value in info.items():\n",
    "        if key != 'json_structure':  # Skip detailed structure for now\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüìä JSON Structure:\")\n",
    "    for key, value in info['json_structure'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç File Validation and Quality Checks\n",
    "\n",
    "Before processing files, it's crucial to validate their structure and content. Let's learn how to build robust validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File validation function\n",
    "def validate_csv_file(df, required_columns=None, min_rows=1):\n",
    "    \"\"\"\n",
    "    Validate CSV file structure and content\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to validate\n",
    "        required_columns (list): List of required column names\n",
    "        min_rows (int): Minimum number of rows required\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation results\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'is_valid': True,\n",
    "        'errors': [],\n",
    "        'warnings': [],\n",
    "        'stats': {\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'missing_values': df.isnull().sum().sum(),\n",
    "            'duplicate_rows': df.duplicated().sum()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check if DataFrame is empty\n",
    "    if df.empty:\n",
    "        validation_results['is_valid'] = False\n",
    "        validation_results['errors'].append(\"File is empty\")\n",
    "        return validation_results\n",
    "    \n",
    "    # Check minimum rows\n",
    "    if len(df) < min_rows:\n",
    "        validation_results['is_valid'] = False\n",
    "        validation_results['errors'].append(f\"File has {len(df)} rows, minimum required: {min_rows}\")\n",
    "    \n",
    "    # Check required columns\n",
    "    if required_columns:\n",
    "        missing_columns = set(required_columns) - set(df.columns)\n",
    "        if missing_columns:\n",
    "            validation_results['is_valid'] = False\n",
    "            validation_results['errors'].append(f\"Missing required columns: {list(missing_columns)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_by_column = df.isnull().sum()\n",
    "    for col, missing_count in missing_by_column.items():\n",
    "        if missing_count > 0:\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            if missing_pct > 50:\n",
    "                validation_results['warnings'].append(f\"Column '{col}' has {missing_pct:.1f}% missing values\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if validation_results['stats']['duplicate_rows'] > 0:\n",
    "        validation_results['warnings'].append(f\"Found {validation_results['stats']['duplicate_rows']} duplicate rows\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Test validation on our CSV data\n",
    "print(\"üîç Validating CSV File\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "required_cols = ['order_id', 'customer_name', 'product', 'quantity', 'price']\n",
    "validation = validate_csv_file(df_advanced, required_columns=required_cols)\n",
    "\n",
    "print(f\"‚úÖ Validation Status: {'PASSED' if validation['is_valid'] else 'FAILED'}\")\n",
    "print(f\"üìä File Statistics:\")\n",
    "for key, value in validation['stats'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "if validation['errors']:\n",
    "    print(f\"\\n‚ùå Errors:\")\n",
    "    for error in validation['errors']:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "if validation['warnings']:\n",
    "    print(f\"\\n‚ö†Ô∏è Warnings:\")\n",
    "    for warning in validation['warnings']:\n",
    "        print(f\"  - {warning}\")\n",
    "\n",
    "if validation['is_valid'] and not validation['warnings']:\n",
    "    print(\"\\nüéâ File validation passed with no issues!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Processing Multiple Files\n",
    "\n",
    "In real-world scenarios, you'll often need to process multiple files at once. Let's learn how to handle batch file processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional sample files for batch processing\n",
    "print(\"üìÅ Creating Additional Sample Files\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create more CSV files\n",
    "additional_csv_data = [\n",
    "    {\n",
    "        'filename': 'store_sales_2024_02.csv',\n",
    "        'data': {\n",
    "            'order_id': ['ORD-2024-006', 'ORD-2024-007', 'ORD-2024-008'],\n",
    "            'customer_name': ['David Lee', 'Lisa Wang', 'Tom Brown'],\n",
    "            'product': ['iPad Pro', 'AirPods Max', 'Mac Studio'],\n",
    "            'quantity': [1, 1, 1],\n",
    "            'price': [1099.99, 549.99, 1999.99],\n",
    "            'order_date': ['2024-02-01', '2024-02-01', '2024-02-02'],\n",
    "            'store_location': ['Miami', 'Seattle', 'Boston']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'filename': 'store_sales_2024_03.csv',\n",
    "        'data': {\n",
    "            'order_id': ['ORD-2024-009', 'ORD-2024-010'],\n",
    "            'customer_name': ['Anna Garcia', 'Chris Wilson'],\n",
    "            'product': ['iPhone 15 Plus', 'MacBook Air'],\n",
    "            'quantity': [1, 1],\n",
    "            'price': [1099.99, 1199.99],\n",
    "            'order_date': ['2024-03-01', '2024-03-01'],\n",
    "            'store_location': ['Denver', 'Portland']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save additional CSV files\n",
    "for file_info in additional_csv_data:\n",
    "    df_temp = pd.DataFrame(file_info['data'])\n",
    "    file_path = csv_dir / file_info['filename']\n",
    "    df_temp.to_csv(file_path, index=False)\n",
    "    print(f\"üìÑ Created: {file_info['filename']} ({len(df_temp)} records)\")\n",
    "\n",
    "# Create more JSON files\n",
    "additional_json_data = {\n",
    "    \"app_info\": {\n",
    "        \"version\": \"2.2.0\",\n",
    "        \"platform\": \"Android\",\n",
    "        \"upload_timestamp\": \"2024-02-01T14:30:00Z\"\n",
    "    },\n",
    "    \"orders\": [\n",
    "        {\n",
    "            \"order_id\": \"APP-2024-004\",\n",
    "            \"customer_name\": \"Kevin Park\",\n",
    "            \"customer_email\": \"kevin@example.com\",\n",
    "            \"product\": \"Samsung Galaxy S24\",\n",
    "            \"quantity\": 1,\n",
    "            \"price\": 899.99,\n",
    "            \"order_date\": \"2024-02-01\",\n",
    "            \"device_info\": {\n",
    "                \"model\": \"Galaxy S23\",\n",
    "                \"os_version\": \"14\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"summary\": {\n",
    "        \"total_orders\": 1,\n",
    "        \"total_revenue\": 899.99\n",
    "    }\n",
    "}\n",
    "\n",
    "json_file_path_2 = json_dir / \"mobile_orders_2024_02.json\"\n",
    "with open(json_file_path_2, 'w') as f:\n",
    "    json.dump(additional_json_data, f, indent=2)\n",
    "\n",
    "print(f\"üì± Created: mobile_orders_2024_02.json (1 record)\")\n",
    "\n",
    "print(f\"\\nüìä Total files created:\")\n",
    "csv_files = list(csv_dir.glob('*.csv'))\n",
    "json_files = list(json_dir.glob('*.json'))\n",
    "print(f\"  CSV files: {len(csv_files)}\")\n",
    "print(f\"  JSON files: {len(json_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch file processing function\n",
    "def process_multiple_files(directory, file_pattern, file_type='csv'):\n",
    "    \"\"\"\n",
    "    Process multiple files from a directory\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path\n",
    "        file_pattern (str): File pattern (e.g., '*.csv')\n",
    "        file_type (str): Type of files ('csv' or 'json')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Processing results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'successful_files': [],\n",
    "        'failed_files': [],\n",
    "        'total_records': 0,\n",
    "        'combined_data': None,\n",
    "        'processing_summary': []\n",
    "    }\n",
    "    \n",
    "    # Find all matching files\n",
    "    file_paths = glob.glob(os.path.join(directory, file_pattern))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"‚ö†Ô∏è No files found matching pattern: {file_pattern}\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"üìÅ Found {len(file_paths)} files to process\")\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"\\nüìÑ Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            if file_type == 'csv':\n",
    "                success, data, info = read_csv_safely(file_path)\n",
    "            elif file_type == 'json':\n",
    "                success, data, info = read_json_safely(file_path)\n",
    "                if success:\n",
    "                    # Convert JSON to DataFrame\n",
    "                    data = flatten_json_orders(data)\n",
    "            else:\n",
    "                success = False\n",
    "                data = f\"Unsupported file type: {file_type}\"\n",
    "                info = None\n",
    "            \n",
    "            if success:\n",
    "                # Add source file information\n",
    "                data['source_file'] = filename\n",
    "                data['processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "                all_dataframes.append(data)\n",
    "                results['successful_files'].append(filename)\n",
    "                results['total_records'] += len(data)\n",
    "                \n",
    "                summary = {\n",
    "                    'filename': filename,\n",
    "                    'status': 'success',\n",
    "                    'records': len(data),\n",
    "                    'columns': len(data.columns),\n",
    "                    'file_size_mb': info['file_size_mb'] if info else 0\n",
    "                }\n",
    "                results['processing_summary'].append(summary)\n",
    "                \n",
    "                print(f\"  ‚úÖ Success: {len(data)} records\")\n",
    "            else:\n",
    "                results['failed_files'].append({'filename': filename, 'error': data})\n",
    "                summary = {\n",
    "                    'filename': filename,\n",
    "                    'status': 'failed',\n",
    "                    'error': data\n",
    "                }\n",
    "                results['processing_summary'].append(summary)\n",
    "                print(f\"  ‚ùå Failed: {data}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error: {str(e)}\"\n",
    "            results['failed_files'].append({'filename': filename, 'error': error_msg})\n",
    "            print(f\"  ‚ùå Error: {error_msg}\")\n",
    "    \n",
    "    # Combine all successful DataFrames\n",
    "    if all_dataframes:\n",
    "        try:\n",
    "            results['combined_data'] = pd.concat(all_dataframes, ignore_index=True)\n",
    "            print(f\"\\nüîó Combined {len(all_dataframes)} files into single dataset\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Warning: Could not combine files: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process all CSV files\n",
    "print(\"üìä Processing All CSV Files\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "csv_results = process_multiple_files(csv_dir, '*.csv', 'csv')\n",
    "\n",
    "print(f\"\\nüìà CSV Processing Summary:\")\n",
    "print(f\"  Successful files: {len(csv_results['successful_files'])}\")\n",
    "print(f\"  Failed files: {len(csv_results['failed_files'])}\")\n",
    "print(f\"  Total records: {csv_results['total_records']:,}\")\n",
    "\n",
    "if csv_results['combined_data'] is not None:\n",
    "    print(f\"  Combined data shape: {csv_results['combined_data'].shape}\")\n",
    "    print(f\"\\nüìã Sample of combined data:\")\n",
    "    display(csv_results['combined_data'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all JSON files\n",
    "print(\"üì± Processing All JSON Files\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "json_results = process_multiple_files(json_dir, '*.json', 'json')\n",
    "\n",
    "print(f\"\\nüìà JSON Processing Summary:\")\n",
    "print(f\"  Successful files: {len(json_results['successful_files'])}\")\n",
    "print(f\"  Failed files: {len(json_results['failed_files'])}\")\n",
    "print(f\"  Total records: {json_results['total_records']:,}\")\n",
    "\n",
    "if json_results['combined_data'] is not None:\n",
    "    print(f\"  Combined data shape: {json_results['combined_data'].shape}\")\n",
    "    print(f\"\\nüìã Sample of combined JSON data:\")\n",
    "    display(json_results['combined_data'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Combining Data from Multiple Sources\n",
    "\n",
    "Now let's combine data from both CSV and JSON sources into a unified dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CSV and JSON data\n",
    "print(\"üîó Combining Data from Multiple Sources\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def combine_multi_source_data(csv_data, json_data):\n",
    "    \"\"\"\n",
    "    Combine data from CSV and JSON sources\n",
    "    \n",
    "    Args:\n",
    "        csv_data (pd.DataFrame): Data from CSV files\n",
    "        json_data (pd.DataFrame): Data from JSON files\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined and standardized data\n",
    "    \"\"\"\n",
    "    combined_datasets = []\n",
    "    \n",
    "    # Process CSV data\n",
    "    if csv_data is not None and not csv_data.empty:\n",
    "        csv_standardized = csv_data.copy()\n",
    "        csv_standardized['source_type'] = 'csv'\n",
    "        csv_standardized['customer_email'] = None  # CSV doesn't have email\n",
    "        \n",
    "        # Standardize column order\n",
    "        csv_columns = ['order_id', 'customer_name', 'customer_email', 'product', \n",
    "                      'quantity', 'price', 'order_date', 'source_file', 'source_type']\n",
    "        \n",
    "        # Add missing columns with default values\n",
    "        for col in csv_columns:\n",
    "            if col not in csv_standardized.columns:\n",
    "                csv_standardized[col] = None\n",
    "        \n",
    "        csv_standardized = csv_standardized[csv_columns + ['store_location', 'processed_at']]\n",
    "        combined_datasets.append(csv_standardized)\n",
    "        print(f\"üìä CSV data: {len(csv_standardized)} records\")\n",
    "    \n",
    "    # Process JSON data\n",
    "    if json_data is not None and not json_data.empty:\n",
    "        json_standardized = json_data.copy()\n",
    "        json_standardized['source_type'] = 'json'\n",
    "        json_standardized['store_location'] = None  # JSON doesn't have store location\n",
    "        \n",
    "        # Standardize column order\n",
    "        json_columns = ['order_id', 'customer_name', 'customer_email', 'product', \n",
    "                       'quantity', 'price', 'order_date', 'source_file', 'source_type']\n",
    "        \n",
    "        # Add missing columns with default values\n",
    "        for col in json_columns:\n",
    "            if col not in json_standardized.columns:\n",
    "                json_standardized[col] = None\n",
    "        \n",
    "        json_standardized = json_standardized[json_columns + ['store_location', 'processed_at']]\n",
    "        combined_datasets.append(json_standardized)\n",
    "        print(f\"üì± JSON data: {len(json_standardized)} records\")\n",
    "    \n",
    "    # Combine all datasets\n",
    "    if combined_datasets:\n",
    "        final_data = pd.concat(combined_datasets, ignore_index=True)\n",
    "        \n",
    "        # Add final processing metadata\n",
    "        final_data['combined_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        final_data['record_id'] = range(1, len(final_data) + 1)\n",
    "        \n",
    "        return final_data\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Combine the data\n",
    "combined_data = combine_multi_source_data(\n",
    "    csv_results['combined_data'], \n",
    "    json_results['combined_data']\n",
    ")\n",
    "\n",
    "if not combined_data.empty:\n",
    "    print(f\"\\nüéâ Successfully combined data!\")\n",
    "    print(f\"üìè Final dataset shape: {combined_data.shape}\")\n",
    "    print(f\"üìã Columns: {list(combined_data.columns)}\")\n",
    "    \n",
    "    # Show source breakdown\n",
    "    source_breakdown = combined_data['source_type'].value_counts()\n",
    "    print(f\"\\nüìä Source Breakdown:\")\n",
    "    for source, count in source_breakdown.items():\n",
    "        print(f\"  {source.upper()}: {count} records\")\n",
    "    \n",
    "    print(f\"\\nüìã Combined Dataset Sample:\")\n",
    "    display(combined_data[['record_id', 'order_id', 'customer_name', 'product', 'price', 'source_type']].head(10))\n",
    "else:\n",
    "    print(\"‚ùå No data to combine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Analysis and Visualization\n",
    "\n",
    "Now that we have our combined dataset, let's do some basic analysis to understand our data better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data analysis\n",
    "if not combined_data.empty:\n",
    "    print(\"üìä Data Analysis Summary\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"üìà Dataset Overview:\")\n",
    "    print(f\"  Total Orders: {len(combined_data):,}\")\n",
    "    print(f\"  Unique Customers: {combined_data['customer_name'].nunique()}\")\n",
    "    print(f\"  Unique Products: {combined_data['product'].nunique()}\")\n",
    "    print(f\"  Date Range: {combined_data['order_date'].min()} to {combined_data['order_date'].max()}\")\n",
    "    \n",
    "    # Revenue analysis\n",
    "    total_revenue = combined_data['price'].sum()\n",
    "    avg_order_value = combined_data['price'].mean()\n",
    "    print(f\"\\nüí∞ Revenue Analysis:\")\n",
    "    print(f\"  Total Revenue: ${total_revenue:,.2f}\")\n",
    "    print(f\"  Average Order Value: ${avg_order_value:.2f}\")\n",
    "    print(f\"  Highest Order: ${combined_data['price'].max():.2f}\")\n",
    "    print(f\"  Lowest Order: ${combined_data['price'].min():.2f}\")\n",
    "    \n",
    "    # Product analysis\n",
    "    print(f\"\\nüì± Top Products:\")\n",
    "    top_products = combined_data['product'].value_counts().head(5)\n",
    "    for product, count in top_products.items():\n",
    "        print(f\"  {product}: {count} orders\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Revenue by source type\n",
    "    revenue_by_source = combined_data.groupby('source_type')['price'].sum()\n",
    "    ax1.pie(revenue_by_source.values, labels=revenue_by_source.index, autopct='%1.1f%%')\n",
    "    ax1.set_title('Revenue by Source Type')\n",
    "    \n",
    "    # 2. Order count by product\n",
    "    top_products.plot(kind='bar', ax=ax2)\n",
    "    ax2.set_title('Top Products by Order Count')\n",
    "    ax2.set_xlabel('Product')\n",
    "    ax2.set_ylabel('Number of Orders')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Price distribution\n",
    "    ax3.hist(combined_data['price'], bins=10, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_title('Price Distribution')\n",
    "    ax3.set_xlabel('Price ($)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    \n",
    "    # 4. Orders over time (if we have date data)\n",
    "    combined_data['order_date'] = pd.to_datetime(combined_data['order_date'])\n",
    "    daily_orders = combined_data.groupby('order_date').size()\n",
    "    daily_orders.plot(kind='line', marker='o', ax=ax4)\n",
    "    ax4.set_title('Orders Over Time')\n",
    "    ax4.set_xlabel('Date')\n",
    "    ax4.set_ylabel('Number of Orders')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Error Handling and Best Practices\n",
    "\n",
    "Let's learn how to handle common file processing errors gracefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create problematic files to test error handling\n",
    "print(\"üß™ Creating Test Files with Issues\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create an empty CSV file\n",
    "empty_csv = csv_dir / \"empty_file.csv\"\n",
    "empty_csv.touch()\n",
    "print(\"üìÑ Created empty CSV file\")\n",
    "\n",
    "# Create a CSV with missing columns\n",
    "bad_csv_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['A', 'B', 'C']\n",
    "})\n",
    "bad_csv = csv_dir / \"bad_structure.csv\"\n",
    "bad_csv_data.to_csv(bad_csv, index=False)\n",
    "print(\"üìÑ Created CSV with wrong structure\")\n",
    "\n",
    "# Create an invalid JSON file\n",
    "invalid_json = json_dir / \"invalid.json\"\n",
    "with open(invalid_json, 'w') as f:\n",
    "    f.write('{\"invalid\": json content}')\n",
    "print(\"üìÑ Created invalid JSON file\")\n",
    "\n",
    "print(\"\\nüß™ Testing Error Handling\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Test files with issues\n",
    "test_files = [\n",
    "    (empty_csv, 'csv'),\n",
    "    (bad_csv, 'csv'),\n",
    "    (invalid_json, 'json'),\n",
    "    ('nonexistent_file.csv', 'csv')\n",
    "]\n",
    "\n",
    "for file_path, file_type in test_files:\n",
    "    print(f\"\\nüìÑ Testing: {os.path.basename(str(file_path))}\")\n",
    "    \n",
    "    if file_type == 'csv':\n",
    "        success, data, info = read_csv_safely(file_path)\n",
    "    else:\n",
    "        success, data, info = read_json_safely(file_path)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"  ‚úÖ Success: {len(data)} records\")\n",
    "        \n",
    "        # Validate the data\n",
    "        if file_type == 'csv':\n",
    "            validation = validate_csv_file(data, required_columns=['order_id', 'customer_name'])\n",
    "            if not validation['is_valid']:\n",
    "                print(f\"  ‚ö†Ô∏è Validation failed: {validation['errors']}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Failed: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Building Your First File Ingestion System\n",
    "\n",
    "Let's put everything together and build a complete file ingestion system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete file ingestion system\n",
    "class SimpleFileIngestion:\n",
    "    \"\"\"\n",
    "    A simple but robust file ingestion system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_directory, output_directory=None):\n",
    "        self.input_directory = Path(input_directory)\n",
    "        self.output_directory = Path(output_directory) if output_directory else Path(\"processed_data\")\n",
    "        self.output_directory.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Processing statistics\n",
    "        self.stats = {\n",
    "            'files_processed': 0,\n",
    "            'files_failed': 0,\n",
    "            'total_records': 0,\n",
    "            'processing_errors': []\n",
    "        }\n",
    "    \n",
    "    def process_all_files(self):\n",
    "        \"\"\"\n",
    "        Process all CSV and JSON files in the input directory\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Combined processed data\n",
    "        \"\"\"\n",
    "        print(f\"üöÄ Starting file ingestion from: {self.input_directory}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        # Process CSV files\n",
    "        csv_files = list(self.input_directory.glob('**/*.csv'))\n",
    "        print(f\"üìä Found {len(csv_files)} CSV files\")\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            data = self._process_csv_file(csv_file)\n",
    "            if data is not None:\n",
    "                all_data.append(data)\n",
    "        \n",
    "        # Process JSON files\n",
    "        json_files = list(self.input_directory.glob('**/*.json'))\n",
    "        print(f\"üì± Found {len(json_files)} JSON files\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            data = self._process_json_file(json_file)\n",
    "            if data is not None:\n",
    "                all_data.append(data)\n",
    "        \n",
    "        # Combine all data\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data, ignore_index=True)\n",
    "            self.stats['total_records'] = len(combined_data)\n",
    "            \n",
    "            # Save combined data\n",
    "            output_file = self.output_directory / f\"combined_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            combined_data.to_csv(output_file, index=False)\n",
    "            \n",
    "            print(f\"\\nüíæ Saved combined data to: {output_file}\")\n",
    "            self._print_summary()\n",
    "            \n",
    "            return combined_data\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No data was successfully processed\")\n",
    "            self._print_summary()\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _process_csv_file(self, file_path):\n",
    "        \"\"\"Process a single CSV file\"\"\"\n",
    "        print(f\"\\nüìÑ Processing CSV: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            success, data, info = read_csv_safely(file_path)\n",
    "            \n",
    "            if success:\n",
    "                # Validate the data\n",
    "                validation = validate_csv_file(data, min_rows=1)\n",
    "                \n",
    "                if validation['is_valid']:\n",
    "                    # Add metadata\n",
    "                    data['source_file'] = file_path.name\n",
    "                    data['source_type'] = 'csv'\n",
    "                    data['processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    \n",
    "                    self.stats['files_processed'] += 1\n",
    "                    print(f\"  ‚úÖ Success: {len(data)} records\")\n",
    "                    return data\n",
    "                else:\n",
    "                    self.stats['files_failed'] += 1\n",
    "                    error_msg = f\"Validation failed: {validation['errors']}\"\n",
    "                    self.stats['processing_errors'].append({'file': file_path.name, 'error': error_msg})\n",
    "                    print(f\"  ‚ùå Validation failed: {validation['errors']}\")\n",
    "            else:\n",
    "                self.stats['files_failed'] += 1\n",
    "                self.stats['processing_errors'].append({'file': file_path.name, 'error': data})\n",
    "                print(f\"  ‚ùå Failed: {data}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.stats['files_failed'] += 1\n",
    "            error_msg = f\"Unexpected error: {str(e)}\"\n",
    "            self.stats['processing_errors'].append({'file': file_path.name, 'error': error_msg})\n",
    "            print(f\"  ‚ùå Error: {error_msg}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _process_json_file(self, file_path):\n",
    "        \"\"\"Process a single JSON file\"\"\"\n",
    "        print(f\"\\nüìÑ Processing JSON: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            success, data, info = read_json_safely(file_path)\n",
    "            \n",
    "            if success:\n",
    "                # Convert to DataFrame\n",
    "                if 'orders' in data:\n",
    "                    df = flatten_json_orders(data)\n",
    "                    \n",
    "                    # Add metadata\n",
    "                    df['source_file'] = file_path.name\n",
    "                    df['source_type'] = 'json'\n",
    "                    df['processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    \n",
    "                    self.stats['files_processed'] += 1\n",
    "                    print(f\"  ‚úÖ Success: {len(df)} records\")\n",
    "                    return df\n",
    "                else:\n",
    "                    self.stats['files_failed'] += 1\n",
    "                    error_msg = \"No 'orders' key found in JSON\"\n",
    "                    self.stats['processing_errors'].append({'file': file_path.name, 'error': error_msg})\n",
    "                    print(f\"  ‚ùå {error_msg}\")\n",
    "            else:\n",
    "                self.stats['files_failed'] += 1\n",
    "                self.stats['processing_errors'].append({'file': file_path.name, 'error': data})\n",
    "                print(f\"  ‚ùå Failed: {data}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.stats['files_failed'] += 1\n",
    "            error_msg = f\"Unexpected error: {str(e)}\"\n",
    "            self.stats['processing_errors'].append({'file': file_path.name, 'error': error_msg})\n",
    "            print(f\"  ‚ùå Error: {error_msg}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _print_summary(self):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(f\"\\nüìä Processing Summary\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f\"‚úÖ Files Processed: {self.stats['files_processed']}\")\n",
    "        print(f\"‚ùå Files Failed: {self.stats['files_failed']}\")\n",
    "        print(f\"üìà Total Records: {self.stats['total_records']:,}\")\n",
    "        \n",
    "        if self.stats['processing_errors']:\n",
    "            print(f\"\\n‚ö†Ô∏è Errors:\")\n",
    "            for error in self.stats['processing_errors']:\n",
    "                print(f\"  {error['file']}: {error['error']}\")\n",
    "\n",
    "# Test our complete ingestion system\n",
    "print(\"üéØ Testing Complete File Ingestion System\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize the ingestion system\n",
    "ingestion_system = SimpleFileIngestion(data_dir)\n",
    "\n",
    "# Process all files\n",
    "final_data = ingestion_system.process_all_files()\n",
    "\n",
    "if not final_data.empty:\n",
    "    print(f\"\\nüéâ Ingestion completed successfully!\")\n",
    "    print(f\"üìè Final dataset: {final_data.shape}\")\n",
    "    print(f\"\\nüìã Sample of final data:\")\n",
    "    display(final_data[['source_file', 'order_id', 'customer_name', 'product', 'price', 'source_type']].head())\n",
    "else:\n",
    "    print(f\"\\n‚ùå No data was processed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "Congratulations! You've completed the file reading tutorial. Here's what you've learned:\n",
    "\n",
    "### ‚úÖ **Core Skills Mastered**\n",
    "- **üìä CSV Reading**: Basic and advanced CSV processing with pandas\n",
    "- **üì± JSON Processing**: Handling nested JSON structures and flattening data\n",
    "- **üîç File Validation**: Checking file structure and data quality\n",
    "- **üìÅ Batch Processing**: Processing multiple files automatically\n",
    "- **üõ°Ô∏è Error Handling**: Graceful handling of common file issues\n",
    "- **üîó Data Combination**: Merging data from multiple sources\n",
    "\n",
    "### ‚úÖ **Best Practices Learned**\n",
    "- Always validate files before processing\n",
    "- Handle different encodings gracefully\n",
    "- Add metadata to track data lineage\n",
    "- Implement comprehensive error handling\n",
    "- Combine data sources systematically\n",
    "- Log processing results for monitoring\n",
    "\n",
    "### ‚úÖ **Real-World Applications**\n",
    "- **Business Reports**: Processing daily/weekly sales files\n",
    "- **Data Integration**: Combining data from multiple systems\n",
    "- **ETL Pipelines**: First stage of data transformation\n",
    "- **Data Quality**: Validating incoming data files\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "In the next tutorial, **\"03_calling_apis.ipynb\"**, you'll learn:\n",
    "- üåê How to call REST APIs to fetch data\n",
    "- üîê Handling API authentication and rate limiting\n",
    "- üîÑ Processing API responses and pagination\n",
    "- ‚ö° Real-time data ingestion from APIs\n",
    "- üõ°Ô∏è Error handling for network issues\n",
    "\n",
    "### üéØ **Practice Exercise**\n",
    "\n",
    "Before moving to the next tutorial, try this exercise:\n",
    "\n",
    "1. **Create your own sample data** for a different business (restaurant, library, etc.)\n",
    "2. **Save it in both CSV and JSON formats** with different structures\n",
    "3. **Use the ingestion system** we built to process your files\n",
    "4. **Add custom validation rules** specific to your business domain\n",
    "5. **Create visualizations** to analyze your processed data\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup\n",
    "\n",
    "Let's clean up the sample files we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up sample files\n",
    "import shutil\n",
    "\n",
    "cleanup = input(\"Do you want to clean up the sample files? (y/n): \").lower().strip()\n",
    "\n",
    "if cleanup == 'y':\n",
    "    try:\n",
    "        shutil.rmtree(data_dir)\n",
    "        print(f\"üßπ Cleaned up sample data directory: {data_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not clean up: {e}\")\n",
    "else:\n",
    "    print(f\"üìÅ Sample files kept in: {data_dir}\")\n",
    "    print(f\"   You can explore them or use them for practice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Great job completing this tutorial! üéâ**\n",
    "\n",
    "You now have solid foundations in file-based data ingestion. In the next tutorial, we'll expand your skills to include API-based data collection.\n",
    "\n",
    "**Happy Learning! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}