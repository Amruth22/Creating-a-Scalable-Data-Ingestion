{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“– Understanding Data Ingestion\n",
    "\n",
    "Welcome to the first tutorial in our **Data Ingestion Pipeline** series! In this notebook, you'll learn the fundamentals of data ingestion and why it's crucial for modern businesses.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- âœ… Understand what data ingestion is and why it matters\n",
    "- âœ… Learn about different types of data sources\n",
    "- âœ… Explore real-world data ingestion challenges\n",
    "- âœ… Understand the components of a data pipeline\n",
    "- âœ… See practical examples of data ingestion scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ What is Data Ingestion?\n",
    "\n",
    "**Data Ingestion** is the process of collecting, importing, and processing data from various sources into a storage system where it can be accessed, analyzed, and used by applications.\n",
    "\n",
    "Think of it as a **digital conveyor belt** that:\n",
    "1. ðŸ“¥ **Collects** data from multiple sources\n",
    "2. ðŸ” **Validates** the data quality\n",
    "3. ðŸ§¹ **Cleans** and transforms the data\n",
    "4. ðŸ’¾ **Stores** it in a usable format\n",
    "5. ðŸ“Š **Makes** it available for analysis\n",
    "\n",
    "### ðŸª Real-World Example: E-commerce Store\n",
    "\n",
    "Imagine you run **\"TechStore\"** - an electronics retailer. Every day, you receive orders from:\n",
    "- ðŸŒ Your website\n",
    "- ðŸ“± Mobile app\n",
    "- ðŸ¬ Physical stores\n",
    "- ðŸ“ž Phone orders\n",
    "- ðŸ¤ Partner retailers\n",
    "\n",
    "Without data ingestion, you'd have:\n",
    "- âŒ Data scattered across different systems\n",
    "- âŒ Inconsistent formats\n",
    "- âŒ Manual data entry errors\n",
    "- âŒ Delayed reporting\n",
    "- âŒ Poor decision making\n",
    "\n",
    "With data ingestion, you get:\n",
    "- âœ… Centralized data storage\n",
    "- âœ… Consistent data formats\n",
    "- âœ… Automated quality checks\n",
    "- âœ… Real-time insights\n",
    "- âœ… Better business decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with some basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“¦ Libraries imported successfully!\")\n",
    "print(f\"ðŸ Python version: {pd.__version__}\")\n",
    "print(f\"ðŸ“Š Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Types of Data Sources\n",
    "\n",
    "Data can come from many different sources. Let's explore the most common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of different data sources\n",
    "data_sources = {\n",
    "    'Source Type': ['Files (CSV/JSON)', 'APIs', 'Databases', 'Streams', 'Web Scraping', 'IoT Sensors'],\n",
    "    'Frequency': ['Daily/Hourly', 'Real-time', 'Batch', 'Continuous', 'On-demand', 'Continuous'],\n",
    "    'Volume': ['Medium', 'High', 'High', 'Very High', 'Low', 'High'],\n",
    "    'Complexity': ['Low', 'Medium', 'Medium', 'High', 'High', 'Medium'],\n",
    "    'Use Cases': [\n",
    "        'Reports, Exports',\n",
    "        'Live Data, Integrations', \n",
    "        'Business Systems',\n",
    "        'Real-time Analytics',\n",
    "        'Market Research',\n",
    "        'Monitoring, Telemetry'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_sources = pd.DataFrame(data_sources)\n",
    "print(\"ðŸ“‹ Common Data Sources:\")\n",
    "print(\"=\" * 50)\n",
    "display(df_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ File-Based Sources\n",
    "\n",
    "**Most Common Types:**\n",
    "- **CSV Files** - Comma-separated values, easy to read\n",
    "- **JSON Files** - JavaScript Object Notation, flexible structure\n",
    "- **Excel Files** - Spreadsheets with multiple sheets\n",
    "- **XML Files** - Structured markup language\n",
    "- **Log Files** - Application and system logs\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… Simple to understand and process\n",
    "- âœ… No network dependencies\n",
    "- âœ… Can be processed offline\n",
    "- âœ… Easy to backup and archive\n",
    "\n",
    "**Challenges:**\n",
    "- âŒ Manual file transfers\n",
    "- âŒ File format inconsistencies\n",
    "- âŒ Large file sizes\n",
    "- âŒ Timing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating sample data that might come from different sources\n",
    "\n",
    "# 1. CSV-style data (from store POS system)\n",
    "csv_data = {\n",
    "    'order_id': ['ORD-001', 'ORD-002', 'ORD-003'],\n",
    "    'customer_name': ['John Doe', 'Jane Smith', 'Bob Wilson'],\n",
    "    'product': ['iPhone 15', 'MacBook Pro', 'AirPods Pro'],\n",
    "    'quantity': [1, 1, 2],\n",
    "    'price': [999.99, 1999.99, 249.99],\n",
    "    'store_location': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "# 2. JSON-style data (from mobile app)\n",
    "json_data = {\n",
    "    'app_version': '2.1.0',\n",
    "    'upload_time': '2024-01-15T12:00:00Z',\n",
    "    'orders': [\n",
    "        {\n",
    "            'order_id': 'APP-001',\n",
    "            'customer_name': 'Alice Johnson',\n",
    "            'product': 'iPad Air',\n",
    "            'quantity': 1,\n",
    "            'price': 599.99,\n",
    "            'device_type': 'iOS'\n",
    "        },\n",
    "        {\n",
    "            'order_id': 'APP-002',\n",
    "            'customer_name': 'Charlie Brown',\n",
    "            'product': 'Apple Watch',\n",
    "            'quantity': 1,\n",
    "            'price': 399.99,\n",
    "            'device_type': 'iOS'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š Sample CSV Data (Store POS):\")\n",
    "df_csv = pd.DataFrame(csv_data)\n",
    "display(df_csv)\n",
    "\n",
    "print(\"\\nðŸ“± Sample JSON Data (Mobile App):\")\n",
    "print(json.dumps(json_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ API-Based Sources\n",
    "\n",
    "**Application Programming Interfaces (APIs)** allow systems to communicate and exchange data in real-time.\n",
    "\n",
    "**Common API Types:**\n",
    "- **REST APIs** - Most common, uses HTTP methods\n",
    "- **GraphQL APIs** - Flexible query language\n",
    "- **WebSocket APIs** - Real-time bidirectional communication\n",
    "- **Webhook APIs** - Event-driven data push\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… Real-time data access\n",
    "- âœ… Standardized formats\n",
    "- âœ… Automatic updates\n",
    "- âœ… Rich metadata\n",
    "\n",
    "**Challenges:**\n",
    "- âŒ Network dependencies\n",
    "- âŒ Rate limiting\n",
    "- âŒ Authentication complexity\n",
    "- âŒ API changes and versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulating API response data\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Simulate an API response (we'll use a real API in later notebooks)\n",
    "api_response = {\n",
    "    \"status\": \"success\",\n",
    "    \"timestamp\": \"2024-01-15T14:30:00Z\",\n",
    "    \"data\": {\n",
    "        \"orders\": [\n",
    "            {\n",
    "                \"id\": \"WEB-001\",\n",
    "                \"customer\": {\n",
    "                    \"name\": \"Sarah Connor\",\n",
    "                    \"email\": \"sarah@example.com\",\n",
    "                    \"tier\": \"premium\"\n",
    "                },\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"product\": \"MacBook Pro\",\n",
    "                        \"quantity\": 1,\n",
    "                        \"price\": 1999.99\n",
    "                    }\n",
    "                ],\n",
    "                \"total\": 1999.99,\n",
    "                \"created_at\": \"2024-01-15T14:25:00Z\"\n",
    "            }\n",
    "        ],\n",
    "        \"pagination\": {\n",
    "            \"page\": 1,\n",
    "            \"per_page\": 10,\n",
    "            \"total\": 1,\n",
    "            \"has_more\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ðŸŒ Sample API Response:\")\n",
    "print(json.dumps(api_response, indent=2))\n",
    "\n",
    "# Extract order data from API response\n",
    "orders = api_response['data']['orders']\n",
    "print(f\"\\nðŸ“Š Extracted {len(orders)} orders from API response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Data Pipeline Components\n",
    "\n",
    "A complete data ingestion pipeline consists of several key components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data pipeline components\n",
    "pipeline_components = {\n",
    "    'Stage': ['1. Ingestion', '2. Validation', '3. Transformation', '4. Storage', '5. Monitoring'],\n",
    "    'Purpose': [\n",
    "        'Collect data from sources',\n",
    "        'Check data quality',\n",
    "        'Clean and enrich data', \n",
    "        'Save processed data',\n",
    "        'Track pipeline health'\n",
    "    ],\n",
    "    'Key Activities': [\n",
    "        'Read files, Call APIs, Query DBs',\n",
    "        'Schema validation, Business rules',\n",
    "        'Cleaning, Standardization, Enrichment',\n",
    "        'Database writes, File exports',\n",
    "        'Logging, Metrics, Alerts'\n",
    "    ],\n",
    "    'Tools/Technologies': [\n",
    "        'Pandas, Requests, SQL',\n",
    "        'JSON Schema, Custom validators',\n",
    "        'Pandas, NumPy, Custom logic',\n",
    "        'SQLite, PostgreSQL, Files',\n",
    "        'Logging, Prometheus, Grafana'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_pipeline = pd.DataFrame(pipeline_components)\n",
    "print(\"ðŸ”„ Data Pipeline Components:\")\n",
    "print(\"=\" * 80)\n",
    "display(df_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¥ Stage 1: Data Ingestion\n",
    "\n",
    "**Purpose:** Collect data from various sources\n",
    "\n",
    "**Key Activities:**\n",
    "- ðŸ“ Read files (CSV, JSON, Excel)\n",
    "- ðŸŒ Call REST APIs\n",
    "- ðŸ—„ï¸ Query databases\n",
    "- ðŸ“¡ Stream real-time data\n",
    "\n",
    "**Challenges:**\n",
    "- Different data formats\n",
    "- Network connectivity issues\n",
    "- Large file sizes\n",
    "- Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple data ingestion simulation\n",
    "def simulate_data_ingestion():\n",
    "    \"\"\"Simulate collecting data from multiple sources\"\"\"\n",
    "    \n",
    "    # Source 1: CSV file data\n",
    "    csv_orders = pd.DataFrame({\n",
    "        'order_id': ['CSV-001', 'CSV-002'],\n",
    "        'source': ['store', 'store'],\n",
    "        'customer': ['John Doe', 'Jane Smith'],\n",
    "        'amount': [999.99, 1999.99]\n",
    "    })\n",
    "    \n",
    "    # Source 2: API data\n",
    "    api_orders = pd.DataFrame({\n",
    "        'order_id': ['API-001', 'API-002'],\n",
    "        'source': ['website', 'mobile'],\n",
    "        'customer': ['Bob Wilson', 'Alice Johnson'],\n",
    "        'amount': [599.99, 399.99]\n",
    "    })\n",
    "    \n",
    "    # Combine data from all sources\n",
    "    all_orders = pd.concat([csv_orders, api_orders], ignore_index=True)\n",
    "    \n",
    "    return all_orders\n",
    "\n",
    "# Simulate ingestion\n",
    "ingested_data = simulate_data_ingestion()\n",
    "print(\"ðŸ“¥ Data Ingestion Results:\")\n",
    "display(ingested_data)\n",
    "print(f\"\\nâœ… Successfully ingested {len(ingested_data)} orders from multiple sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Stage 2: Data Validation\n",
    "\n",
    "**Purpose:** Ensure data quality and completeness\n",
    "\n",
    "**Key Activities:**\n",
    "- âœ… Schema validation (correct fields, data types)\n",
    "- ðŸ” Business rule validation (positive prices, valid dates)\n",
    "- ðŸ“Š Data quality scoring\n",
    "- âš ï¸ Error reporting\n",
    "\n",
    "**Common Issues:**\n",
    "- Missing required fields\n",
    "- Invalid data types\n",
    "- Business rule violations\n",
    "- Duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple data validation\n",
    "def validate_order_data(data):\n",
    "    \"\"\"Validate order data quality\"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        'total_records': len(data),\n",
    "        'valid_records': 0,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # Check required fields\n",
    "    required_fields = ['order_id', 'customer', 'amount']\n",
    "    for field in required_fields:\n",
    "        if field not in data.columns:\n",
    "            validation_results['issues'].append(f\"Missing required field: {field}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = data.isnull().sum()\n",
    "    for field, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            validation_results['issues'].append(f\"{field}: {count} missing values\")\n",
    "    \n",
    "    # Check business rules\n",
    "    if 'amount' in data.columns:\n",
    "        negative_amounts = (data['amount'] <= 0).sum()\n",
    "        if negative_amounts > 0:\n",
    "            validation_results['issues'].append(f\"Found {negative_amounts} orders with non-positive amounts\")\n",
    "    \n",
    "    # Calculate valid records\n",
    "    validation_results['valid_records'] = len(data) - len(validation_results['issues'])\n",
    "    validation_results['quality_score'] = (validation_results['valid_records'] / validation_results['total_records']) * 100\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate our ingested data\n",
    "validation_results = validate_order_data(ingested_data)\n",
    "\n",
    "print(\"ðŸ” Data Validation Results:\")\n",
    "print(f\"Total Records: {validation_results['total_records']}\")\n",
    "print(f\"Quality Score: {validation_results['quality_score']:.1f}%\")\n",
    "\n",
    "if validation_results['issues']:\n",
    "    print(\"\\nâš ï¸ Issues Found:\")\n",
    "    for issue in validation_results['issues']:\n",
    "        print(f\"  - {issue}\")\nelse:\n",
    "    print(\"\\nâœ… No validation issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§¹ Stage 3: Data Transformation\n",
    "\n",
    "**Purpose:** Clean, standardize, and enrich data\n",
    "\n",
    "**Key Activities:**\n",
    "- ðŸ§½ **Data Cleaning:** Remove duplicates, fix formats\n",
    "- ðŸ“ **Standardization:** Consistent formats and units\n",
    "- âž• **Enrichment:** Add calculated fields and metadata\n",
    "- ðŸ”„ **Normalization:** Structure data for analysis\n",
    "\n",
    "**Common Transformations:**\n",
    "- Date format standardization\n",
    "- Text cleaning and normalization\n",
    "- Currency conversion\n",
    "- Category mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data transformation\n",
    "def transform_order_data(data):\n",
    "    \"\"\"Transform and enrich order data\"\"\"\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    transformed_data = data.copy()\n",
    "    \n",
    "    # 1. Standardize customer names (Title Case)\n",
    "    transformed_data['customer'] = transformed_data['customer'].str.title()\n",
    "    \n",
    "    # 2. Add calculated fields\n",
    "    transformed_data['order_size'] = pd.cut(\n",
    "        transformed_data['amount'], \n",
    "        bins=[0, 500, 1000, 2000, float('inf')],\n",
    "        labels=['Small', 'Medium', 'Large', 'XLarge']\n",
    "    )\n",
    "    \n",
    "    # 3. Add processing metadata\n",
    "    transformed_data['processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # 4. Add source category\n",
    "    source_mapping = {\n",
    "        'store': 'Physical',\n",
    "        'website': 'Online',\n",
    "        'mobile': 'Online'\n",
    "    }\n",
    "    transformed_data['source_category'] = transformed_data['source'].map(source_mapping)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "# Transform our data\n",
    "transformed_data = transform_order_data(ingested_data)\n",
    "\n",
    "print(\"ðŸ§¹ Data Transformation Results:\")\n",
    "display(transformed_data)\n",
    "\n",
    "print(f\"\\nðŸ“Š Order Size Distribution:\")\n",
    "print(transformed_data['order_size'].value_counts())\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Source Category Distribution:\")\n",
    "print(transformed_data['source_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ Common Data Ingestion Challenges\n",
    "\n",
    "Real-world data ingestion comes with many challenges. Let's explore the most common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize common challenges and their impact\n",
    "challenges_data = {\n",
    "    'Challenge': [\n",
    "        'Data Quality Issues',\n",
    "        'Format Inconsistencies', \n",
    "        'Network Failures',\n",
    "        'Large Data Volumes',\n",
    "        'Schema Changes',\n",
    "        'Rate Limiting',\n",
    "        'Security & Authentication',\n",
    "        'Real-time Processing'\n",
    "    ],\n",
    "    'Frequency': ['Very High', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Medium'],\n",
    "    'Impact': ['High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Medium'],\n",
    "    'Solutions': [\n",
    "        'Validation, Cleaning, Monitoring',\n",
    "        'Standardization, Schema validation',\n",
    "        'Retry logic, Circuit breakers',\n",
    "        'Batch processing, Streaming',\n",
    "        'Version control, Backward compatibility',\n",
    "        'Throttling, Queue management',\n",
    "        'OAuth, API keys, Encryption',\n",
    "        'Stream processing, Event-driven'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_challenges = pd.DataFrame(challenges_data)\n",
    "print(\"ðŸš§ Common Data Ingestion Challenges:\")\n",
    "print(\"=\" * 80)\n",
    "display(df_challenges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Challenge Deep Dive: Data Quality Issues\n",
    "\n",
    "Let's simulate some common data quality issues and see how to handle them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with quality issues\n",
    "problematic_data = pd.DataFrame({\n",
    "    'order_id': ['ORD-001', '', 'ORD-003', 'ORD-001'],  # Missing and duplicate IDs\n",
    "    'customer_name': ['john doe', 'JANE SMITH', '', 'Bob Wilson'],  # Inconsistent case, missing\n",
    "    'product': ['iPhone 15', 'macbook pro', 'AirPods Pro', 'iPad Air'],\n",
    "    'quantity': [1, -1, 2, 0],  # Negative and zero quantities\n",
    "    'price': [999.99, 1999.99, -249.99, 599.99],  # Negative price\n",
    "    'order_date': ['2024-01-15', '2025-12-31', '2024-01-16', 'invalid-date'],  # Future and invalid dates\n",
    "    'email': ['john@example.com', 'invalid-email', 'bob@example.com', 'alice@example.com']\n",
    "})\n",
    "\n",
    "print(\"ðŸš¨ Sample Data with Quality Issues:\")\n",
    "display(problematic_data)\n",
    "\n",
    "# Analyze the issues\n",
    "print(\"\\nðŸ” Data Quality Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Missing values\n",
    "missing_values = problematic_data.isnull().sum() + (problematic_data == '').sum()\n",
    "print(\"Missing/Empty Values:\")\n",
    "for col, count in missing_values.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count}\")\n",
    "\n",
    "# 2. Duplicates\n",
    "duplicates = problematic_data.duplicated(subset=['order_id']).sum()\n",
    "print(f\"\\nDuplicate Order IDs: {duplicates}\")\n",
    "\n",
    "# 3. Business rule violations\n",
    "negative_quantities = (problematic_data['quantity'] <= 0).sum()\n",
    "negative_prices = (problematic_data['price'] <= 0).sum()\n",
    "print(f\"Negative/Zero Quantities: {negative_quantities}\")\n",
    "print(f\"Negative/Zero Prices: {negative_prices}\")\n",
    "\n",
    "# 4. Invalid emails\n",
    "import re\n",
    "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "invalid_emails = ~problematic_data['email'].str.match(email_pattern, na=False)\n",
    "print(f\"Invalid Email Addresses: {invalid_emails.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data Ingestion Patterns\n",
    "\n",
    "Different scenarios require different ingestion patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different ingestion patterns\n",
    "patterns_data = {\n",
    "    'Pattern': ['Batch Processing', 'Real-time Streaming', 'Micro-batching', 'Event-driven', 'Hybrid'],\n",
    "    'Frequency': ['Hours/Days', 'Continuous', 'Minutes', 'On-demand', 'Mixed'],\n",
    "    'Latency': ['High', 'Very Low', 'Low', 'Variable', 'Mixed'],\n",
    "    'Complexity': ['Low', 'High', 'Medium', 'Medium', 'High'],\n",
    "    'Use Cases': [\n",
    "        'Daily reports, ETL jobs',\n",
    "        'Live dashboards, Alerts',\n",
    "        'Near real-time analytics',\n",
    "        'Webhooks, Notifications',\n",
    "        'Enterprise systems'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_patterns = pd.DataFrame(patterns_data)\n",
    "print(\"ðŸ”„ Data Ingestion Patterns:\")\n",
    "print(\"=\" * 70)\n",
    "display(df_patterns)\n",
    "\n",
    "# Create a simple visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Complexity vs Latency\n",
    "complexity_map = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "latency_map = {'Very Low': 1, 'Low': 2, 'Variable': 2.5, 'High': 3, 'Mixed': 2.5}\n",
    "\n",
    "x = [complexity_map[c] for c in df_patterns['Complexity']]\n",
    "y = [latency_map[l] for l in df_patterns['Latency']]\n",
    "\n",
    "ax1.scatter(x, y, s=100, alpha=0.7)\n",
    "for i, pattern in enumerate(df_patterns['Pattern']):\n",
    "    ax1.annotate(pattern, (x[i], y[i]), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax1.set_xlabel('Complexity')\n",
    "ax1.set_ylabel('Latency')\n",
    "ax1.set_title('Ingestion Patterns: Complexity vs Latency')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Pattern distribution\n",
    "pattern_counts = df_patterns['Pattern'].value_counts()\n",
    "ax2.pie([1]*len(pattern_counts), labels=pattern_counts.index, autopct='%1.0f%%')\n",
    "ax2.set_title('Ingestion Pattern Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Best Practices for Data Ingestion\n",
    "\n",
    "Based on industry experience, here are the key best practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices checklist\n",
    "best_practices = {\n",
    "    'Category': [\n",
    "        'Data Quality',\n",
    "        'Error Handling', \n",
    "        'Performance',\n",
    "        'Security',\n",
    "        'Monitoring',\n",
    "        'Documentation',\n",
    "        'Testing',\n",
    "        'Scalability'\n",
    "    ],\n",
    "    'Key Practices': [\n",
    "        'Validate early, Clean consistently, Monitor quality',\n",
    "        'Retry logic, Circuit breakers, Graceful degradation',\n",
    "        'Batch processing, Parallel execution, Caching',\n",
    "        'Encrypt data, Secure APIs, Access control',\n",
    "        'Log everything, Track metrics, Set up alerts',\n",
    "        'Document schemas, API contracts, Processes',\n",
    "        'Unit tests, Integration tests, Data tests',\n",
    "        'Horizontal scaling, Load balancing, Queuing'\n",
    "    ],\n",
    "    'Priority': ['Critical', 'Critical', 'High', 'Critical', 'High', 'Medium', 'High', 'Medium']\n",
    "}\n",
    "\n",
    "df_practices = pd.DataFrame(best_practices)\n",
    "print(\"ðŸ’¡ Data Ingestion Best Practices:\")\n",
    "print(\"=\" * 80)\n",
    "display(df_practices)\n",
    "\n",
    "# Priority distribution\n",
    "priority_counts = df_practices['Priority'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'orange', 'yellow']\n",
    "plt.pie(priority_counts.values, labels=priority_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Best Practices by Priority Level')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Priority Breakdown:\")\n",
    "for priority, count in priority_counts.items():\n",
    "    print(f\"  {priority}: {count} practices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Future of Data Ingestion\n",
    "\n",
    "The data ingestion landscape is rapidly evolving. Here are key trends to watch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future trends in data ingestion\n",
    "trends_data = {\n",
    "    'Trend': [\n",
    "        'AI-Powered Data Quality',\n",
    "        'Serverless Ingestion',\n",
    "        'Real-time Everything',\n",
    "        'Schema Evolution',\n",
    "        'Edge Computing',\n",
    "        'Data Mesh Architecture',\n",
    "        'Privacy-First Design',\n",
    "        'No-Code/Low-Code'\n",
    "    ],\n",
    "    'Impact': ['High', 'High', 'Very High', 'Medium', 'High', 'Medium', 'High', 'Medium'],\n",
    "    'Timeline': ['2-3 years', '1-2 years', 'Now', '2-3 years', '1-2 years', '3-5 years', 'Now', '1-2 years'],\n",
    "    'Description': [\n",
    "        'ML-based anomaly detection and auto-correction',\n",
    "        'Cloud functions for event-driven ingestion',\n",
    "        'Sub-second latency for all data processing',\n",
    "        'Automatic schema migration and compatibility',\n",
    "        'Processing data closer to the source',\n",
    "        'Decentralized data ownership and governance',\n",
    "        'Built-in privacy and compliance features',\n",
    "        'Visual pipeline builders for non-technical users'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_trends = pd.DataFrame(trends_data)\n",
    "print(\"ðŸ”® Future Trends in Data Ingestion:\")\n",
    "print(\"=\" * 80)\n",
    "display(df_trends)\n",
    "\n",
    "# Timeline visualization\n",
    "timeline_map = {'Now': 0, '1-2 years': 1.5, '2-3 years': 2.5, '3-5 years': 4}\n",
    "impact_map = {'Medium': 1, 'High': 2, 'Very High': 3}\n",
    "\n",
    "x = [timeline_map[t] for t in df_trends['Timeline']]\n",
    "y = [impact_map[i] for i in df_trends['Impact']]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(x, y, s=150, alpha=0.7, c=y, cmap='viridis')\n",
    "\n",
    "for i, trend in enumerate(df_trends['Trend']):\n",
    "    plt.annotate(trend, (x[i], y[i]), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.xlabel('Timeline (Years from Now)')\n",
    "plt.ylabel('Expected Impact')\n",
    "plt.title('Data Ingestion Trends: Timeline vs Impact')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, label='Impact Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "Congratulations! You've completed the first tutorial in our data ingestion series. Here's what you've learned:\n",
    "\n",
    "### âœ… **Core Concepts**\n",
    "- **Data Ingestion** is the foundation of all data analytics\n",
    "- **Multiple Sources** require different handling strategies\n",
    "- **Data Quality** is critical for reliable insights\n",
    "- **Pipeline Stages** each serve a specific purpose\n",
    "\n",
    "### âœ… **Practical Skills**\n",
    "- Identifying different data source types\n",
    "- Understanding common data quality issues\n",
    "- Recognizing ingestion patterns and their use cases\n",
    "- Applying best practices for reliable pipelines\n",
    "\n",
    "### âœ… **Industry Knowledge**\n",
    "- Common challenges and their solutions\n",
    "- Best practices from real-world implementations\n",
    "- Future trends shaping the industry\n",
    "- Career opportunities in data engineering\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ What's Next?\n",
    "\n",
    "In the next tutorial, **\"02_reading_files.ipynb\"**, you'll learn:\n",
    "- ðŸ“ How to read different file formats (CSV, JSON, Excel)\n",
    "- ðŸ” File validation and error handling\n",
    "- ðŸ“Š Processing large files efficiently\n",
    "- ðŸ”„ Automating file monitoring and processing\n",
    "\n",
    "### ðŸŽ¯ **Practice Exercise**\n",
    "\n",
    "Before moving to the next tutorial, try this exercise:\n",
    "\n",
    "1. **Create sample data** for a different business (e.g., restaurant, hospital, school)\n",
    "2. **Identify data sources** that business might have\n",
    "3. **List potential data quality issues** for that domain\n",
    "4. **Design a simple pipeline** for that use case\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- **ðŸ“– Books**: \"Designing Data-Intensive Applications\" by Martin Kleppmann\n",
    "- **ðŸŒ Online**: Apache Kafka documentation, Apache Airflow tutorials\n",
    "- **ðŸŽ“ Courses**: Data Engineering courses on Coursera, Udacity\n",
    "- **ðŸ’¼ Communities**: Data Engineering Slack, Reddit r/dataengineering\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}